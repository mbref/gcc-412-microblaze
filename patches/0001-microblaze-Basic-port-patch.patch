From 930a7bf07b58a3d6a48393a6419b2c958fc9dc2a Mon Sep 17 00:00:00 2001
Message-Id: <930a7bf07b58a3d6a48393a6419b2c958fc9dc2a.1241484427.git.john.williams@petalogix.com>
From: John Williams <john.williams@petalogix.com>
Date: Tue, 28 Apr 2009 14:38:55 +1000
Subject: [PATCH 01/63] microblaze: Basic port patch

---
 config.guess                                  |    3 +
 config.sub                                    |    7 +-
 gcc/config.gcc                                |    7 +
 gcc/config/microblaze/crti.s                  |   17 +
 gcc/config/microblaze/crtn.s                  |   12 +
 gcc/config/microblaze/divsi3.asm              |   74 +
 gcc/config/microblaze/divsi3_table.c          |   20 +
 gcc/config/microblaze/microblaze-protos.h     |   41 +
 gcc/config/microblaze/microblaze.c            | 5506 ++++++++++++
 gcc/config/microblaze/microblaze.h            | 3688 ++++++++
 gcc/config/microblaze/microblaze.md           | 3767 ++++++++
 gcc/config/microblaze/modsi3.asm              |   74 +
 gcc/config/microblaze/muldi3_hard.asm         |  127 +
 gcc/config/microblaze/mulsi3.asm              |   51 +
 gcc/config/microblaze/stack_overflow_exit.asm |   34 +
 gcc/config/microblaze/t-microblaze            |   45 +
 gcc/config/microblaze/udivsi3.asm             |   85 +
 gcc/config/microblaze/umodsi3.asm             |   90 +
 gcc/config/microblaze/x-microblaze            |   20 +
 gcc/config/microblaze/xm-microblaze.h         |   76 +
 gcc/fold-const.c.orig                         |11862 +++++++++++++++++++++++++
 gcc/fold-const.c.rej                          |   21 +
 gcc/rtl.h                                     |    9 +
 23 files changed, 25635 insertions(+), 1 deletions(-)
 create mode 100644 gcc/config/microblaze/crti.s
 create mode 100644 gcc/config/microblaze/crtn.s
 create mode 100644 gcc/config/microblaze/divsi3.asm
 create mode 100644 gcc/config/microblaze/divsi3_table.c
 create mode 100644 gcc/config/microblaze/microblaze-protos.h
 create mode 100644 gcc/config/microblaze/microblaze.c
 create mode 100644 gcc/config/microblaze/microblaze.h
 create mode 100644 gcc/config/microblaze/microblaze.md
 create mode 100644 gcc/config/microblaze/modsi3.asm
 create mode 100644 gcc/config/microblaze/muldi3_hard.asm
 create mode 100644 gcc/config/microblaze/mulsi3.asm
 create mode 100644 gcc/config/microblaze/stack_overflow_exit.asm
 create mode 100644 gcc/config/microblaze/t-microblaze
 create mode 100644 gcc/config/microblaze/udivsi3.asm
 create mode 100644 gcc/config/microblaze/umodsi3.asm
 create mode 100644 gcc/config/microblaze/x-microblaze
 create mode 100644 gcc/config/microblaze/xm-microblaze.h
 create mode 100644 gcc/fold-const.c.orig
 create mode 100644 gcc/fold-const.c.rej

diff --git a/config.guess b/config.guess
index 52c01be..9e9ca97 100755
--- a/config.guess
+++ b/config.guess
@@ -835,6 +835,9 @@
     m68*:Linux:*:*)
 	echo ${UNAME_MACHINE}-unknown-linux-gnu
 	exit ;;
+    microblaze:xilinx:*:*)  
+        echo microblaze-xilinx-elf
+        exit ;;
     mips:Linux:*:*)
 	eval $set_cc_for_build
 	sed 's/^	//' << EOF >$dummy.c
--- a/config.sub
+++ b/config.sub
@@ -146,7 +146,7 @@
 	-convergent* | -ncr* | -news | -32* | -3600* | -3100* | -hitachi* |\
 	-c[123]* | -convex* | -sun | -crds | -omron* | -dg | -ultra | -tti* | \
 	-harris | -dolphin | -highlevel | -gould | -cbm | -ns | -masscomp | \
-	-apple | -axis | -knuth | -cray)
+	-apple | -axis | -knuth | -cray | -microblaze )
 		os=
 		basic_machine=$1
 		;;
@@ -678,6 +678,9 @@
 		basic_machine=ns32k-utek
 		os=-sysv
 		;;
+	microblaze*-*)
+		basic_machine=microblaze-xilinx
+                ;;
 	mingw32)
 		basic_machine=i386-pc
 		os=-mingw32
--- a/gcc/config.gcc
+++ b/gcc/config.gcc
@@ -287,6 +287,9 @@
 m68k-*-*)
 	extra_headers=math-68881.h
 	;;
+microblaze*-*-*)
+        cpu_type=microblaze
+        ;;
 mips*-*-*)
 	cpu_type=mips
 	need_64bit_hwint=yes
@@ -1445,6 +1448,10 @@
 	tmake_file=mcore/t-mcore-pe
 	use_fixproto=yes
 	;;
+microblaze*-*-*)
+        tm_file="microblaze/microblaze.h"
+        tmake_file=microblaze/t-microblaze
+        ;;
 mips-sgi-irix[56]*)
 	tm_file="elfos.h ${tm_file} mips/iris.h"
 	tmake_file="mips/t-iris mips/t-slibgcc-irix"
--- /dev/null
+++ b/gcc/config/microblaze/crti.s
@@ -0,0 +1,17 @@
+/* crti.s for __init, __fini
+   This file supplies the prologue for __init and __fini routines */
+
+    .section .init, "ax"
+    .global __init
+    .align 2
+__init: 
+    addik   r1, r1, -8
+    sw      r15, r0, r1
+
+    .section .fini, "ax"
+    .global __fini
+    .align 2
+__fini: 
+    addik   r1, r1, -8
+    sw      r15, r0, r1
+        
\ No newline at end of file
diff --git a/gcc/config/microblaze/crtn.s b/gcc/config/microblaze/crtn.s
--- /dev/null
+++ b/gcc/config/microblaze/crtn.s
@@ -0,0 +1,12 @@
+/* crtn.s for __init, __fini
+   This file supplies the epilogue for __init and __fini routines */
+
+    .section .init, "ax"
+    lw      r15, r0, r1
+    rtsd    r15, 8 
+    addik   r1, r1, 8
+
+    .section .fini, "ax"
+    lw      r15, r0, r1
+    rtsd    r15, 8 
+    addik   r1, r1, 8    
--- /dev/null
+++ b/gcc/config/microblaze/divsi3.asm
@@ -0,0 +1,74 @@
+###################################-*-asm*- 
+# 
+# Copyright (c) 2001 Xilinx, Inc.  All rights reserved. 
+# 
+# Xilinx, Inc. CONFIDENTIAL 
+# 
+# divsi3.asm 
+# 
+# Divide operation for 32 bit integers.
+#	Input :	Divisor in Reg r5
+#		Dividend in Reg r6
+#	Output: Result in Reg r3
+# 
+# $Header: /devl/xcs/repo/env/Jobs/MDT/sw/ThirdParty/gnu/src/gcc/src-3.4/gcc/config/microblaze/divsi3.s,v 1.1.2.6 2005/11/15 23:32:46 salindac Exp $
+# 
+#######################################
+	
+	.globl	__divsi3
+	.ent	__divsi3
+__divsi3:
+	.frame	r1,0,r15	
+
+	addik r1,r1,-16
+	swi r28,r1,0
+	swi r29,r1,4
+	swi r30,r1,8
+	swi r31,r1,12
+
+	BEQI r6,$LaDiv_By_Zero       # Div_by_Zero   # Division Error
+	BEQI r5,$LaResult_Is_Zero  # Result is Zero 
+	BGEId r5,$LaR5_Pos 
+	XOR  r28,r5,r6  # Get the sign of the result
+	RSUBI r5,r5,0	    # Make r5 positive
+$LaR5_Pos:
+	BGEI r6,$LaR6_Pos
+	RSUBI r6,r6,0	    # Make r6 positive
+$LaR6_Pos:
+	ADDIK r30,r0,0     # Clear mod
+	ADDIK r3,r0,0     # clear div
+	ADDIK r29,r0,32  # Initialize the loop count
+   # First part try to find the first '1' in the r5
+$LaDIV1:
+	ADD r5,r5,r5     # left shift logical r5
+	BGEID r5,$LaDIV1       #
+	ADDIK r29,r29,-1
+$LaDIV2:
+	ADD r5,r5,r5     # left shift logical  r5 get the '1' into the Carry
+	ADDC r30,r30,r30 # Move that bit into the Mod register
+	rSUB r31,r6,r30 # Try to subtract (r30 a r6)
+	BLTi r31,$LaMOD_TOO_SMALL
+	OR  r30,r0,r31  # Move the r31 to mod since the result was positive
+	ADDIK r3,r3,1
+$LaMOD_TOO_SMALL:
+	ADDIK r29,r29,-1
+	BEQi r29,$LaLOOP_END
+	ADD r3,r3,r3 # Shift in the '1' into div
+	BRI $LaDIV2   # Div2
+$LaLOOP_END:
+	BGEI r28,$LaRETURN_HERE
+	BRId $LaRETURN_HERE
+	rsubi r3,r3,0 # Negate the result
+$LaDiv_By_Zero:
+$LaResult_Is_Zero:
+	or r3,r0,r0 # set result to 0
+$LaRETURN_HERE:
+# Restore values of CSRs and that of r3 and the divisor and the dividend
+	lwi r28,r1,0
+	lwi r29,r1,4
+	lwi r30,r1,8
+	lwi r31,r1,12
+	rtsd r15,8
+	addik r1,r1,16
+.end __divsi3
+	
--- /dev/null
+++ b/gcc/config/microblaze/divsi3_table.c
@@ -0,0 +1,20 @@
+unsigned char _divsi3_table[] =
+{
+    0, 0/1, 0/2, 0/3, 0/4, 0/5, 0/6, 0/7, 0/8, 0/9, 0/10, 0/11, 0/12, 0/13, 0/14, 0/15,
+    0, 1/1, 1/2, 1/3, 1/4, 1/5, 1/6, 1/7, 1/8, 1/9, 1/10, 1/11, 1/12, 1/13, 1/14, 1/15,
+    0, 2/1, 2/2, 2/3, 2/4, 2/5, 2/6, 2/7, 2/8, 2/9, 2/10, 2/11, 2/12, 2/13, 2/14, 2/15,
+    0, 3/1, 3/2, 3/3, 3/4, 3/5, 3/6, 3/7, 3/8, 3/9, 3/10, 3/11, 3/12, 3/13, 3/14, 3/15,
+    0, 4/1, 4/2, 4/3, 4/4, 4/5, 4/6, 4/7, 4/8, 4/9, 4/10, 4/11, 4/12, 4/13, 4/14, 4/15,
+    0, 5/1, 5/2, 5/3, 5/4, 5/5, 5/6, 5/7, 5/8, 5/9, 5/10, 5/11, 5/12, 5/13, 5/14, 5/15,
+    0, 6/1, 6/2, 6/3, 6/4, 6/5, 6/6, 6/7, 6/8, 6/9, 6/10, 6/11, 6/12, 6/13, 6/14, 6/15,
+    0, 7/1, 7/2, 7/3, 7/4, 7/5, 7/6, 7/7, 7/8, 7/9, 7/10, 7/11, 7/12, 7/13, 7/14, 7/15,
+    0, 8/1, 8/2, 8/3, 8/4, 8/5, 8/6, 8/7, 8/8, 8/9, 8/10, 8/11, 8/12, 8/13, 8/14, 8/15,
+    0, 9/1, 9/2, 9/3, 9/4, 9/5, 9/6, 9/7, 9/8, 9/9, 9/10, 9/11, 9/12, 9/13, 9/14, 9/15,
+    0, 10/1, 10/2, 10/3, 10/4, 10/5, 10/6, 10/7, 10/8, 10/9, 10/10, 10/11, 10/12, 10/13, 10/14, 10/15,
+    0, 11/1, 11/2, 11/3, 11/4, 11/5, 11/6, 11/7, 11/8, 11/9, 11/10, 11/11, 11/12, 11/13, 11/14, 11/15,
+    0, 12/1, 12/2, 12/3, 12/4, 12/5, 12/6, 12/7, 12/8, 12/9, 12/10, 12/11, 12/12, 12/13, 12/14, 12/15,
+    0, 13/1, 13/2, 13/3, 13/4, 13/5, 13/6, 13/7, 13/8, 13/9, 13/10, 13/11, 13/12, 13/13, 13/14, 13/15,
+    0, 14/1, 14/2, 14/3, 14/4, 14/5, 14/6, 14/7, 14/8, 14/9, 14/10, 14/11, 14/12, 14/13, 14/14, 14/15,
+    0, 15/1, 15/2, 15/3, 15/4, 15/5, 15/6, 15/7, 15/8, 15/9, 15/10, 15/11, 15/12, 15/13, 15/14, 15/15,
+};
+
--- /dev/null
+++ b/gcc/config/microblaze/microblaze-protos.h
@@ -0,0 +1,41 @@
+/* Definitions of target machine for GNU compiler, for Xilinx MicroBlaze.
+   Copyright (C) 2000, 2001, 2002, 2003, 2004
+   Free Software Foundation, Inc.
+   Contributed by Richard Kenner (kenner@vlsi1.ultra.nyu.edu)
+
+   This file is part of GCC.
+
+   GCC is free software; you can redistribute it and/or modify it
+   under the terms of the GNU General Public License as published
+   by the Free Software Foundation; either version 2, or (at your
+   option) any later version.
+
+   GCC is distributed in the hope that it will be useful, but WITHOUT
+   ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
+   or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public
+   License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with GCC; see the file COPYING.  If not, write to the
+   Free Software Foundation, 59 Temple Place - Suite 330, Boston,
+   MA 02111-1307, USA.  */
+
+#ifndef __MICROBLAZE_PROTOS__
+#define __MICROBLAZE_PROTOS__
+
+#ifdef RTX_CODE
+extern void barrel_shift_left_imm(rtx operands[]);
+extern void shift_left_imm(rtx operands[]);
+extern void shift_right_imm(rtx operands[]);
+extern rtx embedded_pic_offset       PARAMS ((rtx));
+extern int pic_address_needs_scratch PARAMS ((rtx));
+extern void expand_block_move        PARAMS ((rtx *));
+extern const char* microblaze_move_1word PARAMS ((  rtx[], rtx, int));
+extern void shift_left_imm  PARAMS ((rtx []));
+extern void microblaze_expand_prologue (void);
+extern void microblaze_expand_epilogue (void);
+extern void shift_double_left_imm    PARAMS ((rtx []));
+extern void override_options (void);
+extern void machine_dependent_reorg PARAMS ((void));
+#endif  /* RTX_CODE */
+#endif  /* __MICROBLAZE_PROTOS__ */
--- /dev/null
+++ b/gcc/config/microblaze/microblaze.c
@@ -0,0 +1,5506 @@
+/* Copyright (C) 1989, 90, 91, 93-98, 1999 Free Software Foundation, Inc.
+   This file is part of GNU CC.
+
+   GNU CC is free software; you can redistribute it and/or modify
+   it under the terms of the GNU General Public License as published by
+   the Free Software Foundation; either version 2, or (at your option)
+   any later version.
+
+   GNU CC is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+   GNU General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with GNU CC; see the file COPYING.  If not, write to
+   the Free Software Foundation, 59 Temple Place - Suite 330,
+   Boston, MA 02111-1307, USA.  */
+
+/***************************************************************************************-*-C-*- 
+ * 
+ * Copyright (c) 2001 Xilinx, Inc.  All rights reserved. 
+ * 
+ * microblaze.c : Home
+ * 
+ * MicroBlaze specific file. Contains functions for generating MicroBlaze code
+ * Certain sections of code are from the Free Software Foundation
+ * 
+ * $Header: /devl/xcs/repo/env/Jobs/MDT/sw/ThirdParty/gnu/src/gcc/src-3.4/gcc/config/microblaze/microblaze.c,v 1.14.2.16 2006/05/22 15:25:27 vasanth Exp $
+ * 
+ *******************************************************************************/
+
+#include "config.h"
+#include "system.h"
+#include "coretypes.h"
+#include <signal.h>
+#include "tm.h"
+#include "rtl.h"
+#include "regs.h"
+#include "hard-reg-set.h"
+#include "real.h"
+#include "insn-config.h"
+#include "conditions.h"
+#include "insn-flags.h"
+#include "insn-attr.h"
+#include "recog.h"
+#include "toplev.h"
+
+#undef MAX			/* sys/param.h may also define these */
+#undef MIN
+
+#include "tree.h"
+#include "function.h"
+#include "expr.h"
+#include "flags.h"
+#include "reload.h"
+#include "output.h"
+#include "ggc.h"
+#include "hashtab.h"
+#include "target.h"
+#include "target-def.h"
+#include "tm_p.h"
+
+
+#if defined(USG) || !defined(HAVE_STAB_H)
+#include "gstab.h"  /* If doing DBX on sysV, use our own stab.h.  */
+#else
+#include <stab.h>  /* On BSD, use the system's stab.h.  */
+#endif /* not USG */
+
+#ifdef __GNU_STAB__
+#define STAB_CODE_TYPE enum __stab_debug_code
+#else
+#define STAB_CODE_TYPE int
+#endif
+
+/* Enumeration for all of the relational tests, so that we can build
+   arrays indexed by the test type, and not worry about the order
+   of EQ, NE, etc. */
+
+enum internal_test {
+  ITEST_EQ,
+  ITEST_NE,
+  ITEST_GT,
+  ITEST_GE,
+  ITEST_LT,
+  ITEST_LE,
+  ITEST_GTU,
+  ITEST_GEU,
+  ITEST_LTU,
+  ITEST_LEU,
+  ITEST_MAX
+};
+
+/* Forward declaration for sruct constant */
+struct constant;
+
+/* Classifies an address.
+
+ADDRESS_INVALID
+An invalid address.
+
+ADDRESS_REG
+
+A natural register or a register + const_int offset address.  
+The register satisfies microblaze_valid_base_register_p and the 
+offset is a const_arith_operand.
+
+ADDRESS_REG_INDEX
+
+A natural register offset by the index contained in an index register. The base
+register satisfies microblaze_valid_base_register_p and the index register
+satisfies microblaze_valid_index_register_p
+
+ADDRESS_CONST_INT
+
+A signed 16/32-bit constant address.
+
+ADDRESS_SYMBOLIC:
+
+A constant symbolic address or a (register + symbol).  */
+
+enum microblaze_address_type {
+  ADDRESS_INVALID,
+  ADDRESS_REG,
+  ADDRESS_REG_INDEX,
+  ADDRESS_CONST_INT,
+  ADDRESS_SYMBOLIC
+};
+
+/* Classifies symbols
+
+SYMBOL_TYPE_GENERAL
+        
+A general symbol. */
+enum microblaze_symbol_type {
+  SYMBOL_TYPE_INVALID,
+  SYMBOL_TYPE_GENERAL
+};
+
+/* Classification of a Microblaze address */
+struct microblaze_address_info
+{
+  enum microblaze_address_type type;
+  rtx regA;                       /* Contains valid values on ADDRES_REG, ADDRESS_REG_INDEX, ADDRESS_SYMBOLIC */
+  rtx regB;                       /* Contains valid values on ADDRESS_REG_INDEX */
+  rtx offset;                     /* Contains valid values on ADDRESS_CONST_INT and ADDRESS_REG */
+  rtx symbol;                     /* Contains valid values on ADDRESS_SYMBOLIC */
+  enum microblaze_symbol_type symbol_type;
+};
+
+static void microblaze_encode_section_info	PARAMS ((tree, rtx, int));
+static void microblaze_globalize_label          PARAMS ((FILE*, const char*));
+void  microblaze_declare_comm_object            PARAMS ((FILE *, char *, char *, char *, int size, int align));
+static void microblaze_unique_section           PARAMS ((tree, int));
+static void microblaze_function_prologue        PARAMS ((FILE*, int));
+static void microblaze_function_epilogue        PARAMS ((FILE*, HOST_WIDE_INT));
+static void microblaze_asm_file_start           PARAMS ((void));
+static void microblaze_asm_file_end             PARAMS ((void));
+static char* microblaze_fill_delay_slot         PARAMS ((char *, enum delay_type ,rtx [],rtx ));
+static void microblaze_count_memory_refs        PARAMS ((rtx, int));
+static rtx embedded_pic_fnaddr_reg              PARAMS ((void));
+static void microblaze_internal_label           PARAMS ((FILE *, const char*, unsigned long));
+static bool microblaze_rtx_costs                PARAMS ((rtx, int, int, int*));
+static int microblaze_address_cost              PARAMS ((rtx));
+static int microblaze_address_insns             PARAMS ((rtx, enum machine_mode));
+static void microblaze_asm_constructor          PARAMS ((rtx, int));
+static void microblaze_asm_destructor           PARAMS ((rtx, int));
+static void microblaze_select_section           PARAMS ((tree, int, unsigned HOST_WIDE_INT));
+static void microblaze_select_rtx_section       PARAMS ((enum machine_mode, rtx, unsigned HOST_WIDE_INT));
+bool microblaze_legitimate_address_p            PARAMS ((enum machine_mode, rtx, int ));
+rtx  microblaze_legitimize_address              PARAMS ((rtx , rtx, enum machine_mode));
+int microblaze_regno_ok_for_base_p              PARAMS ((int, int));
+static char* microblaze_mode_to_mem_modifier    PARAMS ((int, enum machine_mode));
+static bool microblaze_valid_base_register_p    PARAMS ((rtx, enum machine_mode, int));
+static bool microblaze_valid_index_register_p   PARAMS ((rtx, enum machine_mode, int));
+static bool microblaze_classify_address         PARAMS ((struct microblaze_address_info *, rtx, enum machine_mode, int));
+HOST_WIDE_INT compute_frame_size                PARAMS ((HOST_WIDE_INT));
+int double_memory_operand                       PARAMS ((rtx, enum machine_mode));
+void microblaze_output_filename                 PARAMS ((FILE*, const char*));
+HOST_WIDE_INT microblaze_initial_elimination_offset 
+                                                PARAMS ((int, int));
+int microblaze_sched_use_dfa_pipeline_interface PARAMS ((void));
+void microblaze_function_end_prologue           PARAMS ((FILE *));
+static enum internal_test map_test_to_internal_test	
+                                                PARAMS ((enum rtx_code));
+static void block_move_loop			PARAMS ((rtx, rtx, int, int, rtx, rtx));
+static void block_move_call			PARAMS ((rtx, rtx, rtx));
+static void save_restore_insns			PARAMS ((int));
+static rtx add_constant				PARAMS ((struct constant **, rtx, enum machine_mode));
+static void dump_constants			PARAMS ((struct constant *, rtx));
+static int microblaze_version_to_int            PARAMS ((const char *));
+static int microblaze_version_compare           PARAMS ((const char *, const char *));
+
+/* Global variables for machine-dependent things.  */
+
+struct microblaze_cpu_select microblaze_select =
+{
+  MICROBLAZE_DEFAULT_CPU,                 /* CPU      */
+  "none",                                 /* Tuning   */
+  0                                       /* Flags    */
+};
+
+/* Toggle whether CRT clears BSS or not and whether zero initialized goes into BSS or not */
+char *microblaze_no_clearbss = "default";
+
+/* Toggle which pipleline interface to use */
+int microblaze_sched_use_dfa = 0;
+
+/* Threshold for data being put into the small data/bss area, instead
+   of the normal data area (references to the small data/bss area take
+   1 instruction, and use the global pointer, references to the normal
+   data area takes 2 instructions).  */
+int microblaze_section_threshold = -1;
+
+/* Prevent scheduling potentially exception causing instructions in delay slots.
+   -mcpu=v3.00.a or v4.00.a turns this on.
+*/
+int microblaze_no_unsafe_delay;
+
+/* Count the number of .file directives, so that .loc is up to date.  */
+int num_source_filenames = 0;
+
+/* Count the number of sdb related labels are generated (to find block
+   start and end boundaries).  */
+int sdb_label_count = 0;
+
+/* Next label # for each statement for Silicon Graphics IRIS systems. */
+int sym_lineno = 0;
+
+/* Non-zero if inside of a function, because the stupid asm can't
+   handle .files inside of functions.  */
+int inside_function = 0;
+
+/* Files to separate the text and the data output, so that all of the data
+   can be emitted before the text, which will mean that the assembler will
+   generate smaller code, based on the global pointer.  */
+FILE *asm_out_data_file;
+FILE *asm_out_text_file;
+
+
+/* Linked list of all externals that are to be emitted when optimizing
+   for the global pointer if they haven't been declared by the end of
+   the program with an appropriate .comm or initialization.  */
+
+struct extern_list
+{
+  struct extern_list *next;	/* next external */
+  char *name;			/* name of the external */
+  int size;			/* size in bytes */
+} *extern_head = 0;
+
+/* Name of the file containing the current function.  */
+const char *current_function_file = "";
+
+int file_in_function_warning = FALSE;
+
+/* Whether to suppress issuing .loc's because the user attempted
+   to change the filename within a function.  */
+int ignore_line_number = FALSE;
+
+/* Number of nested .set noreorder, noat, nomacro, and volatile requests.  */
+int set_noreorder;
+int set_noat;
+int set_nomacro;
+int set_volatile;
+
+
+/* Count of delay slots and how many are filled.  */
+int dslots_load_total;
+int dslots_load_filled;
+int dslots_jump_total;
+int dslots_jump_filled;
+
+/* # of nops needed by previous insn */
+int dslots_number_nops;
+
+/* Number of 1/2/3 word references to data items (ie, not brlid's).  */
+int num_refs[3];
+
+/* registers to check for load delay */
+rtx microblaze_load_reg, microblaze_load_reg2, microblaze_load_reg3, microblaze_load_reg4;
+
+/* Cached operands, and operator to compare for use in set/branch on
+   condition codes.  */
+rtx branch_cmp[2];
+
+/* what type of branch to use */
+enum cmp_type branch_type;
+
+/* Number of previously seen half-pic pointers and references.  */
+static int prev_half_pic_ptrs = 0;
+static int prev_half_pic_refs = 0;
+
+/* Which CPU pipeline do we use. We haven't really standardized on a CPU version 
+   having only a particular type of pipeline. There can still be options on the CPU 
+   to scale pipeline features up or down. :( Bad Presentation (??), so we let the
+   MD file rely on the value of this variable instead 
+   Making PIPE_5 the default. It should be backward optimal with PIPE_3 MicroBlazes */
+enum pipeline_type microblaze_pipe = MICROBLAZE_PIPE_5;
+
+/* Generating calls to position independent functions?  */
+enum microblaze_abicalls_type microblaze_abicalls;
+
+/* High and low marks for floating point values which we will accept
+   as legitimate constants for LEGITIMATE_CONSTANT_P.  These are
+   initialized in override_options.  */
+REAL_VALUE_TYPE dfhigh, dflow, sfhigh, sflow;
+
+/* Mode used for saving/restoring general purpose registers.  */
+static enum machine_mode gpr_mode;
+
+/* Array giving truth value on whether or not a given hard register
+   can support a given mode.  */
+char microblaze_hard_regno_mode_ok[(int)MAX_MACHINE_MODE][FIRST_PSEUDO_REGISTER];
+
+/* Current frame information calculated by compute_frame_size.  */
+struct microblaze_frame_info current_frame_info;
+
+/* Zero structure to initialize current_frame_info.  */
+struct microblaze_frame_info zero_frame_info;
+
+/* Temporary filename used to buffer .text until end of program
+   for -mgpopt.  */
+static char *temp_filename;
+
+/* Pseudo-reg holding the address of the current function when
+   generating embedded PIC code.  Created by LEGITIMIZE_ADDRESS, used
+   by microblaze_finalize_pic if it was created.  */
+rtx embedded_pic_fnaddr_rtx;
+
+struct string_constant
+{
+  struct string_constant *next;
+  char *label;
+};
+
+static struct string_constant *string_constants;
+
+/* List of all MICROBLAZE punctuation characters used by print_operand.  */
+char microblaze_print_operand_punct[256];
+
+/* Map GCC register number to debugger register number.  */
+int microblaze_dbx_regno[FIRST_PSEUDO_REGISTER];
+
+/* Hardware names for the registers.  If -mrnames is used, this
+   will be overwritten with microblaze_sw_reg_names.  */
+
+char microblaze_reg_names[][8] =
+{
+  "r0",   "r1",   "r2",   "r3",   "r4",   "r5",   "r6",   "r7",
+  "r8",   "r9",   "r10",  "r11",  "r12",  "r13",  "r14",  "r15",
+  "r16",  "r17",  "r18",  "r19",  "r20",  "r21",  "r22",  "r23",
+  "r24",  "r25",  "r26",  "r27",  "r28",  "r29",  "r30",  "r31",
+  "$f0",  "$f1",  "$f2",  "$f3",  "$f4",  "$f5",  "$f6",  "$f7",
+  "$f8",  "$f9",  "$f10", "$f11", "$f12", "$f13", "$f14", "$f15",
+  "$f16", "$f17", "$f18", "$f19", "$f20", "$f21", "$f22", "$f23",
+  "$f24", "$f25", "$f26", "$f27", "$f28", "$f29", "$f30", "$f31",
+  "hi",   "lo",   "accum","rmsr", "$fcc1","$fcc2","$fcc3","$fcc4",
+  "$fcc5","$fcc6","$fcc7","$ap",  "$rap", "$frp"
+};
+
+/* MicroBlaze software names for the registers, used to overwrite the
+   microblaze_reg_names array.  */
+
+char microblaze_sw_reg_names[][8] =
+{
+  "r0",   "r1",   "r2",   "r3",   "r4",   "r5",   "r6",   "r7",
+  "r8",   "r9",   "r10",  "r11",  "r12",  "r13",  "r14",  "r15",
+  "r16",  "r17",  "r18",  "r19",  "r20",  "r21",  "r22",  "r23",
+  "r24",  "r25",  "r26",  "r27",  "r28",  "r29",  "r30",  "r31",
+  "$f0",  "$f1",  "$f2",  "$f3",  "$f4",  "$f5",  "$f6",  "$f7",
+  "$f8",  "$f9",  "$f10", "$f11", "$f12", "$f13", "$f14", "$f15",
+  "$f16", "$f17", "$f18", "$f19", "$f20", "$f21", "$f22", "$f23",
+  "$f24", "$f25", "$f26", "$f27", "$f28", "$f29", "$f30", "$f31",
+  "hi",   "lo",   "accum","rmsr", "$fcc1","$fcc2","$fcc3","$fcc4",
+  "$fcc5","$fcc6","$fcc7","$ap",  "$rap", "$frp"
+};
+
+/* Map hard register number to register class */
+enum reg_class microblaze_regno_to_class[] =
+{
+  GR_REGS,	GR_REGS,	GR_REGS,	GR_REGS,
+  GR_REGS,	GR_REGS,	GR_REGS,	GR_REGS,
+  GR_REGS,	GR_REGS,	GR_REGS,	GR_REGS,
+  GR_REGS,	GR_REGS,	GR_REGS,	GR_REGS,
+  GR_REGS,	GR_REGS,	GR_REGS,	GR_REGS,
+  GR_REGS,	GR_REGS,	GR_REGS,	GR_REGS,
+  GR_REGS,	GR_REGS,	GR_REGS,	GR_REGS,
+  GR_REGS,	GR_REGS,	GR_REGS,	GR_REGS,
+  FP_REGS,	FP_REGS,	FP_REGS,	FP_REGS,
+  FP_REGS,	FP_REGS,	FP_REGS,	FP_REGS,
+  FP_REGS,	FP_REGS,	FP_REGS,	FP_REGS,
+  FP_REGS,	FP_REGS,	FP_REGS,	FP_REGS,
+  FP_REGS,	FP_REGS,	FP_REGS,	FP_REGS,
+  FP_REGS,	FP_REGS,	FP_REGS,	FP_REGS,
+  FP_REGS,	FP_REGS,	FP_REGS,	FP_REGS,
+  FP_REGS,	FP_REGS,	FP_REGS,	FP_REGS,
+  HI_REG,	LO_REG,		HILO_REG,	ST_REGS,
+  ST_REGS,	ST_REGS,	ST_REGS,	ST_REGS,
+  ST_REGS,	ST_REGS,	ST_REGS,	GR_REGS,
+  GR_REGS,    GR_REGS
+};
+
+/* Map register constraint character to register class.  */
+enum reg_class microblaze_char_to_class[256] =
+{
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+  NO_REGS,	NO_REGS,	NO_REGS,	NO_REGS,
+};
+
+int div_count = 0;
+/*extern void rodata_section();
+  extern void sbss_section();
+  extern void bss_section();*/
+int get_base_reg(rtx);
+static int printed = 0;
+enum load_store {LOAD = 0, STORE=1};
+char *format_load_store(char*, enum load_store,  enum machine_mode ,  rtx,int);
+static int prev_offset;
+
+/* True if the current function is an interrupt handler
+   (either via #pragma or an attribute specification).  */
+int interrupt_handler;
+int save_volatiles;
+
+/* Microblaze specific machine attributes.
+ * interrupt_handler - Interrupt handler attribute to add interrupt prologue and epilogue and use appropriate interrupt return.
+ * save_volatiles    - Similiar to interrupt handler, but use normal return.
+ */
+const struct attribute_spec microblaze_attribute_table[] = 
+{
+  /* name             , min_len, max_len, decl_req, type_req, fn_type, req_handler */
+  {"interrupt_handler", 0, 0, true, false, false, NULL},
+  {"save_volatiles"   , 0, 0, true, false, false, NULL},
+  { NULL,        0, 0, false, false, false, NULL }
+};
+
+static int microblaze_interrupt_function_p (tree);
+static int microblaze_save_volatiles (tree);
+
+#undef TARGET_ENCODE_SECTION_INFO
+#define TARGET_ENCODE_SECTION_INFO      microblaze_encode_section_info
+
+#undef TARGET_ASM_GLOBALIZE_LABEL
+#define TARGET_ASM_GLOBALIZE_LABEL      microblaze_globalize_label
+
+#undef TARGET_UNIQUE_SECTION
+#define TARGET_UNIQUE_SECTION           microblaze_unique_section
+
+#undef TARGET_ASM_FUNCTION_PROLOGUE
+#define TARGET_ASM_FUNCTION_PROLOGUE    microblaze_function_prologue
+
+#undef TARGET_ASM_FUNCTION_EPILOGUE
+#define TARGET_ASM_FUNCTION_EPILOGUE    microblaze_function_epilogue
+
+#undef TARGET_ASM_INTERNAL_LABEL
+#define TARGET_ASM_INTERNAL_LABEL       microblaze_internal_label
+
+#undef TARGET_ASM_FILE_START
+#define TARGET_ASM_FILE_START           microblaze_asm_file_start 
+
+#undef TARGET_ASM_FILE_END
+#define TARGET_ASM_FILE_END             microblaze_asm_file_end
+
+#undef TARGET_RTX_COSTS          
+#define TARGET_RTX_COSTS                microblaze_rtx_costs
+
+#undef TARGET_ADDRESS_COST              
+#define TARGET_ADDRESS_COST             microblaze_address_cost
+
+#undef TARGET_ATTRIBUTE_TABLE           
+#define TARGET_ATTRIBUTE_TABLE          microblaze_attribute_table
+
+#undef TARGET_ASM_CONSTRUCTOR
+#define TARGET_ASM_CONSTRUCTOR          microblaze_asm_constructor
+
+#undef TARGET_ASM_DESTRUCTOR
+#define TARGET_ASM_DESTRUCTOR           microblaze_asm_destructor
+
+#undef TARGET_ASM_SELECT_RTX_SECTION   
+#define TARGET_ASM_SELECT_RTX_SECTION   microblaze_select_rtx_section
+
+#undef TARGET_ASM_SELECT_SECTION
+#define TARGET_ASM_SELECT_SECTION       microblaze_select_section
+
+#undef TARGET_SCHED_USE_DFA_PIPELINE_INTERFACE
+#define TARGET_SCHED_USE_DFA_PIPELINE_INTERFACE \
+                                        microblaze_sched_use_dfa_pipeline_interface
+
+#undef TARGET_ASM_FUNCTION_END_PROLOGUE 
+#define TARGET_ASM_FUNCTION_END_PROLOGUE \
+                                        microblaze_function_end_prologue 
+
+struct gcc_target targetm = (struct gcc_target)TARGET_INITIALIZER;
+
+/* Return truth value of whether OP can be used as an operands in arithmetic */
+
+int
+arith_operand (rtx op, enum machine_mode mode)
+{
+  if (GET_CODE (op) == CONST_INT)
+    return 1;
+
+  return register_operand (op, mode);
+}
+
+/* Return truth value of whether OP is a integer which fits in 16 bits  */
+
+int
+small_int (rtx op, enum machine_mode mode ATTRIBUTE_UNUSED)
+{
+  return (GET_CODE (op) == CONST_INT && SMALL_INT (op));
+}
+
+
+/* Return truth value of whether OP is a register or the constant 0 */
+
+int
+reg_or_0_operand (rtx op, enum machine_mode mode)
+{
+  switch (GET_CODE (op))
+  {
+    case CONST_INT:
+      return INTVAL (op) == 0;
+
+    case CONST_DOUBLE:
+      return op == CONST0_RTX (mode);
+
+    case REG:
+    case SUBREG:
+      return register_operand (op, mode);
+
+    default:
+      break;
+  }
+
+  return 0;
+}
+
+/* Return truth value if a CONST_DOUBLE is ok to be a legitimate constant.  */
+
+int
+microblaze_const_double_ok (rtx op, enum machine_mode mode)
+{
+  REAL_VALUE_TYPE d;
+
+  if (GET_CODE (op) != CONST_DOUBLE)
+    return 0;
+
+  if (mode == VOIDmode)
+    return 1;
+
+  if (mode != SFmode && mode != DFmode)
+    return 0;
+
+  if (op == CONST0_RTX (mode))
+    return 1;
+
+  REAL_VALUE_FROM_CONST_DOUBLE (d, op);
+
+  if (REAL_VALUE_ISNAN (d))
+    return FALSE;
+
+  if (REAL_VALUE_NEGATIVE (d))
+    d = REAL_VALUE_NEGATE (d);
+
+  if (mode == DFmode)
+  {
+    if (REAL_VALUES_LESS (d, dfhigh)
+        && REAL_VALUES_LESS (dflow, d))
+      return 1;
+  }
+  else
+  {
+    if (REAL_VALUES_LESS (d, sfhigh)
+        && REAL_VALUES_LESS (sflow, d))
+      return 1;
+  }
+
+  return 0;
+}
+
+/* Accept the floating point constant 1 in the appropriate mode.  */
+
+int
+const_float_1_operand (rtx op, enum machine_mode mode)
+{
+  REAL_VALUE_TYPE d;
+  static REAL_VALUE_TYPE onedf;
+  static REAL_VALUE_TYPE onesf;
+  static int one_initialized;
+
+  if (GET_CODE (op) != CONST_DOUBLE
+      || mode != GET_MODE (op)
+      || (mode != DFmode && mode != SFmode))
+    return 0;
+
+  REAL_VALUE_FROM_CONST_DOUBLE (d, op);
+
+  if (! one_initialized)
+  {
+    onedf = REAL_VALUE_ATOF ("1.0", DFmode);
+    onesf = REAL_VALUE_ATOF ("1.0", SFmode);
+    one_initialized = 1;
+  }
+
+  if (mode == DFmode)
+    return REAL_VALUES_EQUAL (d, onedf);
+  else
+    return REAL_VALUES_EQUAL (d, onesf);
+}
+
+/* Return truth value if a memory operand fits in a single instruction
+   (ie, register + small offset) or (register + register).  */
+
+int
+simple_memory_operand (rtx op, enum machine_mode mode ATTRIBUTE_UNUSED)
+{
+  rtx addr, plus0, plus1;
+
+  /* Eliminate non-memory operations */
+  if (GET_CODE (op) != MEM)
+    return 0;
+
+  /* dword operations really put out 2 instructions, so eliminate them.  */
+  /* ??? This isn't strictly correct.  It is OK to accept multiword modes
+     here, since the length attributes are being set correctly, but only
+     if the address is offsettable.  */
+  if (GET_MODE_SIZE (GET_MODE (op)) > UNITS_PER_WORD)
+    return 0;
+
+ 
+  /* Decode the address now.  */
+  addr = XEXP (op, 0);
+  switch (GET_CODE (addr))
+
+  {
+    case REG:
+      return 1;
+
+      /*    case CONST_INT:
+            return SMALL_INT (op);
+      */
+    case PLUS:
+      plus0 = XEXP (addr, 0);
+      plus1 = XEXP (addr, 1);
+
+      if (GET_CODE (plus0) == REG && GET_CODE (plus1) == CONST_INT && SMALL_INT (plus1)) {
+        return 1;
+      } else if (GET_CODE (plus1) == REG  && GET_CODE (plus0) == CONST_INT){
+        return 1;
+      } else if (GET_CODE (plus0) == REG  && GET_CODE (plus1) == REG) {
+        return 1;
+      } else 
+        return 0;
+
+    case SYMBOL_REF:
+      return 0;
+
+    default:
+      break;
+  }
+
+  return 0;
+}
+
+/* This predicate is for testing if the operand will definitely need an imm instruction */
+
+int
+imm_required_operand (rtx op, enum machine_mode mode)
+{
+
+  rtx addr,plus0,plus1;
+
+  if (GET_CODE (op) != MEM
+      || ! memory_operand (op, mode))
+  {
+    /* During reload, we accept a pseudo register if it has an
+       appropriate memory address.  If we don't do this, we will
+       wind up reloading into a register, and then reloading that
+       register from memory, when we could just reload directly from
+       memory.  */
+    if (reload_in_progress
+        && GET_CODE (op) == REG
+        && REGNO (op) >= FIRST_PSEUDO_REGISTER
+        && reg_renumber[REGNO (op)] < 0
+        && reg_equiv_mem[REGNO (op)] != 0
+        && double_memory_operand (reg_equiv_mem[REGNO (op)], mode))
+      return 1;
+    return 0;
+  }
+
+  /* Make sure that 4 added to the address is a valid memory address.
+     This essentially just checks for overflow in an added constant.  */
+
+  addr = XEXP (op, 0);
+
+  switch (GET_CODE (addr))
+  {
+    case REG:
+      return 0;
+
+    case CONST_INT:
+      if (!SMALL_INT(addr))
+        return 1;
+      else
+        return 0;
+
+      /*    case CONST_INT:
+            return SMALL_INT (op);
+      */
+    case PLUS:
+      plus0 = XEXP (addr, 0);
+      plus1 = XEXP (addr, 1);
+      if (GET_CODE (plus0) == REG
+          && GET_CODE (plus1) == CONST_INT && SMALL_INT (plus1)) {
+        return 0;
+      }
+      
+
+      else if (GET_CODE (plus1) == REG  && GET_CODE (plus0) == CONST_INT){
+        return 0;
+      }
+      
+
+      else
+        return 1;
+
+    case SYMBOL_REF:
+      return 1;
+
+    default:
+      break;
+  }
+
+  if (CONSTANT_ADDRESS_P (addr))
+    return 1;
+
+  return memory_address_p ((GET_MODE_CLASS (mode) == MODE_INT
+                            ? SImode
+                            : SFmode),
+                           plus_constant(addr, 4)); 
+
+}
+
+/* Return nonzero for a memory address that can be used to load or store
+   a doubleword.  */
+
+int
+double_memory_operand (rtx op, enum machine_mode mode)
+{
+  rtx addr;
+   
+  if (GET_CODE (op) != MEM
+      || ! memory_operand (op, mode))
+  {
+    /* During reload, we accept a pseudo register if it has an
+       appropriate memory address.  If we don't do this, we will
+       wind up reloading into a register, and then reloading that
+       register from memory, when we could just reload directly from
+       memory.  */
+    if (reload_in_progress
+        && GET_CODE (op) == REG
+        && REGNO (op) >= FIRST_PSEUDO_REGISTER
+        && reg_renumber[REGNO (op)] < 0
+        && reg_equiv_mem[REGNO (op)] != 0
+        && double_memory_operand (reg_equiv_mem[REGNO (op)], mode))
+      return 1;
+    return 0;
+  }
+
+  /* Make sure that 4 added to the address is a valid memory address.
+     This essentially just checks for overflow in an added constant.  */
+
+  addr = XEXP (op, 0);
+
+  if (CONSTANT_ADDRESS_P (addr))
+    return 1;
+
+  return memory_address_p ((GET_MODE_CLASS (mode) == MODE_INT
+                            ? SImode
+                            : SFmode),
+                           plus_constant (addr, 4));  
+}
+
+
+/* Return nonzero if the code of this rtx pattern is EQ or NE.  */
+
+int
+equality_op (rtx op, enum machine_mode mode)
+{
+  if (mode != GET_MODE (op))
+    return 0;
+
+  return GET_CODE (op) == EQ || GET_CODE (op) == NE;
+}
+
+int
+lessthan_op (rtx op, enum machine_mode mode)
+{
+  if (mode != GET_MODE (op))
+    return 0;
+
+  return GET_CODE (op) == LT || GET_CODE (op) == LTU;
+}
+
+/* Return nonzero if the code is a relational operations (EQ, LE, etc.) */
+
+int
+cmp_op (rtx op, enum machine_mode mode)
+{
+  if (mode != GET_MODE (op))
+    return 0;
+
+  return GET_RTX_CLASS (GET_CODE (op)) == '<';
+}
+
+/* Returns nonzero, if the operator is unsigned compare op */
+int
+unsigned_cmp_op (rtx op, enum machine_mode mode)
+{
+  if (mode != GET_MODE (op))
+    return 0;
+
+  return ( GET_RTX_CLASS (GET_CODE (op)) == '<'   && 
+           (GET_CODE(op) == GTU ||
+            GET_CODE(op) == LTU ||
+            GET_CODE(op) == LEU ||
+            GET_CODE(op) == GEU));
+}
+
+/* Return nonzero , if the operator is signed compare op */
+/* the check for class might not be necessary */
+int
+signed_cmp_op (rtx op, enum machine_mode mode)
+{
+  if (mode != GET_MODE (op))
+    return 0;
+
+  return ( GET_RTX_CLASS (GET_CODE (op)) == '<'   && 
+           (GET_CODE(op) == GT ||
+            GET_CODE(op) == LT ||
+            GET_CODE(op) == LE ||
+            GET_CODE(op) == GE ||
+            GET_CODE(op) == EQ ||
+            GET_CODE(op) == NE ));
+}
+
+/* Return nonzero if the operand is either the PC or a label_ref.  */
+
+int
+pc_or_label_operand (rtx op, enum machine_mode mode ATTRIBUTE_UNUSED)
+{
+  if (op == pc_rtx)
+    return 1;
+
+  if (GET_CODE (op) == LABEL_REF)
+    return 1;
+
+  if ((GET_CODE (op) == SYMBOL_REF) && !(strcmp ((XSTR (op, 0)), "_stack_overflow_exit")))
+    return 1;
+
+  return 0;
+}
+
+/* Test for a valid operand for a call instruction.
+   Don't allow the arg pointer register or virtual regs
+   since they may change into reg + const, which the patterns
+   can't handle yet.  */
+
+int
+call_insn_operand (rtx op, enum machine_mode mode ATTRIBUTE_UNUSED)
+{
+  return (CONSTANT_ADDRESS_P (op)
+          || (GET_CODE (op) == REG && op != arg_pointer_rtx
+              && ! (REGNO (op) >= FIRST_PSEUDO_REGISTER
+                    && REGNO (op) <= LAST_VIRTUAL_REGISTER)));
+}
+
+/* Return nonzero if OPERAND is valid as a source operand for a move
+   instruction.  */
+
+int
+move_operand (rtx op, enum machine_mode mode)
+{
+  /* Accept any general operand after reload has started; doing so
+     avoids losing if reload does an in-place replacement of a register
+     with a SYMBOL_REF or CONST.  */
+
+  /* FIXME: What is this? Some temporary hack. 
+     Check if we are losing out on optimal code */
+  /* GCC 3.4.1 
+   * Special check added 
+   * Some of the aggressive operations are causing problems 
+   */
+  if ( GET_CODE(op) == PLUS )
+    if (!(( GET_CODE(XEXP (op,0)) == REG ) ^ (GET_CODE (XEXP (op,1)) == REG)))
+      return 0;
+    
+  return (general_operand (op, mode));
+}
+
+
+/* Return nonzero if OPERAND is valid as a source operand for movdi.
+   This accepts not only general_operand, but also sign extended
+   constants and registers.  We need to accept sign extended constants
+   in case a sign extended register which is used in an expression,
+   and is equivalent to a constant, is spilled.  */
+
+int
+movdi_operand (rtx op, enum machine_mode mode)
+{
+  return (general_operand (op, mode));
+}
+
+int 
+immediate32_operand (rtx op, enum machine_mode mode ATTRIBUTE_UNUSED)
+{
+  int temp = INTVAL(op) & 0xffffffff;
+  if ((GET_CODE (op) == CONST_INT) && 
+      (temp == INTVAL(op)))
+    return 1;
+  return 0;
+}
+
+/* This hook is called many times during insn scheduling. If the hook 
+   returns nonzero, the automaton based pipeline description is used 
+   for insn scheduling. Otherwise the traditional pipeline description 
+   is used. The default is usage of the traditional pipeline description. */
+int microblaze_sched_use_dfa_pipeline_interface (void)
+{
+  return microblaze_sched_use_dfa; 
+}
+
+/* Write a message to stderr (for use in macros expanded in files that do not
+   include stdio.h).  */
+
+void
+trace (s, s1, s2)
+    char *s, *s1, *s2;
+{
+    fprintf (stderr, s, s1, s2);
+}
+
+
+/* Implement REG_OK_FOR_BASE_P -and- REG_OK_FOR_INDEX_P  */
+int
+microblaze_regno_ok_for_base_p (int regno, int strict)
+{
+  if (regno >= FIRST_PSEUDO_REGISTER)
+  {
+    if (!strict)
+      return true;
+    regno = reg_renumber[regno];
+  }
+
+  /* These fake registers will be eliminated to either the stack or
+     hard frame pointer, both of which are usually valid base registers.
+     Reload deals with the cases where the eliminated form isn't valid.  */
+  if (regno == ARG_POINTER_REGNUM || regno == FRAME_POINTER_REGNUM)
+    return true;
+
+  return GP_REG_P (regno);
+}
+
+/* Return true if X is a valid base register for the given mode.
+   Allow only hard registers if STRICT.  */
+
+static bool
+microblaze_valid_base_register_p (rtx x, enum machine_mode mode, int strict)
+{
+  if (!strict && GET_CODE (x) == SUBREG)
+    x = SUBREG_REG (x);
+
+  return (GET_CODE (x) == REG
+	  && microblaze_regno_ok_for_base_p (REGNO (x), strict));
+}
+
+/* Return true if X is a valid index register for the given mode.
+   Allow only hard registers if STRICT.  */
+
+static bool
+microblaze_valid_index_register_p (rtx x, enum machine_mode mode, int strict)
+{
+  if (!strict && GET_CODE (x) == SUBREG)
+    x = SUBREG_REG (x);
+
+  return (GET_CODE (x) == REG
+          /* A base register is good enough to be an index register on Microblaze */
+	  && microblaze_regno_ok_for_base_p (REGNO (x), strict));            
+}
+
+/* Return true if X is a valid address for machine mode MODE.  If it is,
+   fill in INFO appropriately.  STRICT is true if we should only accept
+   hard base registers.  
+
+      type                     regA      regB    offset      symbol
+
+   ADDRESS_INVALID             NULL      NULL     NULL        NULL
+
+   ADDRESS_REG                 %0        NULL     const_0 /   NULL
+                                                  const_int
+   ADDRESS_REG_INDEX           %0        %1       NULL        NULL
+
+   ADDRESS_SYMBOLIC            r0 /      NULL     NULL        symbol    
+                           sda_base_reg 
+
+   ADDRESS_CONST_INT           r0       NULL      const       NULL
+
+   For modes spanning multiple registers (DFmode in 32-bit GPRs,
+   DImode, TImode), indexed addressing cannot be used because
+   adjacent memory cells are accessed by adding word-sized offsets
+   during assembly output.  */
+
+static bool
+microblaze_classify_address (struct microblaze_address_info *info, rtx x,
+                             enum machine_mode mode, int strict)
+{
+  rtx xplus0;
+  rtx xplus1;
+    
+  info->type = ADDRESS_INVALID;
+  info->regA = NULL;
+  info->regB = NULL;
+  info->offset = NULL;
+  info->symbol = NULL;
+  info->symbol_type = SYMBOL_TYPE_INVALID;
+
+  switch (GET_CODE (x))
+  {
+    case REG:
+    case SUBREG:
+    {
+      info->type = ADDRESS_REG;
+      info->regA = x;
+      info->offset = const0_rtx;
+      return microblaze_valid_base_register_p (info->regA, mode, strict);
+    }   
+    case PLUS:
+    {
+      xplus0 = XEXP (x, 0);				
+      xplus1 = XEXP (x, 1);				
+            
+      if (microblaze_valid_base_register_p (xplus0, mode, strict)) {								
+        info->type = ADDRESS_REG;
+        info->regA = xplus0;
+
+        if (GET_CODE (xplus1) == CONST_INT) {
+          info->offset = xplus1;
+          return true;							
+        } else if (GET_CODE (xplus1) == SYMBOL_REF || 
+                   GET_CODE (xplus1) == LABEL_REF  || 
+                   GET_CODE (xplus1) == CONST) {                   
+/*           if ((GET_CODE (xplus1) == SYMBOL_REF) &&                  /\* Prevent small data symbols from being accessed off an incompatible base register *\/ */
+/*               ((get_base_reg (xplus1) == MB_ABI_BASE_REGNUM)  */
+/*                ? 0  */
+/*                : get_base_reg (xplus1) != REGNO (info->regA))) */
+/*             return false; */
+          info->type = ADDRESS_SYMBOLIC;
+          info->symbol = xplus1;
+          /* info->regA =  gen_rtx_raw_REG (mode, get_base_reg (xplus1)); */
+          info->symbol_type = SYMBOL_TYPE_GENERAL;
+          return true;
+        } else if (GET_CODE (xplus1) == REG && microblaze_valid_index_register_p (xplus1, mode, strict) &&
+                   (GET_MODE_SIZE (mode) <= UNITS_PER_WORD)) {      /* Restrict larger than word-width modes from using an index register */
+          info->type = ADDRESS_REG_INDEX;
+          info->regB = xplus1;
+          return true;
+        }
+      }
+      break;
+    }
+    case CONST_INT:
+    {
+      info->regA = gen_rtx_raw_REG (mode, 0);
+      info->type = ADDRESS_CONST_INT;
+      info->offset = x;
+      return true;
+    }    
+    case CONST:
+    case LABEL_REF:
+    case SYMBOL_REF:
+    {
+      info->type = ADDRESS_SYMBOLIC;
+      info->symbol_type = SYMBOL_TYPE_GENERAL;
+      info->symbol = x;
+      info->regA =  gen_rtx_raw_REG (mode, get_base_reg (x));
+      if (HALF_PIC_P () || HALF_PIC_ADDRESS_P (x))
+        return false;
+            
+      if (GET_CODE (x) == CONST) {
+        return !(flag_pic && pic_address_needs_scratch (x));
+      }
+
+      return true;           
+    }
+    default:
+      return false;
+  }
+    
+  return false;
+}
+
+/* This function is used to implement GO_IF_LEGITIMATE_ADDRESS.  It
+   returns a nonzero value if X is a legitimate address for a memory
+   operand of the indicated MODE.  STRICT is nonzero if this function
+   is called during reload.  */
+
+bool
+microblaze_legitimate_address_p (enum machine_mode mode, rtx x, int strict)
+{
+  struct microblaze_address_info addr;
+
+  return microblaze_classify_address (&addr, x, mode, strict);
+}
+
+
+/* Try machine-dependent ways of modifying an illegitimate address
+   to be legitimate.  If we find one, return the new, valid address.
+   This is used from only one place: `memory_address' in explow.c.
+
+   OLDX is the address as it was before break_out_memory_refs was
+   called.  In some cases it is useful to look at this to decide what
+   needs to be done.
+
+   MODE is passed so that this function can use GO_IF_LEGITIMATE_ADDRESS.
+
+   It is always safe for this function to do nothing.  It exists to
+   recognize opportunities to optimize the output.
+
+   For the MICROBLAZE, transform:
+
+   memory(X + <large int>)
+
+   into:
+
+   Y = <large int> & ~0x7fff;
+   Z = X + Y
+   memory (Z + (<large int> & 0x7fff));
+
+   This is for CSE to find several similar references, and only use one Z.
+
+   When PIC, convert addresses of the form memory (symbol+large int) to
+   memory (reg+large int).  */
+rtx
+microblaze_legitimize_address (rtx x, rtx oldx ATTRIBUTE_UNUSED,
+                               enum machine_mode mode)
+{									
+  register rtx xinsn = x, result;						
+									
+  if (TARGET_DEBUG_B_MODE)						
+  {									
+    GO_PRINTF ("\n========== LEGITIMIZE_ADDRESS\n");			
+    GO_DEBUG_RTX (xinsn);						
+  }									
+									
+  if (GET_CODE (xinsn) == CONST						
+      && flag_pic && pic_address_needs_scratch (xinsn))		        
+  {									
+    rtx ptr_reg = gen_reg_rtx (Pmode);				
+    rtx constant = XEXP (XEXP (xinsn, 0), 1);				
+									
+    emit_move_insn (ptr_reg, XEXP (XEXP (xinsn, 0), 0));		
+									
+    result = gen_rtx (PLUS, Pmode, ptr_reg, constant);			
+    if (SMALL_INT (constant))						
+      return result;							
+    /* Otherwise we fall through so the code below will fix the	
+       constant.  */							
+    xinsn = result;							
+  }									
+									
+  if (GET_CODE (xinsn) == PLUS)						
+  {									
+    register rtx xplus0 = XEXP (xinsn, 0);				
+    register rtx xplus1 = XEXP (xinsn, 1);				
+    register enum rtx_code code0 = GET_CODE (xplus0);			
+    register enum rtx_code code1 = GET_CODE (xplus1);			
+									
+    if (code0 != REG && code1 == REG)					
+    {								
+      xplus0 = XEXP (xinsn, 1);					
+      xplus1 = XEXP (xinsn, 0);					
+      code0 = GET_CODE (xplus0);					
+      code1 = GET_CODE (xplus1);					
+    }								
+									
+    if (code0 == REG && REG_OK_FOR_BASE_P (xplus0)		        
+        && code1 == CONST_INT && !SMALL_INT (xplus1))			
+    {								
+      rtx int_reg = gen_reg_rtx (Pmode);				
+      rtx ptr_reg = gen_reg_rtx (Pmode);				
+									
+      emit_move_insn (int_reg,					
+                      GEN_INT (INTVAL (xplus1) & ~ 0x7fff));	
+									
+      emit_insn (gen_rtx (SET, VOIDmode,				
+                          ptr_reg,					
+                          gen_rtx (PLUS, Pmode, xplus0, int_reg)));	
+									
+      result = gen_rtx (PLUS, Pmode, ptr_reg,				
+                        GEN_INT (INTVAL (xplus1) & 0x7fff));		
+      return result;							
+    }								
+  }									
+									
+  if (TARGET_DEBUG_B_MODE)						
+    GO_PRINTF ("LEGITIMIZE_ADDRESS could not fix.\n");			
+
+  return NULL_RTX;
+}
+
+
+/* Returns an operand string for the given instruction's delay slot,
+   after updating filled delay slot statistics.
+
+   We assume that operands[0] is the target register that is set.
+
+   In order to check the next insn, most of this functionality is moved
+   to FINAL_PRESCAN_INSN, and we just set the global variables that
+   it needs.  */
+
+/* ??? This function no longer does anything useful, because final_prescan_insn
+   now will never emit a nop.  */
+
+char *
+microblaze_fill_delay_slot (ret, type, operands, cur_insn)
+  char *ret;			/* normal string to return */
+  enum delay_type type;	/* type of delay */
+  rtx operands[];		/* operands to use */
+  rtx cur_insn;		/* current insn */
+{
+  register rtx set_reg;
+  register enum machine_mode mode;
+  register rtx next_insn = cur_insn ? NEXT_INSN (cur_insn) : NULL_RTX;
+  register int num_nops;
+
+  if (type == DELAY_LOAD || type == DELAY_FCMP)
+    num_nops = 1;
+
+  else if (type == DELAY_HILO)
+    num_nops = 2;
+
+  else
+    num_nops = 0;
+
+  /* Make sure that we don't put nop's after labels.  */
+  next_insn = NEXT_INSN (cur_insn);
+  while (next_insn != 0 && GET_CODE (next_insn) == NOTE)
+    next_insn = NEXT_INSN (next_insn);
+
+  dslots_load_total += num_nops;
+  if (TARGET_DEBUG_F_MODE
+      || !optimize
+      || type == DELAY_NONE
+      || operands == 0
+      || cur_insn == 0
+      || next_insn == 0
+      || GET_CODE (next_insn) == CODE_LABEL
+      || (set_reg = operands[0]) == 0)
+  {
+    dslots_number_nops = 0;
+    microblaze_load_reg  = 0;
+    microblaze_load_reg2 = 0;
+    microblaze_load_reg3 = 0;
+    microblaze_load_reg4 = 0;
+    return ret;
+  }
+
+  set_reg = operands[0];
+  if (set_reg == 0)
+    return ret;
+
+  while (GET_CODE (set_reg) == SUBREG)
+    set_reg = SUBREG_REG (set_reg);
+
+  mode = GET_MODE (set_reg);
+  dslots_number_nops = num_nops;
+  microblaze_load_reg = set_reg;
+  if (GET_MODE_SIZE (mode)
+      > (FP_REG_P (REGNO (set_reg)) ? UNITS_PER_FPREG : UNITS_PER_WORD))
+    microblaze_load_reg2 = gen_rtx (REG, SImode, REGNO (set_reg) + 1);
+  else
+    microblaze_load_reg2 = 0;
+
+  if (type == DELAY_HILO)
+  {
+    microblaze_load_reg3 = gen_rtx (REG, SImode, MD_REG_FIRST);
+    microblaze_load_reg4 = gen_rtx (REG, SImode, MD_REG_FIRST+1);
+  }
+  else
+  {
+    microblaze_load_reg3 = 0;
+    microblaze_load_reg4 = 0;
+  }
+
+  return ret;
+}
+
+
+/* Determine whether a memory reference takes one (based off of the GP
+   pointer), two (normal), or three (label + reg) instructions, and bump the
+   appropriate counter for -mstats.  */
+
+void
+microblaze_count_memory_refs (op, num)
+  rtx op;
+  int num;
+{
+  int additional = 0;
+  int n_words = 0;
+  rtx addr, plus0, plus1;
+  enum rtx_code code0, code1;
+  int looping;
+
+  if (TARGET_DEBUG_B_MODE)
+  {
+    fprintf (stderr, "\n========== microblaze_count_memory_refs:\n");
+    debug_rtx (op);
+  }
+
+  /* Skip MEM if passed, otherwise handle movsi of address.  */
+  addr = (GET_CODE (op) != MEM) ? op : XEXP (op, 0);
+
+  /* Loop, going through the address RTL.  */
+  do
+  {
+    looping = FALSE;
+    switch (GET_CODE (addr))
+    {
+      case REG:
+      case CONST_INT:
+        break;
+
+      case PLUS:
+        plus0 = XEXP (addr, 0);
+        plus1 = XEXP (addr, 1);
+        code0 = GET_CODE (plus0);
+        code1 = GET_CODE (plus1);
+
+        if (code0 == REG)
+        {
+          additional++;
+          addr = plus1;
+          looping = 1;
+          continue;
+        }
+
+        if (code0 == CONST_INT)
+        {
+          addr = plus1;
+          looping = 1;
+          continue;
+        }
+
+        if (code1 == REG)
+        {
+          additional++;
+          addr = plus0;
+          looping = 1;
+          continue;
+        }
+
+        if (code1 == CONST_INT)
+        {
+          addr = plus0;
+          looping = 1;
+          continue;
+        }
+
+        if (code0 == SYMBOL_REF || code0 == LABEL_REF || code0 == CONST)
+        {
+          addr = plus0;
+          looping = 1;
+          continue;
+        }
+
+        if (code1 == SYMBOL_REF || code1 == LABEL_REF || code1 == CONST)
+        {
+          addr = plus1;
+          looping = 1;
+          continue;
+        }
+
+        break;
+
+      case LABEL_REF:
+        n_words = 2;		/* always 2 words */
+        break;
+
+      case CONST:
+        addr = XEXP (addr, 0);
+        looping = 1;
+        continue;
+
+      case SYMBOL_REF:
+        n_words = SYMBOL_REF_FLAG (addr) ? 1 : 2;
+        break;
+
+      default:
+        break;
+    }
+  }
+  while (looping);
+
+  if (n_words == 0)
+    return;
+
+  n_words += additional;
+  if (n_words > 3)
+    n_words = 3;
+
+  num_refs[n_words-1] += num;
+}
+
+
+rtx
+embedded_pic_fnaddr_reg ()
+{
+#if 0
+  if (cfun->machine->embedded_pic_fnaddr_rtx == NULL)
+  {
+    rtx seq;
+
+    cfun->machine->embedded_pic_fnaddr_rtx = gen_reg_rtx (Pmode);
+
+    /* Output code at function start to initialize the pseudo-reg.  */
+    /* ??? We used to do this in FINALIZE_PIC, but that does not work for
+       inline functions, because it is called after RTL for the function
+       has been copied.  The pseudo-reg in embedded_pic_fnaddr_rtx however
+       does not get copied, and ends up not matching the rest of the RTL.
+       This solution works, but means that we get unnecessary code to
+       initialize this value every time a function is inlined into another
+       function.  */
+    start_sequence ();
+    emit_insn (gen_get_fnaddr (cfun->machine->embedded_pic_fnaddr_rtx,
+                               XEXP (DECL_RTL (current_function_decl), 0)));
+    seq = get_insns ();
+    end_sequence ();
+    push_topmost_sequence ();
+    emit_insn_after (seq, get_insns ());
+    pop_topmost_sequence ();
+  }
+
+  return cfun->machine->embedded_pic_fnaddr_rtx;
+#endif
+  /* Returning NULL Pointer. Not a safe way to do this */
+  return NULL;
+}
+
+/* Return RTL for the offset from the current function to the argument.
+   X is the symbol whose offset from the current function we want.  */
+
+/* Looks like this is not called anywhere */
+rtx
+embedded_pic_offset (x)
+  rtx x;
+{
+  /* Make sure it is emitted.  */
+  embedded_pic_fnaddr_reg ();
+
+  return
+    gen_rtx_CONST (Pmode,
+                   gen_rtx_MINUS (Pmode, x,
+                                  XEXP (DECL_RTL (current_function_decl), 0)));
+}
+
+
+char* 
+microblaze_mode_to_mem_modifier (int load, enum machine_mode mode)
+{
+  switch (mode) {
+
+    case QImode:
+      if (load)
+        return "bu";
+      else
+        return "b";
+    case HImode:
+      if (load)
+        return "hu";
+      else
+        return "h";
+    default:
+      return "w";
+  }
+}
+
+/* Return the appropriate instructions to move one operand to another.  */
+
+const char *
+microblaze_move_1word (operands, insn, unsignedp)
+  rtx operands[];
+  rtx insn;
+  int unsignedp;
+{
+  char *ret=0; 
+  rtx op0 = operands[0];
+  rtx op1 = operands[1];
+  enum rtx_code code0 = GET_CODE (op0);
+  enum rtx_code code1 = GET_CODE (op1);
+  enum machine_mode mode0 = GET_MODE (op0);
+  enum machine_mode mode1 = GET_MODE (op1);
+  int subreg_word0 = 0;
+  int subreg_word1 = 0;
+  enum delay_type delay = DELAY_NONE;
+  int i;
+  int empty_ret = 0;
+
+  ret = (char*)xmalloc(100);                                                          /* Allocate 50 bytes for the assembly instruction */
+   
+  for(i = 0 ; i < 100; i++) ret[i]='\0';
+
+  while (code0 == SUBREG)
+  {
+    subreg_word0 += subreg_regno_offset (REGNO (SUBREG_REG (op0)),
+                                         GET_MODE (SUBREG_REG (op0)),
+                                         SUBREG_BYTE (op0),
+                                         GET_MODE (op0));
+    op0 = SUBREG_REG (op0);
+    code0 = GET_CODE (op0);
+  }
+
+  while (code1 == SUBREG)
+  {
+    subreg_word1 += subreg_regno_offset (REGNO (SUBREG_REG (op1)),
+                                         GET_MODE (SUBREG_REG (op1)),
+                                         SUBREG_BYTE (op1),
+                                         GET_MODE (op1));
+    op1 = SUBREG_REG (op1);
+    code1 = GET_CODE (op1);
+  }
+
+  /* For our purposes, a condition code mode is the same as SImode.  */
+  if (mode0 == CCmode)
+    mode0 = SImode;
+
+  if (code0 == REG) {
+    int regno0 = REGNO (op0) + subreg_word0;
+
+    if (code1 == REG) {
+      int regno1 = REGNO (op1) + subreg_word1;
+
+      /* Just in case, don't do anything for assigning a register
+         to itself, unless we are filling a delay slot.  */
+      if (regno0 == regno1 && set_nomacro == 0){
+        empty_ret = 1;
+        ret = "";
+      }
+	  
+      else if (GP_REG_P (regno0) && GP_REG_P (regno1) )
+        ret = "addk\t%0,%1,r0";
+      else if (GP_REG_P (regno0) && ST_REG_P (regno1))
+        ret = "mfs\t%0,%1";
+      else if (ST_REG_P (regno0) && GP_REG_P (regno1))
+        ret = "mts\t%0,%1";
+    } else if (code1 == MEM) {
+      rtx offset = const0_rtx;
+      op1 = eliminate_constant_term (XEXP (op1, 0), &offset);
+          
+      delay = DELAY_NONE;
+
+      if (TARGET_STATS)
+        microblaze_count_memory_refs (op1, 1);
+
+      if (GP_REG_P (regno0))
+      {
+        struct microblaze_address_info info;
+        if (!microblaze_classify_address (&info, XEXP (operands[1], 0), GET_MODE (operands[1]), 1)) 
+          fatal_insn ("insn contains an invalid address !", insn);
+
+        switch (info.type) {
+          case ADDRESS_REG:
+            if ((GET_CODE (info.offset) == CONST_INT)) 
+              sprintf (ret, "l%si\t%%0,%%1", microblaze_mode_to_mem_modifier (1, GET_MODE (operands[1])));
+            break;
+          case ADDRESS_REG_INDEX:
+            sprintf (ret, "l%s\t%%0,%%1", microblaze_mode_to_mem_modifier (1, GET_MODE (operands[1])));
+            break;
+          case ADDRESS_CONST_INT:
+          case ADDRESS_SYMBOLIC:
+            sprintf (ret, "l%si\t%%0,%%1", microblaze_mode_to_mem_modifier (1, GET_MODE (operands[1])));
+            break;
+        }
+        return ret;
+      }
+    } else if ((code1 == CONST_INT) || 
+               (code1 == CONST_DOUBLE && GET_MODE (op1) == VOIDmode)) {
+      if (code1 == CONST_DOUBLE) {
+        /* This can happen when storing constants into long long
+           bitfields.  Just store the least significant word of
+           the value.  */
+        operands[1] = op1 = GEN_INT (CONST_DOUBLE_LOW (op1));
+      }
+
+      if (INTVAL (op1) == 0 )
+      {
+        if (GP_REG_P (regno0))
+          ret = "addk\t%0,r0,%z1";
+      } else if (GP_REG_P (regno0))
+      {
+        ret = "addik\t%0,r0,%1\t# %X1";
+      }
+    }
+    /* Sid [07/16/01] Need to get DOUBLE and FLOATS out . Instead have either normal moves or function calls */
+    else if (code1 == CONST_DOUBLE && mode0 == SFmode)
+    {
+      if (op1 == CONST0_RTX (SFmode))
+      {
+        if (GP_REG_P (regno0))
+          ret = "addk\t%0,r0,%.";
+      }
+
+      else
+      {
+        delay = DELAY_NONE;
+        {
+          unsigned int value_long;
+          int i;
+          REAL_VALUE_TYPE value;
+
+          REAL_VALUE_FROM_CONST_DOUBLE(value,operands[1]);
+          REAL_VALUE_TO_TARGET_SINGLE (value, value_long);
+		
+          ret = (char*)xmalloc(30); 
+                    
+          for(i = 0; i < 30; i++) 
+            ret[i] = 0;
+
+          sprintf (ret, "addik\t%%0,r0,0x%x", value_long);
+        }
+	      
+      }
+    }
+
+    else if (code1 == LABEL_REF)
+    {
+
+      int base_reg = get_base_reg (XEXP (operands[1], 0));
+      
+      if (TARGET_STATS)
+        microblaze_count_memory_refs (op1, 1);
+      
+      ret = (char*)xmalloc (strlen ("addik\t%%0,r13,%%a1")); /* need r2 as the reg, but use r13 to get more space */
+      sprintf (ret, "addik\t%%0,%%a1");
+    }
+
+    else if (code1 == SYMBOL_REF || code1 == CONST)
+    {
+      if (HALF_PIC_P () && CONSTANT_P (op1) && HALF_PIC_ADDRESS_P (op1))
+      {
+        rtx offset = const0_rtx;
+
+        if (GET_CODE (op1) == CONST)
+          op1 = eliminate_constant_term (XEXP (op1, 0), &offset);
+
+        if (GET_CODE (op1) == SYMBOL_REF)
+        {
+          operands[2] = HALF_PIC_PTR (op1);
+
+          if (TARGET_STATS)
+            microblaze_count_memory_refs (operands[2], 1);
+
+          if (INTVAL (offset) == 0)
+          {
+            /*		      delay = DELAY_LOAD;*/
+            delay = DELAY_NONE;
+            ret = "lw\t%0,%2";
+          }
+          else
+          {
+            /* XLNX [Check out]*/
+            dslots_load_total++;
+            operands[3] = offset;
+            ret = "lwi\t%0,%2,0\n\tadd\t%0,%0,%3";
+          }
+        }
+      }
+      else
+      {
+        int base_reg = get_base_reg (operands[1]);
+        sprintf (ret,"addik\t%%0,%%a1");
+        
+        if (TARGET_STATS)
+          microblaze_count_memory_refs (op1, 1);
+      }
+    }
+
+    else if (code1 == PLUS)
+    {
+      rtx add_op0 = XEXP (op1, 0);
+      rtx add_op1 = XEXP (op1, 1);
+
+      if (GET_CODE (XEXP (op1, 1)) == REG
+          && GET_CODE (XEXP (op1, 0)) == CONST_INT)
+        add_op0 = XEXP (op1, 1), add_op1 = XEXP (op1, 0);
+
+      operands[2] = add_op0;
+      operands[3] = add_op1;
+      ret = "addk%:\t%0,%2,%3";
+    }
+
+    else if (code1 == HIGH)
+    {
+      operands[1] = XEXP (op1, 0);
+      ret = "lui\t%0,%%hi(%1)";
+    }
+  }
+
+  else if (code0 == MEM)
+  {
+        
+    struct microblaze_address_info info;
+    if (!microblaze_classify_address (&info, XEXP (operands[0], 0), GET_MODE (operands[0]), 1)) 
+      fatal_insn ("insn contains an invalid addresss !", insn);
+        
+    switch (info.type) {
+      case ADDRESS_REG:
+          sprintf (ret, "s%si\t%%z1,%%0", microblaze_mode_to_mem_modifier (0, GET_MODE (operands[0])));
+        break;
+      case ADDRESS_REG_INDEX:
+        sprintf (ret, "s%s\t%%z1,%%0", microblaze_mode_to_mem_modifier (0, GET_MODE (operands[0])));
+        break;
+      case ADDRESS_CONST_INT:
+      case ADDRESS_SYMBOLIC:
+        sprintf (ret, "s%si\t%%z1,%%0", microblaze_mode_to_mem_modifier (0, GET_MODE (operands[0])));
+        break;
+    }
+    return ret;
+  }
+
+  if (ret == 0)
+  {
+    fatal_insn ("Bad move", insn);
+    return 0;
+  }
+  else
+    if (!empty_ret && strlen(ret) == 0){
+      fatal_insn ("Bad move", insn);
+      return 0;
+    }
+
+  if (delay != DELAY_NONE)
+    return microblaze_fill_delay_slot (ret, delay, operands, insn);
+
+  return ret;
+}
+
+
+/* Return the appropriate instructions to move 2 words */
+
+const char*
+microblaze_move_2words (operands, insn)
+  rtx operands[];
+  rtx insn;
+{
+  char *ret = 0;
+  rtx op0 = operands[0];
+  rtx op1 = operands[1];
+  enum rtx_code code0 = GET_CODE (operands[0]);
+  enum rtx_code code1 = GET_CODE (operands[1]);
+  int subreg_word0 = 0;
+  int subreg_word1 = 0;
+  enum delay_type delay = DELAY_NONE;
+
+  while (code0 == SUBREG)
+  {
+    subreg_word0 += SUBREG_REG (op0);
+    op0 = SUBREG_REG (op0);
+    code0 = GET_CODE (op0);
+  }
+
+  if (code1 == SIGN_EXTEND)
+  {
+    op1 = XEXP (op1, 0);
+    code1 = GET_CODE (op1);
+  }
+
+  while (code1 == SUBREG)
+  {
+    subreg_word1 += SUBREG_REG (op1);
+    op1 = SUBREG_REG (op1);
+    code1 = GET_CODE (op1);
+  }
+      
+  /* Sanity check.  */
+  if (GET_CODE (operands[1]) == SIGN_EXTEND
+      && code1 != REG
+      && code1 != CONST_INT
+      /* The following three can happen as the result of a questionable
+         cast.  */
+      && code1 != LABEL_REF
+      && code1 != SYMBOL_REF
+      && code1 != CONST)
+    abort ();
+
+  if (code0 == REG)
+  {
+    int regno0 = REGNO (op0) + subreg_word0;
+
+    if (code1 == REG)
+    {
+      int regno1 = REGNO (op1) + subreg_word1;
+
+      /* Just in case, don't do anything for assigning a register
+         to itself, unless we are filling a delay slot.  */
+      if (regno0 == regno1 && set_nomacro == 0)
+        ret = "";
+      else if (regno0 != (regno1+1))
+        ret = "addk\t%0,r0,%1\n\taddk\t%D0,r0,%D1";
+
+      else
+        ret = "addk\t%D0,r0,%D1\n\taddk\t%0,r0,%1";
+    }
+
+    else if (code1 == CONST_DOUBLE)
+    {
+      /* Move zero from $0 unless recipient
+         is 64-bit fp reg, in which case generate a constant.  */
+      if (op1 != CONST0_RTX (GET_MODE (op1)))
+      {
+        if (GET_MODE (op1) == DFmode)
+        {
+          /*		  delay = DELAY_LOAD;*/
+          delay = DELAY_NONE;
+
+#ifdef TARGET_FP_CALL_32
+          if (FP_CALL_GP_REG_P (regno0))
+          {
+            ret = "li.d\t%0,%1\n\tdsll\t%D0,%0,32\n\tdsrl\t%D0,32\n\tdsrl\t%0,32";
+          }
+          else
+#endif
+          {
+            unsigned int value_long[2];
+            int i;
+            REAL_VALUE_TYPE value;
+            REAL_VALUE_FROM_CONST_DOUBLE(value,operands[1]);
+            REAL_VALUE_TO_TARGET_DOUBLE (value, value_long);
+
+            ret = (char*)xmalloc (80); 
+            for (i = 0; i < 80; i++) 
+              ret[i] = 0;
+            sprintf (ret, "addik\t%%0,r0,0x%x \n\taddik\t%%D0,r0,0x%x #Xfer Lo", value_long[0], value_long[1]);
+            printed = 1;
+          }
+        }
+
+        else
+        {
+          split_double (op1, operands + 2, operands + 3);
+          /*		  ret = "MICROBLAZEli\t%0,%2\n\tMICROBLAZEli\t%D0,%3 #li1";*/
+          /*   	          fprintf(stderr,"li ==> la\n");*/
+          ret="addik\t%0,r0,%2\n\taddik\t%D0,r0,%3 #li => la";
+        }
+      }
+
+      else
+      {
+        if (GP_REG_P (regno0))
+          ret = "addk\t%0,r0,%.\n\taddk\t%D0,r0,%.";
+      }
+    }
+
+    else if (code1 == CONST_INT && INTVAL (op1) == 0 )
+    {
+      if (GP_REG_P (regno0))
+        ret = "addk\t%0,r0,%.\n\taddk\t%D0,r0,%.";
+    }
+	
+    else if (code1 == CONST_INT && GET_MODE (op0) == DImode
+             && GP_REG_P (regno0)){
+      if (HOST_BITS_PER_WIDE_INT < 64) {
+        operands[2] = GEN_INT (INTVAL (operands[1]) >= 0 ? 0 : -1);
+        /*	      ret = "MICROBLAZEli\t%M0,%2\n\tMICROBLAZEli\t%L0,%1 #li7";*/
+        ret = "addik\t%M0,r0,%2\n\taddik\t%L0,r0,%1";
+      }
+      else {
+        /* We use multiple shifts here, to avoid warnings about out
+           of range shifts on 32 bit hosts.  */
+        operands[2] = GEN_INT (INTVAL (operands[1]) >> 16 >> 16);
+        operands[1]
+          = GEN_INT (INTVAL (operands[1]) << 16 << 16 >> 16 >> 16);
+        /*ret = "MICROBLAZEli\t%M0,%2\n\tMICROBLAZEli\t%L0,%1 #li8";*/
+        ret = "addik\t%M0,r0,%2\n\taddik\t%L0,r0,%1";
+      }
+    }
+
+    else if (code1 == MEM){
+      /*	  delay = DELAY_LOAD;*/
+      delay = DELAY_NONE;
+
+      if (TARGET_STATS)
+        microblaze_count_memory_refs (op1, 2);
+
+      if (double_memory_operand (op1, GET_MODE (op1))) {
+        operands[2] = adjust_address (op1, GET_MODE(op1),4);
+            
+        /* if operands[1] is REG or op1 = MEM, which points to REG */
+        if (GET_CODE(op1) == REG || GET_CODE (XEXP (op1,0)) == REG)
+          ret = (reg_mentioned_p (op0, op1)
+                 ? "lwi\t%D0,%2\n\tlwi\t%0,%1"
+                 : "lwi\t%0,%1\n\tlwi\t%D0,%2");
+        else
+          if (GET_CODE(op1) == SYMBOL_REF || GET_CODE (XEXP (op1, 0)) == SYMBOL_REF) {
+            int i;
+            int ret_reg;
+            free(ret);
+            ret = (char*) xmalloc(100);
+            for(i = 0 ; i < 100; i++)ret[i] = '\0';
+            if (GET_CODE(op1) == SYMBOL_REF)
+              ret_reg = get_base_reg(op1);
+            else
+              ret_reg = get_base_reg (XEXP (op1,0));
+            
+            if (reg_mentioned_p (op0, op1)){
+              sprintf (ret,"lwi\t%%D0,%%2\n\tlwi\t%%0,%%1");
+            }
+            else
+              sprintf (ret,"lwi\t%%0,%%1\n\tlwi\t%%D0,%%2 #MICROBLAZE-CHECK");
+          }
+          else if (GET_CODE(op1) == CONST || GET_CODE (XEXP (op1,0)) == CONST || GET_CODE (XEXP (op1, 0)) == CONST_INT) {
+            ret = (reg_mentioned_p (op0, op1)
+                   ? "lwi\t%D0,%2\n\tlwi\t%0,%1"
+                   : "lwi\t%0,%1\n\tlwi\t%D0,%2");
+          }
+          else {
+            ret = (reg_mentioned_p (op0, op1)
+                   ? "lwi\t%D0,%2\n\tlwi\t%0,%1"
+                   : "lwi\t%0,%1\n\tlwi\t%D0,%2");
+          }
+      }
+    }
+      
+    else if (code1 == LABEL_REF)
+    {
+      if (TARGET_STATS)
+        microblaze_count_memory_refs (op1, 2);
+    }
+    else if (code1 == SYMBOL_REF || code1 == CONST)
+    {
+      if (TARGET_STATS)
+        microblaze_count_memory_refs (op1, 2);
+    }
+  }
+
+  else if (code0 == MEM)
+  {
+    if (code1 == REG)
+    {
+      int regno1 = REGNO (op1) + subreg_word1;
+
+      if (FP_REG_P (regno1))
+        ret = "s.d\t%1,%0";
+
+      else if (double_memory_operand (op0, GET_MODE (op0)))
+      {
+        int op0_base_reg = get_base_reg(op0);
+        int i;
+        operands[2] = adjust_address(op0,GET_MODE (op0), 4);
+	      
+        ret = (char*)xmalloc(100); 
+        for (i = 0; i <  100; i++) ret[i] = 0;
+
+        /* if operands[0] happens to be plus, we shdn't add r0 to the resulting output */
+        /* Check this code properly..Seems to be complicated */
+        if (GET_CODE (XEXP (operands[0], 0)) == PLUS) 
+          if (GET_CODE (XEXP (operands[2], 0)) == PLUS)
+            ret = "swi\t%1,%0\n\tswi\t%D1,%2";
+          else
+            ret = "swi\t%1,%0\n\tswi\t%D1,%2";
+        else
+          if ((GET_CODE (XEXP (operands[0], 0)) == SYMBOL_REF) ||
+              (GET_CODE (XEXP (operands[0], 0)) == CONST)) {
+            sprintf (ret, "swi\t%%1,%%0\n\tswi\t%%D1,%%2");
+          }
+          else
+            if (GET_CODE (XEXP (operands[2], 0)) == PLUS)
+              ret = "swi\t%1,%0\n\tswi\t%D1,%2";
+            else
+              ret = "swi\t%1,%0\n\tswi\t%D1,%2";
+
+      }
+    }
+
+    else if (((code1 == CONST_INT && INTVAL (op1) == 0)
+              || (code1 == CONST_DOUBLE
+                  && op1 == CONST0_RTX (GET_MODE (op1))))
+             && (double_memory_operand (op0, GET_MODE (op0))))
+    {
+      operands[2] = adjust_address(op0, GET_MODE(op0),4);
+      /*         operands[2] = adj_offsettable_operand (op0, 4);*/
+      ret = "swi\t%.,%0\n\tswi\t%.,%2";
+    }
+
+    if (TARGET_STATS)
+      microblaze_count_memory_refs (op0, 2);
+  }
+
+  if (ret == 0)
+  {
+    fatal_insn ("Bad move", insn);
+    return 0;
+  }
+
+  if (delay != DELAY_NONE)
+    return microblaze_fill_delay_slot (ret, delay, operands, insn);
+
+  return ret;
+}
+
+
+bool 
+microblaze_rtx_costs (x, code, outer_code, total)
+  rtx x;
+  int code;
+  int outer_code;
+  int *total;
+{
+  enum machine_mode mode = GET_MODE (x);
+
+  switch (code) {
+    case MEM:							
+    {									
+      int num_words = (GET_MODE_SIZE (mode) > UNITS_PER_WORD) ? 2 : 1; 
+      if (simple_memory_operand (x, mode)) 
+        *total = COSTS_N_INSNS (2 * num_words);			
+      else
+        *total = COSTS_N_INSNS (2 * (2 * num_words));				
+        
+      return true;
+    }									       
+    case NOT: 
+    {
+      if (mode == DImode) {
+        *total = COSTS_N_INSNS (2);
+      } else 
+        *total = COSTS_N_INSNS (1);
+      return false;
+    }   
+    case AND:								
+    case IOR:								
+    case XOR:								
+    {
+      if (mode == DImode) {
+        *total = COSTS_N_INSNS (2);						
+      } else 
+        *total = COSTS_N_INSNS (1);
+
+      return false;
+    }
+    case ASHIFT:								
+    case ASHIFTRT:							
+    case LSHIFTRT:	
+    {
+      if (TARGET_BARREL_SHIFT) {
+        if (microblaze_version_compare (microblaze_select.cpu, "v5.00.a") >= 0)
+          *total = COSTS_N_INSNS (1);                                         
+        else
+          *total = COSTS_N_INSNS (2);
+      }
+      else if (GET_CODE (XEXP (x, 1)) == CONST_INT) 
+        *total = COSTS_N_INSNS (INTVAL (XEXP (x, 1)));
+      else 
+        *total = COSTS_N_INSNS (32 * 4);                /* Double the worst cost of shifts when there is no barrel shifter and the shift amount is in a reg */
+      return false;
+    }
+    case PLUS:								
+    case MINUS:								
+    {									
+      if (mode == SFmode || mode == DFmode)				
+      {						
+        if (TARGET_HARD_FLOAT)
+          *total = COSTS_N_INSNS (6);					
+      } 
+      else if (mode == DImode)
+      {
+        *total = COSTS_N_INSNS (4);					
+      } else
+        *total = COSTS_N_INSNS (1);
+
+      return false;
+    }									
+    case NEG:								
+    {
+      if (mode == DImode) 
+        *total = COSTS_N_INSNS (4);
+
+      return false;
+    }
+    case MULT:								
+    {									
+      if (mode == SFmode) {
+        if (TARGET_HARD_FLOAT)
+          *total = COSTS_N_INSNS (6);					
+      }
+      else if (!TARGET_SOFT_MUL) {                                                               
+        if (microblaze_version_compare (microblaze_select.cpu, "v5.00.a") >= 0)
+          *total = COSTS_N_INSNS (1);                                     
+        else 
+          *total = COSTS_N_INSNS (3);                                               
+      }                                                               
+      return false;
+    }									
+    case DIV:								
+    case UDIV:								
+    {									
+      if (mode == SFmode)						
+      {			
+        if (TARGET_HARD_FLOAT)
+          *total = COSTS_N_INSNS (23);					
+      }								
+      return false;
+    }									
+    case SIGN_EXTEND:							
+    {
+      *total = COSTS_N_INSNS (1);						
+      return false;
+    }
+    case ZERO_EXTEND:							
+    {
+      *total = COSTS_N_INSNS (1);
+      return false;
+    }
+  }
+    
+  return false;
+}
+
+
+
+/* Return the number of instructions needed to load or store a value
+   of mode MODE at X.  Return 0 if X isn't valid for MODE.
+
+*/
+
+int
+microblaze_address_insns (rtx x, enum machine_mode mode)
+{
+  struct microblaze_address_info addr;
+
+  if (microblaze_classify_address (&addr, x, mode, false)) {
+    switch (addr.type)
+    {
+      case ADDRESS_REG:
+        if (SMALL_INT (addr.offset)) 
+          return 1;
+        else
+          return 2;
+      case ADDRESS_CONST_INT:
+        if (SMALL_INT (x))
+          return 1;
+        else
+          return 2;
+      case ADDRESS_REG_INDEX:
+      case ADDRESS_SYMBOLIC:
+        return 1;
+      default:
+        break;
+    }
+  }
+  return 0;
+}
+
+/* Provide the costs of an addressing mode that contains ADDR.
+   If ADDR is not a valid address, its cost is irrelevant.  */
+int
+microblaze_address_cost (addr)
+  rtx addr;
+{
+  return COSTS_N_INSNS (microblaze_address_insns (addr, GET_MODE (addr)));
+}
+
+/* Return nonzero if X is an address which needs a temporary register when 
+   reloaded while generating PIC code.  */
+
+/* XLNX [08/16/01] Need to look into this*/
+int
+pic_address_needs_scratch (x)
+  rtx x;
+{
+  /* An address which is a symbolic plus a non SMALL_INT needs a temp reg.  */
+  if (GET_CODE (x) == CONST && GET_CODE (XEXP (x, 0)) == PLUS
+      && GET_CODE (XEXP (XEXP (x, 0), 0)) == SYMBOL_REF
+      && GET_CODE (XEXP (XEXP (x, 0), 1)) == CONST_INT
+      && ! SMALL_INT (XEXP (XEXP (x, 0), 1)))
+    return 1;
+
+  return 0;
+}
+
+/* Make normal rtx_code into something we can index from an array */
+
+static enum internal_test
+map_test_to_internal_test (test_code)
+  enum rtx_code test_code;
+{
+  enum internal_test test = ITEST_MAX;
+
+  switch (test_code)
+  {
+    case EQ:  test = ITEST_EQ;  break;
+    case NE:  test = ITEST_NE;  break;
+    case GT:  test = ITEST_GT;  break;
+    case GE:  test = ITEST_GE;  break;
+    case LT:  test = ITEST_LT;  break;
+    case LE:  test = ITEST_LE;  break;
+    case GTU: test = ITEST_GTU; break;
+    case GEU: test = ITEST_GEU; break;
+    case LTU: test = ITEST_LTU; break;
+    case LEU: test = ITEST_LEU; break;
+    default:			break;
+  }
+
+  return test;
+}
+
+/* Emit the common code for doing conditional branches.
+   operand[0] is the label to jump to.
+   The comparison operands are saved away by cmp{si,di,sf,df}.  */
+
+void
+gen_conditional_branch (rtx operands[], enum rtx_code test_code)
+{
+  enum cmp_type type = branch_type;
+  rtx cmp0 = branch_cmp[0];
+  rtx cmp1 = branch_cmp[1];
+  enum machine_mode mode;
+  rtx reg0, reg1;
+  rtx label1, label2;
+
+  switch (type)
+  {
+    case CMP_SI:
+    case CMP_DI:
+      mode = type == CMP_SI ? SImode : DImode; 
+      break;
+   case CMP_SF:
+      if (TARGET_HARD_FLOAT) {
+        reg0 = gen_reg_rtx (SImode);
+        /* For cmp0 != cmp1, build cmp0 == cmp1, and test for result == 0 in the following instruction. */
+        emit_insn (gen_rtx (SET, VOIDmode, reg0,
+                            gen_rtx ((test_code == NE ? EQ : test_code), SImode, cmp0, cmp1)));
+
+        /* Setup test and branch for following instruction
+           Setup a test for zero as opposed to non-zero.
+           This is more optimally implemented. */
+        test_code = (test_code == NE) ? EQ : NE;
+        mode = SImode;
+        cmp0 = reg0;
+        cmp1 = const0_rtx;
+        break;
+      } else
+        fatal_insn ("gen_conditional_branch:", gen_rtx (test_code, VOIDmode, cmp0, cmp1));
+    default:
+      fatal_insn ("gen_conditional_branch:", gen_rtx (test_code, VOIDmode, cmp0, cmp1));
+  }
+
+  /* Generate the branch.  */
+
+  label1 = gen_rtx_LABEL_REF (VOIDmode, operands[0]);
+  label2 = pc_rtx;
+
+  if (!(GET_CODE (cmp1) == CONST_INT && INTVAL (cmp1) == 0)) {          /* Except for branch_zero */
+    emit_jump_insn (gen_rtx_PARALLEL (VOIDmode, 
+                                      gen_rtvec (2,
+                                                 gen_rtx (SET, VOIDmode, pc_rtx, gen_rtx (IF_THEN_ELSE, 
+                                                                                          VOIDmode, 
+                                                                                          gen_rtx (test_code, mode, cmp0, cmp1),
+                                                                                          label1, 
+                                                                                          label2)),
+                                                 gen_rtx_CLOBBER (VOIDmode, gen_rtx_REG (SImode, MB_ABI_ASM_TEMP_REGNUM)))));
+  } else 
+    emit_jump_insn (gen_rtx (SET, VOIDmode, pc_rtx, 
+                             gen_rtx (IF_THEN_ELSE, VOIDmode, 
+                                      gen_rtx (test_code, mode, cmp0, cmp1),
+                                      label1, label2)));
+}
+
+/* Write a loop to move a constant number of bytes.
+   Generate load/stores as follows:
+
+   do {
+   temp1 = src[0];
+   temp2 = src[1];
+   ...
+   temp<last> = src[MAX_MOVE_REGS-1];
+   dest[0] = temp1;
+   dest[1] = temp2;
+   ...
+   dest[MAX_MOVE_REGS-1] = temp<last>;
+   src += MAX_MOVE_REGS;
+   dest += MAX_MOVE_REGS;
+   } while (src != final);
+
+   This way, no NOP's are needed, and only MAX_MOVE_REGS+3 temp
+   registers are needed.
+
+   Aligned moves move MAX_MOVE_REGS*4 bytes every (2*MAX_MOVE_REGS)+3
+   cycles, unaligned moves move MAX_MOVE_REGS*4 bytes every
+   (4*MAX_MOVE_REGS)+3 cycles, assuming no cache misses.  */
+/*#undef UNITS_PER_WORD
+  #define UNITS_PER_WORD 1
+*/
+#define MAX_MOVE_REGS 4
+#define MAX_MOVE_BYTES (MAX_MOVE_REGS * UNITS_PER_WORD)
+
+
+
+static void
+block_move_loop (dest_reg, src_reg, bytes, align, orig_dest, orig_src)
+  rtx dest_reg;		/* register holding destination address */
+  rtx src_reg;		/* register holding source address */
+  int bytes;			/* # bytes to move */
+  int align;			/* alignment */
+  rtx orig_dest;		/* original dest for change_address */
+  rtx orig_src;		/* original source for making a reg note */
+{
+  rtx dest_mem = change_address (orig_dest, BLKmode, dest_reg);
+  rtx src_mem = change_address (orig_src, BLKmode, src_reg);
+  rtx align_rtx = GEN_INT (align);
+  rtx label;
+  rtx final_src;
+  rtx bytes_rtx;
+  int leftover;
+
+  if (bytes < 2 * MAX_MOVE_BYTES)
+    abort ();
+
+  leftover = bytes % MAX_MOVE_BYTES;
+  bytes -= leftover;
+
+  label = gen_label_rtx ();
+  final_src = gen_reg_rtx (Pmode);
+  bytes_rtx = GEN_INT (bytes);
+
+  if (bytes > 0x7fff)
+  {
+    emit_insn (gen_movsi (final_src, bytes_rtx));
+    emit_insn (gen_addsi3 (final_src, final_src, src_reg));
+  }
+  else
+  {
+    emit_insn (gen_addsi3 (final_src, src_reg, bytes_rtx));
+  }
+
+  emit_label (label);
+
+  bytes_rtx = GEN_INT (MAX_MOVE_BYTES);
+
+  emit_insn (gen_movstrsi_internal (dest_mem, src_mem, bytes_rtx, align_rtx));
+
+  emit_insn (gen_addsi3 (src_reg, src_reg, bytes_rtx));
+  emit_insn (gen_addsi3 (dest_reg, dest_reg, bytes_rtx));
+  emit_insn (gen_cmpsi (src_reg, final_src));
+
+  emit_jump_insn (gen_bne (label));
+
+  if (leftover)
+    emit_insn (gen_movstrsi_internal (dest_mem, src_mem, GEN_INT (leftover),
+                                      align_rtx));
+}
+
+/* Use a library function to move some bytes.  */
+
+static void
+block_move_call (dest_reg, src_reg, bytes_rtx)
+  rtx dest_reg;
+  rtx src_reg;
+  rtx bytes_rtx;
+{
+  /* We want to pass the size as Pmode, which will normally be SImode
+     but will be DImode if we are using 64 bit longs and pointers.  */
+  if (GET_MODE (bytes_rtx) != VOIDmode
+      && GET_MODE (bytes_rtx) != Pmode)
+    bytes_rtx = convert_to_mode (Pmode, bytes_rtx, 1);
+
+#ifdef TARGET_MEM_FUNCTIONS
+  emit_library_call (gen_rtx (SYMBOL_REF, Pmode, "memcpy"), 0,
+                     VOIDmode, 3, dest_reg, Pmode, src_reg, Pmode,
+                     convert_to_mode (TYPE_MODE (sizetype), bytes_rtx,
+                                      TREE_UNSIGNED (sizetype)),
+                     TYPE_MODE (sizetype));
+#else
+  emit_library_call (gen_rtx (SYMBOL_REF, Pmode, "bcopy"), 0,
+                     VOIDmode, 3, src_reg, Pmode, dest_reg, Pmode,
+                     convert_to_mode (TYPE_MODE (integer_type_node), bytes_rtx,
+                                      TREE_UNSIGNED (integer_type_node)),
+                     TYPE_MODE (integer_type_node));
+#endif
+}
+
+/* Expand string/block move operations.
+
+operands[0] is the pointer to the destination.
+operands[1] is the pointer to the source.
+operands[2] is the number of bytes to move.
+operands[3] is the alignment.  */
+
+void
+expand_block_move (operands)
+  rtx operands[];
+{
+  rtx bytes_rtx	= operands[2];
+  rtx align_rtx = operands[3];
+  int constp = GET_CODE (bytes_rtx) == CONST_INT;
+  HOST_WIDE_INT bytes = constp ? INTVAL (bytes_rtx) : 0;
+  int align = INTVAL (align_rtx);
+  rtx orig_src	= operands[1];
+  rtx orig_dest	= operands[0];
+  rtx src_reg;
+  rtx dest_reg;
+
+  if (constp && bytes <= 0)
+    return;
+
+  if (align > UNITS_PER_WORD)
+    align = UNITS_PER_WORD;
+
+  /* Move the address into scratch registers.  */
+  dest_reg = copy_addr_to_reg (XEXP (orig_dest, 0));
+  src_reg  = copy_addr_to_reg (XEXP (orig_src, 0));
+
+  if (TARGET_MEMCPY) {
+    block_move_call (dest_reg, src_reg, bytes_rtx);
+  }
+  else if (constp && bytes <= 2 * MAX_MOVE_BYTES
+           && align == UNITS_PER_WORD) {
+    move_by_pieces (orig_dest, orig_src, bytes, align, 0);                  
+  }	
+  else if (constp && bytes <= 2 * MAX_MOVE_BYTES) {
+    emit_insn (gen_movstrsi_internal (change_address (orig_dest, BLKmode,
+                                                      dest_reg),
+                                      change_address (orig_src, BLKmode,
+                                                      src_reg),
+                                      bytes_rtx, align_rtx));
+  }
+  else if (constp && align >= UNITS_PER_WORD && optimize) {
+    block_move_loop (dest_reg, src_reg, bytes, align, orig_dest, orig_src);
+  }
+  else if (constp /* && optimize */)
+  {
+    /* If the alignment is not word aligned, generate a test at
+       runtime, to see whether things wound up aligned, and we
+       can use the faster lw/sw instead ulw/usw.  */
+
+    rtx temp = gen_reg_rtx (Pmode);
+    rtx aligned_label = gen_label_rtx ();
+    rtx join_label = gen_label_rtx ();
+    int leftover = bytes % MAX_MOVE_BYTES;
+
+    bytes -= leftover;
+    emit_insn (gen_iorsi3 (temp, src_reg, dest_reg));
+    emit_insn (gen_andsi3 (temp, temp, GEN_INT (UNITS_PER_WORD - 1)));
+    emit_insn (gen_cmpsi (temp, const0_rtx));
+    emit_jump_insn (gen_beq (aligned_label));
+
+    /* Unaligned loop.  */
+    block_move_loop (dest_reg, src_reg, bytes, 1, orig_dest, orig_src);
+    emit_jump_insn (gen_jump (join_label));
+    emit_barrier ();
+
+    /* Aligned loop.  */
+    emit_label (aligned_label);
+    block_move_loop (dest_reg, src_reg, bytes, UNITS_PER_WORD, orig_dest,
+                     orig_src);
+    emit_label (join_label);
+
+    /* Bytes at the end of the loop.  */
+    if (leftover)
+      emit_insn (gen_movstrsi_internal (change_address (orig_dest, BLKmode,
+                                                        dest_reg),
+                                        change_address (orig_src, BLKmode,
+                                                        src_reg),
+                                        GEN_INT (leftover),
+                                        GEN_INT (align)));
+  }
+
+  else {
+    block_move_call (dest_reg, src_reg, bytes_rtx);
+  }
+}
+
+/* Emit load/stores for a small constant block_move. 
+
+operands[0] is the memory address of the destination.
+operands[1] is the memory address of the source.
+operands[2] is the number of bytes to move.
+operands[3] is the alignment.
+operands[4] is a temp register.
+operands[5] is a temp register.
+...
+operands[3+num_regs] is the last temp register.
+
+The block move type can be one of the following:
+BLOCK_MOVE_NORMAL	Do all of the block move.
+BLOCK_MOVE_NOT_LAST	Do all but the last store.
+BLOCK_MOVE_LAST		Do just the last store. */
+
+const char *
+output_block_move (insn, operands, num_regs, move_type)
+  rtx insn;
+  rtx operands[];
+  int num_regs;
+  enum block_move_type move_type;
+{
+  rtx dest_reg = XEXP (operands[0], 0);
+  rtx src_reg = XEXP (operands[1], 0);
+  HOST_WIDE_INT bytes = INTVAL (operands[2]);
+  int align = INTVAL (operands[3]);
+  int num = 0;
+  int offset = 0;
+  int use_lwl_lwr = 0;
+  int last_operand = num_regs + 4;
+  int safe_regs = 4;
+  int i;
+  rtx xoperands[10];
+  int op0_base_reg=0;
+  int op1_base_reg=0;
+
+  struct {
+    char *load;		/* load insn without nop */
+    char *load_nop;	/* load insn with trailing nop */
+    char *store;		/* store insn */
+    char *final;		/* if last_store used: NULL or swr */
+    char *last_store;	/* last store instruction */
+    int offset;			/* current offset */
+    enum machine_mode mode;	/* mode to use on (MEM) */
+  } load_store[4];
+
+  /* ??? Detect a bug in GCC, where it can give us a register
+     the same as one of the addressing registers and reduce
+     the number of registers available.  */
+  for (i = 4;
+       i < last_operand
+         && safe_regs < (int)(sizeof(xoperands) / sizeof(xoperands[0]));
+       i++)
+    if (! reg_mentioned_p (operands[i], operands[0])
+        && ! reg_mentioned_p (operands[i], operands[1]))
+      xoperands[safe_regs++] = operands[i];
+
+  if (safe_regs < last_operand)
+  {
+    xoperands[0] = operands[0];
+    xoperands[1] = operands[1];
+    xoperands[2] = operands[2];
+    xoperands[3] = operands[3];
+    return output_block_move (insn, xoperands, safe_regs - 4, move_type);
+  }
+
+  /* If we are given global or static addresses, and we would be
+     emitting a few instructions, try to save time by using a
+     temporary register for the pointer.  */
+  if (num_regs > 2 && (bytes > 2 * align || move_type != BLOCK_MOVE_NORMAL))
+  {
+    if (CONSTANT_P (src_reg))
+    {
+      if (TARGET_STATS)
+        microblaze_count_memory_refs (operands[1], 1);
+
+      src_reg = operands[3 + num_regs--];
+      if (move_type != BLOCK_MOVE_LAST)
+      {
+        xoperands[1] = operands[1];
+        xoperands[0] = src_reg;
+        if (GET_CODE(xoperands[1]) == MEM){
+          char *output_string = (char*)xmalloc(sizeof("addik\t%%0,r13,%%1"));		  
+          if ((GET_CODE (XEXP (xoperands[1], 0)) == SYMBOL_REF) ||
+              (GET_CODE (XEXP (xoperands[1], 0)) == CONST)){
+            sprintf (output_string, "addik\t%%0,%%1");
+            output_asm_insn (output_string, xoperands);
+            free (output_string);
+          }
+        }
+        else
+          fprintf(stderr,"Error!!Block move has operations other than MEM %d\n",GET_MODE(xoperands[1]));
+      }
+    }
+
+    if (CONSTANT_P (dest_reg))
+    {
+      if (TARGET_STATS)
+        microblaze_count_memory_refs (operands[0], 1);
+
+      dest_reg = operands[3 + num_regs--];
+      if (move_type != BLOCK_MOVE_LAST)
+      {
+        xoperands[1] = operands[0];
+        xoperands[0] = dest_reg;
+        if (GET_CODE(xoperands[1]) == MEM){
+          if ((GET_CODE (XEXP (xoperands[1], 0)) == SYMBOL_REF)||
+              (GET_CODE (XEXP (xoperands[1], 0)) == CONST)){
+            char *output_string = (char*)xmalloc(sizeof("addik\t%%0,r13,%%1"));
+            sprintf (output_string," addik\t%%0,%%1");
+            output_asm_insn (output_string, xoperands);
+            free (output_string);
+          }
+        }
+        else
+          fprintf(stderr,"Error!!Block move has operations other than MEM (2)\n");
+      }
+    }
+  }
+
+  if (num_regs > (int)(sizeof (load_store) / sizeof (load_store[0])))
+    num_regs = sizeof (load_store) / sizeof (load_store[0]);
+
+  else if (num_regs < 1)
+    fatal_insn ("Cannot do block move, not enough scratch registers", insn);
+
+  op0_base_reg = get_base_reg (XEXP (operands[0],0));
+  op1_base_reg = get_base_reg (XEXP (operands[1],0));
+
+  while (bytes > 0)
+  {
+    load_store[num].offset = offset;
+
+    if (bytes >= 4 && align >= 4)
+    {
+      /*if (offset == 0) {
+        load_store[num].load = "lw\t%0,r0,%1 # MICROBLAZElw78";
+        load_store[num].load_nop = "lw\t%0,r0,%1%# # MICROBLAZElw78";
+        load_store[num].store = "sw\t%0,r0,%1 # MICROBLAZEsw67";
+        load_store[num].last_store = "sw\t%0,r0,%1 # MICROBLAZEsw67";
+        }
+        else {
+        load_store[num].load = "lwi\t%0,%1 # MICROBLAZElwi78";
+        load_store[num].load_nop = "lwi\t%0,%1%# # MICROBLAZElwi78";
+        load_store[num].store = "swi\t%0,%1 # MICROBLAZEswi67";
+        load_store[num].last_store = "swi\t%0,%1 # MICROBLAZEswi67";
+        }*/
+      load_store[num].load=format_load_store(load_store[num].load,LOAD,SImode,src_reg,offset);
+      load_store[num].load_nop = load_store[num].load;
+      load_store[num].store=format_load_store(load_store[num].store,STORE,SImode,dest_reg,offset);
+      load_store[num].last_store = load_store[num].store;
+	    
+      load_store[num].final = 0;
+      load_store[num].mode = SImode;
+      offset += 4;
+      bytes -= 4;
+    }
+    else if (bytes >= 2 && align >= 2)
+    {
+      /*	  if(offset ==0){
+                  load_store[num].load = "lhu\t%0,%1 # MICROBLAZElhu121";
+                  load_store[num].load_nop = "lhu\t%0,%1% #MICROBLAZElhu122";
+                  load_store[num].store = "sh\t%0,%1 #MICROBLAZEsh121 ";
+                  load_store[num].last_store = "sh\t%0,%1 #MICROBLAZEshu122 ";
+                  }
+                  else	  
+                  {
+                  load_store[num].load = "lhui\t%0,%1 # MICROBLAZElhu111";
+                  load_store[num].load_nop = "lhui\t%0,%1% #MICROBLAZElhu112";
+                  load_store[num].store = "shi\t%0,%1 #MICROBLAZEsh111 ";
+                  load_store[num].last_store = "shi\t%0,%1 #MICROBLAZEshu112 ";
+                  }*/
+      load_store[num].load = 
+        format_load_store(load_store[num].load,LOAD,HImode,src_reg,offset);
+      load_store[num].load_nop = load_store[num].load;
+
+      load_store[num].store = 
+        format_load_store(load_store[num].store,STORE,HImode,dest_reg,offset);
+      load_store[num].last_store = load_store[num].store;
+	  
+      load_store[num].final = 0;
+      load_store[num].mode = HImode;
+      offset += 2;
+      bytes -= 2;
+    }
+    else
+    {
+      /*	  if (offset == 0) {
+                  load_store[num].load = "lbu\t%0,%1,r0 # MICROBLAZElbu45";
+                  load_store[num].load_nop = "lbu\t%0,r0,%1%# # MICROBLAZElbu45";
+                  load_store[num].store = "sbi\t%0,%1 # MICROBLAZEsb4";
+                  load_store[num].last_store = "sbi\t%0,%1 # MICROBLAZEsb5";
+                  }
+                  else {
+                  load_store[num].load = "lbui\t%0,%1 # MICROBLAZElbui45";
+                  load_store[num].load_nop = "lbui\t%0,%1%# # MICROBLAZElbui45";
+                  load_store[num].store = "sbi\t%0,%1 # MICROBLAZEsbi45";
+                  load_store[num].last_store = "sbi\t%0,%1 # MICROBLAZEsbi45";
+                  }
+      */
+      load_store[num].load = 
+        format_load_store(load_store[num].load,LOAD,QImode,src_reg,offset);
+      load_store[num].load_nop = load_store[num].load;
+
+      load_store[num].store = 
+        format_load_store(load_store[num].store,STORE,QImode,dest_reg,offset);
+      load_store[num].last_store = load_store[num].store;
+		    
+      load_store[num].final = 0;
+      load_store[num].mode = QImode;
+      offset++;
+      bytes--;
+    }
+
+    if (TARGET_STATS && move_type != BLOCK_MOVE_LAST)
+    {
+      dslots_load_total++;
+      dslots_load_filled++;
+
+      if (CONSTANT_P (src_reg))
+        microblaze_count_memory_refs (src_reg, 1);
+
+      if (CONSTANT_P (dest_reg))
+        microblaze_count_memory_refs (dest_reg, 1);
+    }
+
+    /* Emit load/stores now if we have run out of registers or are
+       at the end of the move.  */
+
+    if (++num == num_regs || bytes == 0)
+    {
+      /* If only load/store, we need a NOP after the load.  */
+      if (num == 1)
+      {
+        load_store[0].load = load_store[0].load_nop;
+        if (TARGET_STATS && move_type != BLOCK_MOVE_LAST)
+          dslots_load_filled--;
+      }
+
+      if (move_type != BLOCK_MOVE_LAST)
+      {
+        for (i = 0; i < num; i++)
+        {
+          int offset;
+
+          if (!operands[i + 4])
+            abort ();
+
+          if (GET_MODE (operands[i + 4]) != load_store[i].mode)
+            operands[i + 4] = gen_rtx (REG, load_store[i].mode,
+                                       REGNO (operands[i + 4]));
+
+          offset = load_store[i].offset;
+          xoperands[0] = operands[i + 4];
+          xoperands[1] = gen_rtx (MEM, load_store[i].mode,
+                                  plus_constant (src_reg, offset));
+
+          /*		  if(offset == 0){
+                          load_store[i].load = 
+                          format_load_store(load_store[num].load,LOAD,load_store[num].mode,
+                          xoperands[1],offset);
+                          load_store[i].load_nop = load_store[i].load; */
+          /*load_store[i].load = "lbu\t%0,%1,r0 # MICROBLAZElbu451";
+            load_store[i].load_nop = "lbu\t%0,r0,%1%# # MICROBLAZElbu452";*/
+          /*		  }*/
+		  
+
+          if (use_lwl_lwr)
+          {
+            int extra_offset
+              = GET_MODE_SIZE (load_store[i].mode) - 1;
+
+            xoperands[2] = gen_rtx (MEM, load_store[i].mode,
+                                    plus_constant (src_reg,
+                                                   extra_offset
+                                                   + offset));
+
+          }
+
+          output_asm_insn (load_store[i].load, xoperands);
+        }
+      }
+
+      for (i = 0; i < num; i++)
+      {
+        int last_p = (i == num-1 && bytes == 0);
+        int offset = load_store[i].offset;
+
+        xoperands[0] = operands[i + 4];
+        xoperands[1] = gen_rtx (MEM, load_store[i].mode,
+                                plus_constant (dest_reg, offset));
+
+	      
+        if (use_lwl_lwr)
+        {
+          int extra_offset = GET_MODE_SIZE (load_store[i].mode) - 1;
+          xoperands[2] = gen_rtx (MEM, load_store[i].mode,
+                                  plus_constant (dest_reg,
+                                                 extra_offset
+                                                 + offset));
+        }
+
+
+        if (move_type == BLOCK_MOVE_NORMAL)
+          output_asm_insn (load_store[i].store, xoperands);
+
+        else if (move_type == BLOCK_MOVE_NOT_LAST)
+        {
+          if (!last_p)
+            output_asm_insn (load_store[i].store, xoperands);
+
+          else if (load_store[i].final != 0)
+            output_asm_insn (load_store[i].final, xoperands);
+        }
+
+        else if (last_p)
+          output_asm_insn (load_store[i].last_store, xoperands);
+      }
+
+      num = 0;		/* reset load_store */
+      use_lwl_lwr = 0;
+    }
+  }
+
+  return "";
+}
+
+
+
+/* Argument support functions.  */
+
+/* Initialize CUMULATIVE_ARGS for a function.  */
+
+void
+init_cumulative_args (cum, fntype, libname)
+  CUMULATIVE_ARGS *cum;		/* argument info to initialize */
+  tree fntype;			/* tree ptr for function decl */
+  rtx libname ATTRIBUTE_UNUSED;	/* SYMBOL_REF of library name or 0 */
+{
+  static CUMULATIVE_ARGS zero_cum;
+  tree param, next_param;
+
+  if (TARGET_DEBUG_E_MODE)
+  {
+    fprintf (stderr,
+             "\ninit_cumulative_args, fntype = 0x%.8lx", (long)fntype);
+
+    if (!fntype)
+      fputc ('\n', stderr);
+
+    else
+    {
+      tree ret_type = TREE_TYPE (fntype);
+      fprintf (stderr, ", fntype code = %s, ret code = %s\n",
+               tree_code_name[(int)TREE_CODE (fntype)],
+               tree_code_name[(int)TREE_CODE (ret_type)]);
+    }
+  }
+
+  *cum = zero_cum;
+
+  /* Determine if this function has variable arguments.  This is
+     indicated by the last argument being 'void_type_mode' if there
+     are no variable arguments.  The standard MICROBLAZE calling sequence
+     passes all arguments in the general purpose registers in this case. */
+
+  for (param = fntype ? TYPE_ARG_TYPES (fntype) : 0;
+       param != 0; param = next_param)
+  {
+    next_param = TREE_CHAIN (param);
+    if (next_param == 0 && TREE_VALUE (param) != void_type_node)
+      cum->gp_reg_found = 1;
+  }
+}
+
+/* Advance the argument to the next argument position.  */
+
+void
+function_arg_advance (cum, mode, type, named)
+  CUMULATIVE_ARGS *cum;	/* current arg information */
+  enum machine_mode mode;	/* current arg mode */
+  tree type;			/* type of the argument or 0 if lib support */
+  int named;			/* whether or not the argument was named */
+{
+  if (TARGET_DEBUG_E_MODE)
+  {
+    fprintf (stderr,
+             "function_adv({gp reg found = %d, arg # = %2d, words = %2d}, %4s, ",
+             cum->gp_reg_found, cum->arg_number, cum->arg_words,
+             GET_MODE_NAME (mode));
+    fprintf (stderr, HOST_PTR_PRINTF, type);
+    fprintf (stderr, ", %d )\n\n", named);
+  }
+
+  cum->arg_number++;
+  switch (mode)
+  {
+    case VOIDmode:
+      break;
+
+    default:
+      if (GET_MODE_CLASS (mode) != MODE_COMPLEX_INT
+          && GET_MODE_CLASS (mode) != MODE_COMPLEX_FLOAT)
+        abort ();
+
+      cum->gp_reg_found = 1;
+      cum->arg_words += ((GET_MODE_SIZE (mode) + UNITS_PER_WORD - 1)
+                         / UNITS_PER_WORD);
+      break;
+
+    case BLKmode:
+      cum->gp_reg_found = 1;
+      cum->arg_words += ((int_size_in_bytes (type) + UNITS_PER_WORD - 1)
+                         / UNITS_PER_WORD);
+      break;
+
+    case SFmode:
+      cum->arg_words++;
+      if (! cum->gp_reg_found && cum->arg_number <= 2)
+        cum->fp_code += 1 << ((cum->arg_number - 1) * 2);
+      break;
+
+    case DFmode:
+      cum->arg_words += 2;
+      if (! cum->gp_reg_found && ! TARGET_SINGLE_FLOAT && cum->arg_number <= 2)
+        cum->fp_code += 2 << ((cum->arg_number - 1) * 2);
+      break;
+
+    case DImode:
+      cum->gp_reg_found = 1;
+      cum->arg_words += 2;
+      break;
+
+    case QImode:
+    case HImode:
+    case SImode:
+      cum->gp_reg_found = 1;
+      cum->arg_words++;
+      break;
+  }
+}
+
+/* Return an RTL expression containing the register for the given mode,
+   or 0 if the argument is to be passed on the stack.  */
+
+struct rtx_def *
+function_arg (cum, mode, type, named)
+  CUMULATIVE_ARGS *cum;	/* current arg information */
+  enum machine_mode mode;	/* current arg mode */
+  tree type;			/* type of the argument or 0 if lib support */
+  int named;			/* != 0 for normal args, == 0 for ... args */
+{
+  rtx ret;
+  int regbase = -1;
+  int *arg_words = &cum->arg_words;
+  int struct_p = (type != 0
+                  && (TREE_CODE (type) == RECORD_TYPE
+                      || TREE_CODE (type) == UNION_TYPE
+                      || TREE_CODE (type) == QUAL_UNION_TYPE));
+
+  if (TARGET_DEBUG_E_MODE)
+  {
+    fprintf (stderr,
+             "function_arg( {gp reg found = %d, arg # = %2d, words = %2d}, %4s, ",
+             cum->gp_reg_found, cum->arg_number, cum->arg_words,
+             GET_MODE_NAME (mode));
+    fprintf (stderr, HOST_PTR_PRINTF, type);
+    fprintf (stderr, ", %d ) = ", named);
+  }
+  
+  cum->last_arg_fp = 0;
+  switch (mode)
+  {
+    case SFmode:
+      regbase = GP_ARG_FIRST;
+      break;
+    case DFmode:
+      regbase = GP_ARG_FIRST;
+      break;
+    default:
+      if (GET_MODE_CLASS (mode) != MODE_COMPLEX_INT
+          && GET_MODE_CLASS (mode) != MODE_COMPLEX_FLOAT)
+        abort ();
+      /* Drops through.  */
+    case BLKmode:
+      regbase = GP_ARG_FIRST;
+      break;
+
+    case VOIDmode:
+    case QImode:
+    case HImode:
+    case SImode:
+      regbase = GP_ARG_FIRST;
+      break;
+    case DImode:
+      regbase = GP_ARG_FIRST;
+  }
+
+  if (*arg_words >= MAX_ARGS_IN_REGISTERS)
+  {
+    if (TARGET_DEBUG_E_MODE)
+      fprintf (stderr, "<stack>%s\n", struct_p ? ", [struct]" : "");
+
+    ret = 0;
+  }
+  else
+  {
+    if (regbase == -1)
+      abort ();
+
+    ret = gen_rtx (REG, mode, regbase + *arg_words);
+
+    if (TARGET_DEBUG_E_MODE)
+      fprintf (stderr, "%s%s\n", reg_names[regbase + *arg_words],
+               struct_p ? ", [struct]" : "");
+
+    /* The following is a hack in order to pass 1 byte structures
+       the same way that the MICROBLAZE compiler does (namely by passing
+       the structure in the high byte or half word of the register).
+       This also makes varargs work.  If we have such a structure,
+       we save the adjustment RTL, and the call define expands will
+       emit them.  For the VOIDmode argument (argument after the
+       last real argument), pass back a parallel vector holding each
+       of the adjustments.  */
+
+    /* ??? function_arg can be called more than once for each argument.
+       As a result, we compute more adjustments than we need here.
+       See the CUMULATIVE_ARGS definition in microblaze.h.  */
+
+    /* ??? This scheme requires everything smaller than the word size to
+       shifted to the left
+    */
+
+    /* XLNX [We might not need to do this for MicroBlaze . Seems to cause a problem 
+       while passing small structs */
+#ifndef MICROBLAZE
+    if (struct_p && int_size_in_bytes (type) < UNITS_PER_WORD)
+    {
+      rtx amount = GEN_INT (BITS_PER_WORD
+                            - int_size_in_bytes (type) * BITS_PER_UNIT);
+      rtx reg = gen_rtx (REG, word_mode, regbase + *arg_words);
+
+	  
+      cum->adjust[cum->num_adjusts++] = gen_ashlsi3 (reg, reg, amount);
+    }
+
+    asdakdakdksada
+#endif
+      }
+
+  if (mode == VOIDmode)
+  {
+    if (cum->num_adjusts > 0)
+      ret = gen_rtx (PARALLEL, (enum machine_mode) cum->fp_code,
+                     gen_rtvec_v (cum->num_adjusts, cum->adjust));
+  }
+
+  return ret;
+}
+
+int
+function_arg_partial_nregs (cum, mode, type, named)
+  CUMULATIVE_ARGS *cum;	/* current arg information */
+  enum machine_mode mode;	/* current arg mode */
+  tree type;			/* type of the argument or 0 if lib support */
+  int named;                  /* != 0 for normal args, == 0 for ... args */
+{
+  if ((mode == BLKmode
+       || GET_MODE_CLASS (mode) != MODE_COMPLEX_INT
+       || GET_MODE_CLASS (mode) != MODE_COMPLEX_FLOAT)
+      && cum->arg_words < MAX_ARGS_IN_REGISTERS)
+  {
+    int words;
+    if (mode == BLKmode)
+      words = ((int_size_in_bytes (type) + UNITS_PER_WORD - 1)
+               / UNITS_PER_WORD);
+    else
+      words = (GET_MODE_SIZE (mode) + UNITS_PER_WORD - 1) / UNITS_PER_WORD;
+
+    if (words + cum->arg_words <= MAX_ARGS_IN_REGISTERS)
+      return 0;		/* structure fits in registers */
+
+    if (TARGET_DEBUG_E_MODE)
+      fprintf (stderr, "function_arg_partial_nregs = %d\n",
+               MAX_ARGS_IN_REGISTERS - cum->arg_words);
+
+    return MAX_ARGS_IN_REGISTERS - cum->arg_words;
+  }
+
+  else if (mode == DImode && cum->arg_words == MAX_ARGS_IN_REGISTERS-1)
+  {
+    if (TARGET_DEBUG_E_MODE)
+      fprintf (stderr, "function_arg_partial_nregs = 1\n");
+        
+    return 1;
+  }
+    
+  return 0;
+}
+
+/*  Convert a version number of the form "vX.YY.Z" to an integer encoding 
+    for easier range comparison */
+static int 
+microblaze_version_to_int (const char *version)
+{
+  char *p, *v;
+  char *tmpl = "vX.YY.Z";
+  int iver = 0;
+  int pos = 0;
+  
+  iver = 0;
+  
+  p = (char *)version;
+  v = tmpl;
+  
+  while (*v) {
+    if (*v == 'X') {            /* Looking for major */
+      if (!(*p >= '0' && *p <= '9'))
+        return -1;
+      iver += (int)(*p - '0');
+      iver *= 10;
+    } else if (*v == 'Y') {     /* Looking for minor */
+      if (!(*p >= '0' && *p <= '9'))
+        return -1;
+      iver += (int)(*p - '0');
+      iver *= 10;
+    } else if (*v == 'Z') {     /* Looking for compat */
+      if (!(*p >= 'a' && *p <= 'z'))
+        return -1;
+      iver *= 10;
+      iver += (int)(*p - 'a');
+    } else {
+      if (*p != *v)
+        return -1;
+    }
+        
+    v++;
+    p++;
+  }
+    
+  if (*p)
+    return -1;
+
+  return iver;
+}
+
+/* Compare two given microblaze versions and return a verdict
+ */
+static int
+microblaze_version_compare (const char *va, const char *vb) 
+{
+  return strcasecmp (va, vb);
+}
+
+/* Set up the threshold for data to go into the small data area, instead
+   of the normal data area, and detect any conflicts in the switches.  */
+
+void
+override_options ()
+{
+  register int i, start;
+  register int regno;
+  register enum machine_mode mode;
+  int ver;
+
+
+  microblaze_section_threshold = g_switch_set ? g_switch_value : MICROBLAZE_DEFAULT_GVALUE;
+
+  /* Check the Microblaze CPU version for any special action that needs to be done */
+  ver = microblaze_version_to_int (microblaze_select.cpu);
+  if (ver == -1) {
+    error ("(%s) is an invalid argument to -mcpu=\n", microblaze_select.cpu);
+  }
+
+  ver = microblaze_version_compare (microblaze_select.cpu, "v3.00.a");
+  if (ver < 0) {                                                        /* No hardware exceptions in earlier versions. So no worries */
+    microblaze_select.flags &= ~(MICROBLAZE_MASK_NO_UNSAFE_DELAY);
+    microblaze_no_unsafe_delay = 0;
+    microblaze_pipe = MICROBLAZE_PIPE_3;
+  } else if (ver == 0 || (microblaze_version_compare (microblaze_select.cpu, "v4.00.a") == 0)) {
+    microblaze_select.flags |= (MICROBLAZE_MASK_NO_UNSAFE_DELAY);
+    microblaze_no_unsafe_delay = 1;
+    microblaze_pipe = MICROBLAZE_PIPE_3;
+  } else {                                                              /* v5.00.a or greater */
+    microblaze_select.flags &= ~(MICROBLAZE_MASK_NO_UNSAFE_DELAY);
+    microblaze_no_unsafe_delay = 0;                              
+    target_flags |= MASK_PATTERN_COMPARE;                               /* Pattern compares are on by default in later versions of MB */
+    microblaze_pipe = MICROBLAZE_PIPE_5;
+  }
+
+  /* Always use DFA scheduler */
+  microblaze_sched_use_dfa = 1;
+
+  /* Check if we are asked to not clear BSS 
+     If YES, we do not place zero initialized in BSS  */
+  if (!strcmp (microblaze_no_clearbss, "yes")) 
+    flag_zero_initialized_in_bss = 0;
+  
+  /* Tell halfpic.c that we have half-pic code if we do.  */
+  if (TARGET_HALF_PIC)
+    HALF_PIC_INIT ();
+
+  /* -fpic (-KPIC) is the default when TARGET_ABICALLS is defined.  We need
+     to set flag_pic so that the LEGITIMATE_PIC_OPERAND_P macro will work.  */
+  /* ??? -non_shared turns off pic code generation, but this is not
+     implemented.  */
+  if (TARGET_ABICALLS)
+  {
+    microblaze_abicalls = MICROBLAZE_ABICALLS_YES;
+    flag_pic = 1;
+    if (microblaze_section_threshold > 0)
+      warning ("-G is incompatible with PIC code which is the default");
+  }
+  else
+    microblaze_abicalls = MICROBLAZE_ABICALLS_NO;
+  /* printf("microblaze_abi %d microblaze_abicalls %d\n",
+     microblaze_abi, microblaze_abicalls); */
+  /* -membedded-pic is a form of PIC code suitable for embedded
+     systems.  All calls are made using PC relative addressing, and
+     all data is addressed using the $gp register.  This requires gas,
+     which does most of the work, and GNU ld, which automatically
+     expands PC relative calls which are out of range into a longer
+     instruction sequence.  All gcc really does differently is
+     generate a different sequence for a switch.  */
+  if (TARGET_EMBEDDED_PIC)
+  {
+    flag_pic = 1;
+    if (TARGET_ABICALLS)
+      warning ("-membedded-pic and -mabicalls are incompatible");
+
+    if (g_switch_set)
+      warning ("-G and -membedded-pic are incompatible");
+
+    /* Setting microblaze_section_threshold is not required, because gas
+       will force everything to be GP addressable anyhow, but
+       setting it will cause gcc to make better estimates of the
+       number of instructions required to access a particular data
+       item.  */
+    microblaze_section_threshold = 0x7fffffff;
+  }
+
+  /* Initialize the high and low values for legitimate floating point
+     constants.  Rather than trying to get the accuracy down to the
+     last bit, just use approximate ranges.  */
+  dfhigh = REAL_VALUE_ATOF ("1.0e300", DFmode);
+  dflow = REAL_VALUE_ATOF ("1.0e-300", DFmode);
+  sfhigh = REAL_VALUE_ATOF ("1.0e38", SFmode);
+  sflow = REAL_VALUE_ATOF ("1.0e-38", SFmode);
+
+  microblaze_print_operand_punct['?'] = 1;
+  microblaze_print_operand_punct['#'] = 1;
+  microblaze_print_operand_punct['&'] = 1;
+  microblaze_print_operand_punct['!'] = 1;
+  microblaze_print_operand_punct['*'] = 1;
+  microblaze_print_operand_punct['@'] = 1;
+  microblaze_print_operand_punct['.'] = 1;
+  microblaze_print_operand_punct['('] = 1;
+  microblaze_print_operand_punct[')'] = 1;
+  microblaze_print_operand_punct['['] = 1;
+  microblaze_print_operand_punct[']'] = 1;
+  microblaze_print_operand_punct['<'] = 1;
+  microblaze_print_operand_punct['>'] = 1;
+  microblaze_print_operand_punct['{'] = 1;
+  microblaze_print_operand_punct['}'] = 1;
+  microblaze_print_operand_punct['^'] = 1;
+  microblaze_print_operand_punct['$'] = 1;
+  microblaze_print_operand_punct['+'] = 1;
+
+  microblaze_char_to_class['d'] = GR_REGS;
+  microblaze_char_to_class['f'] = NO_REGS;
+  microblaze_char_to_class['h'] = HI_REG;
+  microblaze_char_to_class['l'] = LO_REG;
+  microblaze_char_to_class['a'] = HILO_REG;
+  microblaze_char_to_class['x'] = MD_REGS;
+  microblaze_char_to_class['b'] = ALL_REGS;
+  microblaze_char_to_class['y'] = GR_REGS;
+  microblaze_char_to_class['z'] = ST_REGS;
+
+  /* For Divide count increments */
+  microblaze_print_operand_punct['-'] = 1;  /* print divide label */
+  microblaze_print_operand_punct['_'] = 1;  /* increment divide label */
+
+  /* Set up array to map GCC register number to debug register number.
+     Ignore the special purpose register numbers.  */
+
+  for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)
+    microblaze_dbx_regno[i] = -1;
+
+  start = GP_DBX_FIRST - GP_REG_FIRST;
+  for (i = GP_REG_FIRST; i <= GP_REG_LAST; i++)
+    microblaze_dbx_regno[i] = i + start;
+
+  start = FP_DBX_FIRST - FP_REG_FIRST;
+  for (i = FP_REG_FIRST; i <= FP_REG_LAST; i++)
+    microblaze_dbx_regno[i] = i + start;
+
+
+  /* Save GPR registers in word_mode sized hunks.  word_mode hasn't been
+     initialized yet, so we can't use that here.  */
+  gpr_mode = SImode;
+
+  /* Set up array giving whether a given register can hold a given mode.
+     At present, restrict ints from being in FP registers, because reload
+     is a little enthusiastic about storing extra values in FP registers,
+     and this is not good for things like OS kernels.  Also, due to the
+     mandatory delay, it is as fast to load from cached memory as to move
+     from the FP register.  */
+
+  for (mode = VOIDmode;
+       mode != MAX_MACHINE_MODE;
+       mode = (enum machine_mode) ((int)mode + 1))
+  {
+    register int size              = GET_MODE_SIZE (mode);
+    register enum mode_class class = GET_MODE_CLASS (mode);
+
+    for (regno = 0; regno < FIRST_PSEUDO_REGISTER; regno++)
+    {
+      register int ok;
+
+      if (mode == CCmode) {
+        ok = (ST_REG_P (regno) || GP_REG_P (regno)
+              || FP_REG_P (regno));
+      }
+      else if (GP_REG_P (regno))
+        ok = ((regno & 1) == 0 || size <= UNITS_PER_WORD); 
+      else 
+        ok = 0;
+
+      microblaze_hard_regno_mode_ok[(int)mode][regno] = ok;
+    }
+  }
+}
+
+void
+microblaze_order_regs_for_local_alloc ()
+{
+  register int i;
+
+  for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)
+    reg_alloc_order[i] = i;
+
+}
+
+
+/* The MICROBLAZE debug format wants all automatic variables and arguments
+   to be in terms of the virtual frame pointer (stack pointer before
+   any adjustment in the function), while the MICROBLAZE 3.0 linker wants
+   the frame pointer to be the stack pointer after the initial
+   adjustment.  So, we do the adjustment here.  The arg pointer (which
+   is eliminated) points to the virtual frame pointer, while the frame
+   pointer (which may be eliminated) points to the stack pointer after
+   the initial adjustments.  */
+
+HOST_WIDE_INT
+microblaze_debugger_offset (addr, offset)
+  rtx addr;
+  HOST_WIDE_INT offset;
+{
+  rtx offset2 = const0_rtx;
+  rtx reg = eliminate_constant_term (addr, &offset2);
+
+  if (offset == 0)
+    offset = INTVAL (offset2);
+
+  if (reg == stack_pointer_rtx || reg == frame_pointer_rtx
+      || reg == hard_frame_pointer_rtx)
+  {
+    HOST_WIDE_INT frame_size = (!current_frame_info.initialized)
+      ? compute_frame_size (get_frame_size ())
+      : current_frame_info.total_size;
+
+    offset = offset - frame_size;
+  }
+
+  /* sdbout_parms does not want this to crash for unrecognized cases.  */
+#if 0
+  else if (reg != arg_pointer_rtx)
+    fatal_insn ("microblaze_debugger_offset called with non stack/frame/arg pointer.", addr);
+#endif
+
+  return offset;
+}
+
+/* Implement INITIAL_ELIMINATION_OFFSET.  FROM is either the frame
+   pointer or argument pointer or the return address pointer.  TO is either the stack pointer or
+   hard frame pointer.  */
+
+HOST_WIDE_INT
+microblaze_initial_elimination_offset (int from, int to)
+{  
+  HOST_WIDE_INT offset;
+    
+  switch (from) {
+    case FRAME_POINTER_REGNUM:
+      offset = 0;
+      break;
+    case ARG_POINTER_REGNUM:
+      if (to == STACK_POINTER_REGNUM || to == HARD_FRAME_POINTER_REGNUM)
+        offset = compute_frame_size (get_frame_size ());				 
+      else {
+        abort ();
+      }
+      break;
+    case RETURN_ADDRESS_POINTER_REGNUM:
+      if (current_function_is_leaf) 						 
+        offset = 0;				 			 
+      else 
+        offset = current_frame_info.gp_offset + 
+          ((UNITS_PER_WORD - (POINTER_SIZE / BITS_PER_UNIT)));
+      break;
+    default:
+      abort ();
+  }
+  return offset;
+}
+
+/* A C compound statement to output to stdio stream STREAM the
+   assembler syntax for an instruction operand X.  X is an RTL
+   expression.
+
+   CODE is a value that can be used to specify one of several ways
+   of printing the operand.  It is used when identical operands
+   must be printed differently depending on the context.  CODE
+   comes from the `%' specification that was used to request
+   printing of the operand.  If the specification was just `%DIGIT'
+   then CODE is 0; if the specification was `%LTR DIGIT' then CODE
+   is the ASCII code for LTR.
+
+   If X is a register, this macro should print the register's name.
+   The names can be found in an array `reg_names' whose type is
+   `char *[]'.  `reg_names' is initialized from `REGISTER_NAMES'.
+
+   When the machine description has a specification `%PUNCT' (a `%'
+   followed by a punctuation character), this macro is called with
+   a null pointer for X and the punctuation character for CODE.
+
+   The MICROBLAZE specific codes are:
+
+   'X'  X is CONST_INT, prints 32 bits in hexadecimal format = "0x%08x",
+   'x'  X is CONST_INT, prints 16 bits in hexadecimal format = "0x%04x",
+   'd'  output integer constant in decimal,
+   'z'	if the operand is 0, use $0 instead of normal operand.
+   'D'  print second register of double-word register operand.
+   'L'  print low-order register of double-word register operand.
+   'M'  print high-order register of double-word register operand.
+   'C'  print part of opcode for a branch condition.
+   'N'  print part of opcode for a branch condition, inverted.
+   'S'  X is CODE_LABEL, print with prefix of "LS" (for embedded switch).
+   'B'  print 'z' for EQ, 'n' for NE
+   'b'  print 'n' for EQ, 'z' for NE
+   'T'  print 'f' for EQ, 't' for NE
+   't'  print 't' for EQ, 'f' for NE
+   'Z'  print register and a comma, but print nothing for $fcc0
+   '?'	Print 'd' if we are to use a branch with delay slot instead of normal branch.
+   '@'	Print the name of the assembler temporary register (at or rMB_ABI_ASM_TEMP_REGNUM).
+   '.'	Print the name of the register with a hard-wired zero (zero or r0).
+   '^'	Print the name of the pic call-through register (t9 or rMB_ABI_PIC_FUNC_REGNUM).
+   '$'	Print the name of the stack pointer register (sp or rMB_ABI_STACK_POINTER_REGNUM).
+   '+'	Print the name of the gp register (gp or rMB_ABI_GPRO_REGNUM).  
+   '#'	Print nop if NOT in a .set noreorder section ie if the delay slot of a branch is not filled. */
+void
+print_operand (file, op, letter)
+  FILE *file;		/* file to write to */
+  rtx op;		/* operand to print */
+  int letter;		/* %<letter> or 0 */
+{
+  register enum rtx_code code;
+
+  if (PRINT_OPERAND_PUNCT_VALID_P (letter))
+  {
+    switch (letter)
+    {
+      case '?':                             /* Conditionally add a 'd' to indicate filled delay slot */
+        if (final_sequence != NULL)
+          fputs ("d", file);
+        break;
+
+      case '#':                             /* Conditionally add a nop in unfilled delay slot */
+        if (final_sequence == NULL) 
+          fputs ("nop\t\t# Unfilled delay slot\n", file);
+        break;
+
+      case '@':
+        fputs (reg_names [GP_REG_FIRST + MB_ABI_ASM_TEMP_REGNUM], file);
+        break;
+
+      case '^':
+        fputs (reg_names [PIC_FUNCTION_ADDR_REGNUM], file);
+        break;
+
+      case '.':
+        fputs (reg_names [GP_REG_FIRST + 0], file);
+        break;
+
+      case '$':
+        fputs (reg_names[STACK_POINTER_REGNUM], file);
+        break;
+
+      case '+':
+        fputs (reg_names[GP_REG_FIRST + MB_ABI_GPRO_REGNUM], file);
+        break;
+
+      case '-':
+        fprintf(file,"%sL%d_",LOCAL_LABEL_PREFIX,div_count);
+        break;
+
+      case '_':
+        div_count++;
+        break;
+
+        /*  case 'g':
+            fputs("i",file);
+     
+            break;
+        */
+      default:
+        error ("PRINT_OPERAND: Unknown punctuation '%c'", letter);
+        break;
+    }
+
+    return;
+  }
+
+  if (! op)
+  {
+    error ("PRINT_OPERAND null pointer");
+    return;
+  }
+
+  code = GET_CODE (op);
+
+  if (code == SIGN_EXTEND)
+    op = XEXP (op, 0), code = GET_CODE (op);
+
+  if (letter == 'C')
+    switch (code)
+    {
+      case EQ:	fputs ("eq",  file); break;
+      case NE:	fputs ("ne",  file); break;
+      case GT:	fputs ("gt",  file); break;
+      case GE:	fputs ("ge",  file); break;
+      case LT:	fputs ("lt",  file); break;
+      case LE:	fputs ("le",  file); break;
+      case GTU: fputs ("gtu", file); break;
+      case GEU: fputs ("geu", file); break;
+      case LTU: fputs ("ltu", file); break;
+      case LEU: fputs ("leu", file); break;
+      default:
+        fatal_insn ("PRINT_OPERAND, invalid insn for %%C", op);
+    }
+
+  else if (letter == 'N')
+    switch (code)
+    {
+      case EQ:	fputs ("ne",  file); break;
+      case NE:	fputs ("eq",  file); break;
+      case GT:	fputs ("le",  file); break;
+      case GE:	fputs ("lt",  file); break;
+      case LT:	fputs ("ge",  file); break;
+      case LE:	fputs ("gt",  file); break;
+      case GTU: fputs ("leu", file); break;
+      case GEU: fputs ("ltu", file); break;
+      case LTU: fputs ("geu", file); break;
+      case LEU: fputs ("gtu", file); break;
+      default:
+        fatal_insn ("PRINT_OPERAND, invalid insn for %%N", op);
+    }
+
+  else if (letter == 'S')
+  {
+    char buffer[100];
+
+    ASM_GENERATE_INTERNAL_LABEL (buffer, "LS", CODE_LABEL_NUMBER (op));
+    assemble_name (file, buffer);
+  }
+
+  else if (letter == 'Z')
+  {
+    register int regnum;
+
+    if (code != REG)
+      abort ();
+
+    regnum = REGNO (op);
+    if (! ST_REG_P (regnum))
+      abort ();
+
+    if (regnum != ST_REG_FIRST)
+      fprintf (file, "%s,", reg_names[regnum]);
+  }
+
+  else if (code == REG || code == SUBREG)
+  {
+    register int regnum;
+
+    if (code == REG)
+      regnum = REGNO (op);
+    else
+      regnum = true_regnum (op);
+
+    if ((letter == 'M' && ! WORDS_BIG_ENDIAN)
+        || (letter == 'L' && WORDS_BIG_ENDIAN)
+        || letter == 'D')
+      regnum++;
+
+    fprintf (file, "%s", reg_names[regnum]);
+  }
+
+  else if (code == MEM)
+    output_address (XEXP (op, 0));
+
+  else if (code == CONST_DOUBLE
+           && GET_MODE_CLASS (GET_MODE (op)) == MODE_FLOAT)
+  {
+/* GCC 3.4.1 
+ * Removed the REAL_VALUE_TO_DECIMAL part 
+ */
+#if 0 
+    REAL_VALUE_TYPE d;
+    char s[30];
+    if(!printed){
+      /*fprintf(stderr,"Printing done here\n");*/
+      REAL_VALUE_FROM_CONST_DOUBLE (d, op);
+      REAL_VALUE_TO_DECIMAL (d, "%.20e", s);
+      fprintf (file, s);
+    }
+#else
+    char s[60];
+
+    real_to_decimal (s, CONST_DOUBLE_REAL_VALUE (op), sizeof (s), 0, 1);
+    fputs (s, file);
+#endif
+  }
+   
+
+  else if (letter == 'x' && GET_CODE (op) == CONST_INT)
+    fprintf (file, HOST_WIDE_INT_PRINT_HEX, 0xffff & INTVAL(op));
+
+  else if (letter == 'X' && GET_CODE(op) == CONST_INT)
+    fprintf (file, HOST_WIDE_INT_PRINT_HEX, INTVAL (op));
+
+  else if (letter == 'd' && GET_CODE(op) == CONST_INT)
+    fprintf (file, HOST_WIDE_INT_PRINT_DEC, (INTVAL(op)));
+
+  else if (letter == 'z' && GET_CODE (op) == CONST_INT && INTVAL (op) == 0)
+    fputs (reg_names[GP_REG_FIRST], file);
+
+  else if (letter == 'd' || letter == 'x' || letter == 'X')
+    error ("PRINT_OPERAND: letter %c was found & insn was not CONST_INT",
+           letter);
+
+  else if (letter == 'B')
+    fputs (code == EQ ? "z" : "n", file);
+  else if (letter == 'b')
+    fputs (code == EQ ? "n" : "z", file);
+  else if (letter == 'T')
+    fputs (code == EQ ? "f" : "t", file);
+  else if (letter == 't')
+    fputs (code == EQ ? "t" : "f", file);
+
+  else if (code == CONST && GET_CODE (XEXP (op, 0)) == REG)
+  {
+    print_operand (file, XEXP (op, 0), letter);
+  }
+  else if (letter == 'I')
+    div_count++;
+  else
+    output_addr_const (file, op);
+}
+
+/* A C compound statement to output to stdio stream STREAM the
+   assembler syntax for an instruction operand that is a memory
+   reference whose address is ADDR.  ADDR is an RTL expression.
+
+   Possible address classifications and output formats are,
+   
+   ADDRESS_REG                  "%0, r0"
+
+   ADDRESS_REG with non-zero    "%0, <addr_const>"
+   offset       
+
+   ADDRESS_REG_INDEX            "rA, RB"    
+                                (if rA is r0, rA and rB are swapped)
+
+   ADDRESS_CONST_INT            "r0, <addr_const>"
+
+   ADDRESS_SYMBOLIC             "rBase, <addr_const>"   
+                                (rBase is a base register suitable for the symbol's type)
+*/
+
+void
+print_operand_address (file, addr)
+  FILE *file;
+  rtx addr;
+{
+  struct microblaze_address_info info;
+  if (!microblaze_classify_address (&info, addr, GET_MODE (addr), 1)) 
+    fatal_insn ("insn contains an invalid address !", addr);
+  
+  switch (info.type) {
+    case ADDRESS_REG:
+      fprintf (file, "%s,", reg_names[REGNO (info.regA)]);
+      output_addr_const (file, info.offset);             
+      break;
+    case ADDRESS_REG_INDEX:
+      if (REGNO (info.regA) == 0)                                       /* Make rB == r0 instead of rA == r0. This helps reduce read port congestion */
+        fprintf (file, "%s,%s", reg_names[REGNO (info.regB)], reg_names[REGNO (info.regA)]);    
+      else if (REGNO (info.regB) != 0)
+        fprintf (file, "%s,%s", reg_names[REGNO (info.regB)], reg_names[REGNO (info.regA)]);        /* This is a silly swap to help Dhrystone */
+      break;
+    case ADDRESS_CONST_INT:
+      fprintf (file, "%s,", reg_names[REGNO (info.regA)]);
+      output_addr_const (file, info.offset);
+      break;
+    case ADDRESS_SYMBOLIC:
+      fprintf (file, "%s,", reg_names[REGNO (info.regA)]);
+      output_addr_const (file, info.symbol);
+      break;
+  }
+}
+
+/* If optimizing for the global pointer, keep track of all of the externs, so
+   that at the end of the file, we can emit the appropriate .extern
+   declaration for them, before writing out the text section.  We assume all
+   names passed to us are in the permanent obstack, so they will be valid at
+   the end of the compilation.
+
+   If we have -G 0, or the extern size is unknown, or the object is in a user
+   specified section that is not .sbss/.sdata, don't bother emitting the
+   .externs.  In the case of user specified sections this behaviour is
+   required as otherwise GAS will think the object lives in .sbss/.sdata.  */
+
+int
+microblaze_output_external (file, decl, name)
+  FILE *file ATTRIBUTE_UNUSED;
+  tree decl;
+  char *name;
+{
+  register struct extern_list *p;
+  int len;
+  tree section_name;
+
+  if (TARGET_GP_OPT
+      && TREE_CODE (decl) != FUNCTION_DECL
+      && (len = int_size_in_bytes (TREE_TYPE (decl))) > 0
+      && ((section_name = DECL_SECTION_NAME (decl)) == NULL
+          || strcmp (TREE_STRING_POINTER (section_name), ".sbss") == 0
+          || strcmp (TREE_STRING_POINTER (section_name), ".sdata") == 0))
+  {
+    p = (struct extern_list *) permalloc (sizeof (struct extern_list));
+    p->next = extern_head;
+    p->name = name;
+    p->size = len;
+    extern_head = p;
+  }
+
+#ifdef ASM_OUTPUT_UNDEF_FUNCTION
+  if (TREE_CODE (decl) == FUNCTION_DECL
+      /* ??? Don't include alloca, since gcc will always expand it
+         inline.  If we don't do this, the C++ library fails to build.  */
+      && strcmp (name, "alloca")
+      /* ??? Don't include __builtin_next_arg, because then gcc will not
+         bootstrap under Irix 5.1.  */
+      && strcmp (name, "__builtin_next_arg"))
+  {
+    p = (struct extern_list *) permalloc (sizeof (struct extern_list));
+    p->next = extern_head;
+    p->name = name;
+    p->size = -1;
+    extern_head = p;
+  }
+#endif
+
+  return 0;
+}
+
+#ifdef ASM_OUTPUT_UNDEF_FUNCTION
+int
+microblaze_output_external_libcall (file, name)
+  FILE *file;
+  char *name;
+{
+  register struct extern_list *p;
+
+  p = (struct extern_list *) permalloc (sizeof (struct extern_list));
+  p->next = extern_head;
+  p->name = name;
+  p->size = -1;
+  extern_head = p;
+
+  return 0;
+}
+#endif
+
+/* Compute a string to use as a temporary file name.  */
+
+/* On MSDOS, write temp files in current dir
+   because there's no place else we can expect to use.  */
+#if __MSDOS__
+#ifndef P_tmpdir
+#define P_tmpdir "./"
+#endif
+#endif
+
+/* Emit a new filename to a stream.  If this is MICROBLAZE ECOFF, watch out
+   for .file's that start within a function.  If we are smuggling stabs, try to
+   put out a MICROBLAZE ECOFF file and a stab.  */
+
+void
+microblaze_output_filename (stream, name)
+  FILE *stream;
+  const char* name;
+{
+  static int first_time = 1;
+  char ltext_label_name[100];
+
+  if (first_time)
+  {
+    first_time = 0;
+    SET_FILE_NUMBER ();
+    current_function_file = name;
+    ASM_OUTPUT_FILENAME (stream, num_source_filenames, name);
+    /* This tells microblaze-tfile that stabs will follow.  */
+    if (!TARGET_GAS && write_symbols == DBX_DEBUG)
+      fprintf (stream, "\t#@stabs\n");
+  }
+
+  else if (write_symbols == DBX_DEBUG)
+  {
+    ASM_GENERATE_INTERNAL_LABEL (ltext_label_name, "Ltext", 0);
+    fprintf (stream, "%s ", ASM_STABS_OP);
+    output_quoted_string (stream, name);
+    fprintf (stream, ",%d,0,0,%s\n", N_SOL, &ltext_label_name[1]);
+  }
+
+  else if (name != current_function_file
+           && strcmp (name, current_function_file) != 0)
+  {
+    if (inside_function && !TARGET_GAS)
+    {
+      if (!file_in_function_warning)
+      {
+        file_in_function_warning = 1;
+        ignore_line_number = 1;
+        warning ("MICROBLAZE ECOFF format does not allow changing filenames within functions with #line");
+      }
+    }
+    else
+    {
+      SET_FILE_NUMBER ();
+      current_function_file = name;
+      ASM_OUTPUT_FILENAME (stream, num_source_filenames, name);
+    }
+  }
+}
+
+/* Emit a linenumber.  For encapsulated stabs, we need to put out a stab
+   as well as a .loc, since it is possible that MICROBLAZE ECOFF might not be
+   able to represent the location for inlines that come from a different
+   file.  */
+
+void
+microblaze_output_lineno (stream, line)
+  FILE *stream;
+  int line;
+{
+  if (write_symbols == DBX_DEBUG)
+  {
+    ++sym_lineno;
+    fprintf (stream, "%sLM%d:\n\t%s %d,0,%d,%sLM%d",
+             LOCAL_LABEL_PREFIX, sym_lineno, ASM_STABN_OP, N_SLINE, line,
+             LOCAL_LABEL_PREFIX, sym_lineno);
+    fputc ('-', stream);
+    assemble_name (stream, XSTR (XEXP (DECL_RTL (current_function_decl), 0), 0));
+    fprintf(stream,"\n");
+  }
+
+  else
+  {
+    fprintf (stream, "\n\t%s.loc\t%d %d\n",
+             (ignore_line_number) ? "#" : "",
+             num_source_filenames, line);
+  
+    LABEL_AFTER_LOC (stream);
+  }
+}
+
+/* Code to be executed just prior to the output of assembler code for INSN, 
+   to modify the extracted operands so they will be output differently.
+
+   Here the argument OPVEC is the vector containing the operands extracted
+   from INSN, and NOPERANDS is the number of elements of the vector which
+   contain meaningful data for this insn.  The contents of this vector are
+   what will be used to convert the insn template into assembler code, so you
+   can change the assembler output by changing the contents of the vector.
+
+   We use it to 
+    1) For the Gti pipeline, swap Ra and Rb, is Ra has the potential of being forwarded. 
+       There is congestion at the read port for Rd and Ra. So we want 
+       to minimize the number of times that this congestion occurs
+    3) Update the delay slot statistics.  */
+void
+final_prescan_insn (rtx insn, rtx opvec[], int noperands)
+{
+  /* GTi stuff here 
+     -- Scan instruction sequence to see if a stall on a 'sw' can be avoided
+        by swapping operands, thus using a forwarding path from a preceding
+        instruction.
+
+        FIXME
+  */
+  
+  if (TARGET_STATS
+      && (GET_CODE (insn) == JUMP_INSN || GET_CODE (insn) == CALL_INSN))
+    dslots_jump_total++;
+}
+
+
+/* Output at beginning of assembler file.
+
+If we are optimizing to use the global pointer, create a temporary file to
+hold all of the text stuff, and write it out to the end. This is needed
+because the MICROBLAZE assembler is evidently one pass, and if it hasn't seen the
+relevant .comm/.lcomm/.extern/.sdata declaration when the code is
+processed, it generates a two instruction sequence.  */
+
+void
+microblaze_asm_file_start (void)
+{
+  default_file_start ();
+  /* GCC 3.4.1
+   * Removed. Caused segfaults.
+   */
+  /* ASM_OUTPUT_SOURCE_FILENAME (asm_out_file, main_input_filename); */
+    
+  /* Generate the pseudo ops that System V.4 wants.  */
+#ifndef ABICALLS_ASM_OP
+#define ABICALLS_ASM_OP ".abicalls"
+#endif
+  if (TARGET_ABICALLS)
+    /* ??? but do not want this (or want pic0) if -non-shared? */
+    fprintf (asm_out_file, "\t%s\n", ABICALLS_ASM_OP);
+    
+    
+  /* This code exists so that we can put all externs before all symbol
+     references.  This is necessary for the MICROBLAZE assembler's global pointer
+     optimizations to work.  */
+  if (TARGET_FILE_SWITCHING )
+  {
+    asm_out_data_file = asm_out_file;
+    /* asm_out_text_file = xlnx_make_temp_file (); */
+    asm_out_text_file = tmpfile ();
+  }
+
+  else
+    asm_out_data_file = asm_out_text_file = asm_out_file;
+
+  if (flag_verbose_asm)
+    fprintf (asm_out_file, "\n%s -G value = %d\n",
+             ASM_COMMENT_START,
+             microblaze_section_threshold);
+}
+
+/* If we are optimizing the global pointer, emit the text section now and any
+   small externs which did not have .comm, etc that are needed.  Also, give a
+   warning if the data area is more than 32K and -pic because 3 instructions
+   are needed to reference the data pointers.  */
+
+void
+microblaze_asm_file_end ()
+{
+  char buffer[8192];
+  tree name_tree;
+  struct extern_list *p;
+  int len;
+
+  if (HALF_PIC_P ())
+  {
+    HALF_PIC_FINISH (asm_out_file);
+  }
+
+  if (extern_head)
+  {
+    fputs ("\n", asm_out_file);
+
+    for (p = extern_head; p != 0; p = p->next)
+    {
+      name_tree = get_identifier (p->name);
+
+      /* Positively ensure only one .extern for any given symbol.  */
+      if (! TREE_ASM_WRITTEN (name_tree))
+      {
+        TREE_ASM_WRITTEN (name_tree) = 1;
+#ifdef ASM_OUTPUT_UNDEF_FUNCTION
+        if (p->size == -1)
+          ASM_OUTPUT_UNDEF_FUNCTION (asm_out_file, p->name);
+        else
+#endif
+        {
+          fputs ("\t.extern\t", asm_out_file);
+          assemble_name (asm_out_file, p->name);
+          fprintf (asm_out_file, ", %d\n", p->size);
+        }
+      }
+    }
+  }
+      
+  if (TARGET_FILE_SWITCHING )
+  {
+    fprintf (asm_out_file, "\n\t.text\n");
+    rewind (asm_out_text_file);
+    if (ferror (asm_out_text_file))
+      fatal_io_error (temp_filename);
+
+    while ((len = fread (buffer, 1, sizeof (buffer), asm_out_text_file)) > 0)
+      if ((int) fwrite (buffer, 1, len, asm_out_file) != len)
+        pfatal_with_name (asm_file_name);
+
+    if (len < 0)
+      pfatal_with_name (temp_filename);
+
+    if (fclose (asm_out_text_file) != 0)
+      pfatal_with_name (temp_filename);
+
+#ifdef __MSDOS__
+    unlink (temp_filename);
+#endif
+  }
+}
+
+/* Output an element in the table of global constructors. */
+void 
+microblaze_asm_constructor (rtx symbol, int priority) 
+{ 
+
+  const char *section = ".ctors";
+  char buf[16];
+
+  if (priority != DEFAULT_INIT_PRIORITY)
+  {
+    sprintf (buf, ".ctors.%.5u",
+             /* Invert the numbering so the linker puts us in the proper
+                order; constructors are run from right to left, and the
+                linker sorts in increasing order.  */
+             MAX_INIT_PRIORITY - priority);
+    section = buf;
+  }
+
+  named_section_flags (section, SECTION_WRITE);
+  fprintf (asm_out_file, "\t%s\t", ".word");     
+  assemble_name (asm_out_file, XEXP (symbol, 0));                                       
+  fprintf (asm_out_file, "\n");                                             
+} 
+
+/* Output an element in the table of global destructors. */
+void 
+microblaze_asm_destructor (rtx symbol, int priority) 
+{ 
+  const char *section = ".dtors";
+  char buf[16];
+
+  if (priority != DEFAULT_INIT_PRIORITY)
+  {
+    sprintf (buf, ".dtors.%.5u",
+             /* Invert the numbering so the linker puts us in the proper
+                order; constructors are run from right to left, and the
+                linker sorts in increasing order.  */
+             MAX_INIT_PRIORITY - priority);
+    section = buf;
+  }
+
+  named_section_flags (section, SECTION_WRITE);
+  fprintf (asm_out_file, "\t%s\t", ".word");     
+  assemble_name (asm_out_file, XEXP (symbol, 0));                                       
+  fprintf (asm_out_file, "\n");                                             
+} 
+   
+
+/* A function to output to the stdio stream stream a label whose name is made from the string prefix 
+   and the number labelno. */
+void 
+microblaze_internal_label (STREAM, prefix, labelno)
+  FILE *STREAM;
+  const char* prefix;
+  unsigned long labelno;
+{
+  fprintf (STREAM, "%s%s%ld:\n", LOCAL_LABEL_PREFIX, prefix, labelno);
+}
+
+/* Emit either a label, .comm, or .lcomm directive, and mark that the symbol
+   is used, so that we don't emit an .extern for it in microblaze_asm_file_end.  */
+
+void
+microblaze_declare_object (FILE *stream, char *name, char *section, char *fmt, int size)
+{
+
+  fputs (section, stream);		/* "", "\t.comm\t", or "\t.lcomm\t" */
+  assemble_name (stream, name);
+  fprintf (stream, fmt, size);	/* ":\n", ",%u\n", ",%u\n" */
+
+  if (TARGET_GP_OPT)
+  {
+    tree name_tree = get_identifier (name);
+    TREE_ASM_WRITTEN (name_tree) = 1;
+  }
+}
+
+void
+microblaze_declare_comm_object (FILE *stream, char *name, char *section, char *fmt, int size, int align)
+{
+  if (size > 0 && size <= microblaze_section_threshold && TARGET_XLGP_OPT)
+    SBSS_SECTION();
+  else
+    BSS_SECTION();
+
+  fputs (section, stream);		
+  assemble_name (stream, name);
+  fprintf (stream, fmt, size, ((align)/BITS_PER_UNIT));	 
+  ASM_OUTPUT_TYPE_DIRECTIVE (stream, name, "object");		       
+/*   if (!flag_inhibit_size_directive) */
+/*     ASM_OUTPUT_SIZE_DIRECTIVE (stream, name, size); */
+
+  if (TARGET_GP_OPT)
+  {
+    tree name_tree = get_identifier (name);
+    TREE_ASM_WRITTEN (name_tree) = 1;
+  }
+}
+
+/* Output a double precision value to the assembler.  If both the
+   host and target are IEEE, emit the values in hex.  */
+
+void
+microblaze_output_double (FILE *stream, REAL_VALUE_TYPE value)
+{
+#ifdef REAL_VALUE_TO_TARGET_DOUBLE
+  long value_long[2];
+  REAL_VALUE_TO_TARGET_DOUBLE (value, value_long);
+   
+  fprintf (stream, "\t.word\t0x%08lx\t\t# %.20g\n\t.word\t0x%08lx\n",
+           value_long[0], value, value_long[1]);
+#else
+  fprintf (stream, "\t.double\t%.20g\n", value);
+#endif
+}
+
+/* Output a single precision value to the assembler.  If both the
+   host and target are IEEE, emit the values in hex.  */
+
+void
+microblaze_output_float (FILE *stream, REAL_VALUE_TYPE value)
+{
+#ifdef REAL_VALUE_TO_TARGET_SINGLE
+  long value_long;
+  REAL_VALUE_TO_TARGET_SINGLE (value, value_long);
+    
+  fprintf (stream, "\t.word\t0x%08lx\t\t# %.12g (float)\n", value_long, value);
+#else
+  fprintf (stream, "\t.float\t%.12g\n", value);
+#endif
+}
+
+/* Return the bytes needed to compute the frame pointer from the current
+   stack pointer.
+
+   MicroBlaze stack frames look like:
+
+
+
+             Before call		        After call
+        +-----------------------+	+-----------------------+
+   high |			|       |      			|
+   mem. |  local variables,     |	|  local variables,	|
+        |  callee saved and     |       |  callee saved and    	|
+	|  temps     		|       |  temps     	        |
+        +-----------------------+	+-----------------------+
+        |  arguments for called	|       |  arguments for called |
+	|  subroutines		|	|  subroutines  	|
+        |  (optional)           |       |  (optional)           |
+        +-----------------------+	+-----------------------+
+	|  Link register 	|	|  Link register        |
+    SP->|                       |       |                       |
+	+-----------------------+       +-----------------------+
+					|		        |
+                                        |  local variables,     |
+                                        |  callee saved and     |
+                                        |  temps                |
+					+-----------------------+
+                                        |   MSR (optional if,   |
+                                        |   interrupt handler)  |
+					+-----------------------+
+					|			|
+                                        |  alloca allocations   |
+        				|			|
+					+-----------------------+
+					|			|
+                                        |  arguments for called |
+                                        |  subroutines          |
+                                        |  (optional)           |
+        				|		        |
+					+-----------------------+
+                                        |  Link register        |
+   low                           FP,SP->|                       |
+   memory        			+-----------------------+
+
+*/
+
+HOST_WIDE_INT
+compute_frame_size (size)
+  HOST_WIDE_INT size;                 /* # of var. bytes allocated */
+{
+  int regno;
+  HOST_WIDE_INT total_size;           /* # bytes that the entire frame takes up */
+  HOST_WIDE_INT var_size;             /* # bytes that local variables take up */
+  HOST_WIDE_INT args_size;            /* # bytes that outgoing arguments take up */
+  int link_debug_size;                /* # bytes for link register */
+  HOST_WIDE_INT gp_reg_size;          /* # bytes needed to store calle-saved gp regs */
+  long mask;                          /* mask of saved gp registers */
+  static int check = 0;
+
+  interrupt_handler   = (microblaze_interrupt_function_p (current_function_decl));
+  save_volatiles      = (microblaze_save_volatiles (current_function_decl));
+
+  gp_reg_size = 0;
+  mask = 0;
+  var_size   = size;
+  args_size  = current_function_outgoing_args_size;
+  
+  if (args_size == 0 && current_function_calls_alloca)
+    args_size = NUM_OF_ARGS * UNITS_PER_WORD;
+    
+  total_size = var_size + args_size;
+
+  /* Calculate space needed for gp registers.  */
+  for (regno = GP_REG_FIRST; regno <= GP_REG_LAST; regno++)
+  {
+    if (MUST_SAVE_REGISTER (regno))
+    {
+      
+      if (regno != MB_ABI_SUB_RETURN_ADDR_REGNUM)               /* Don't account for link register. It is accounted specially below */
+        gp_reg_size += GET_MODE_SIZE (gpr_mode);
+
+      mask |= 1L << (regno - GP_REG_FIRST);
+    }
+  }
+  
+  total_size += gp_reg_size;
+
+  /* Add 4 bytes for MSR */                        
+  if (interrupt_handler)
+    total_size += 4; 
+                                              
+  /* No space to be allocated for link register in leaf functions with no other stack requirements */
+  if (total_size == 0 && current_function_is_leaf)
+    link_debug_size = 0;
+  else
+    link_debug_size = UNITS_PER_WORD;    
+
+  total_size += link_debug_size;
+
+  /* Save other computed information.  */
+  current_frame_info.total_size = total_size;
+  current_frame_info.var_size = var_size;
+  current_frame_info.args_size = args_size;
+  current_frame_info.gp_reg_size = gp_reg_size;
+  current_frame_info.mask = mask;
+  current_frame_info.initialized = reload_completed;
+  current_frame_info.num_gp = gp_reg_size / UNITS_PER_WORD;
+  current_frame_info.link_debug_size = link_debug_size; 
+
+#if 0
+  {
+    char *fnname;
+
+    fnname = XSTR (XEXP (DECL_RTL (current_function_decl), 0), 0);
+    fprintf(stderr," ----- stack usage stats for (%s) ---- \n", fnname);
+    fprintf(stderr,"total_size %d\n",total_size);
+    fprintf(stderr,"var_size %d\n",var_size);
+    fprintf(stderr,"args_size %d\n",args_size);
+    fprintf(stderr,"gp_reg_size %d\n",gp_reg_size);
+    fprintf(stderr,"mask %x\n",mask);
+    fprintf(stderr,"num_gp %d\n",current_frame_info.num_gp);
+    fprintf(stderr,"link_debug %d\n",link_debug_size);
+    fprintf(stderr," ----- end stack usage stats ---- \n");
+  }
+#endif
+
+  if (mask)
+    current_frame_info.gp_offset = (total_size - gp_reg_size);      /* Offset from which to callee-save GP regs */
+  else
+    current_frame_info.gp_offset = 0;
+  
+  /* Ok, we're done.  */
+  return total_size;
+}
+
+/* Common code to emit the insns (or to write the instructions to a file)
+   to save/restore registers.
+
+   Other parts of the code assume that MICROBLAZE_TEMP1_REGNUM (aka large_reg)
+   is not modified within save_restore_insns.  */
+
+#define BITSET_P(VALUE,BIT) (((VALUE) & (1L << (BIT))) != 0)
+
+/* Save or restore instructions based on whether this is the prologue or epilogue. 
+   prologue is 1 for the prologue */
+static void
+save_restore_insns (int prologue)
+{
+  rtx base_reg_rtx, reg_rtx, mem_rtx, msr_rtx, isr_reg_rtx, isr_mem_rtx, isr_msr_rtx, insn;
+  long mask = current_frame_info.mask;
+  HOST_WIDE_INT base_offset, gp_offset;
+  int regno;
+
+  if (frame_pointer_needed
+      && ! BITSET_P (mask, HARD_FRAME_POINTER_REGNUM - GP_REG_FIRST))
+    abort ();
+
+  if (mask == 0)
+    return;
+
+  /* Save registers starting from high to low.  The debuggers prefer at least
+     the return register be stored at func+4, and also it allows us not to
+     need a nop in the epilog if at least one register is reloaded in
+     addition to return address.  */
+  
+  
+  /* Pick which pointer to use as a base register.  For small frames, just
+     use the stack pointer.  Otherwise, use a temporary register.  Save 2
+     cycles if the save area is near the end of a large frame, by reusing
+     the constant created in the prologue/epilogue to adjust the stack
+     frame.  */
+  
+  gp_offset  = current_frame_info.gp_offset;
+  
+  if (gp_offset <= 0)
+    error ("gp_offset (%ld) is less than or equal to zero.", (long) gp_offset);
+  
+  base_reg_rtx = stack_pointer_rtx;
+  base_offset  = 0;
+  
+  /* For interrupt_handlers, need to save/restore the MSR */
+  if (interrupt_handler) {
+    isr_mem_rtx = gen_rtx (MEM, gpr_mode, 
+                           gen_rtx (PLUS, Pmode, base_reg_rtx, 
+                                    GEN_INT (current_frame_info.gp_offset - UNITS_PER_WORD)));
+
+    RTX_UNCHANGING_P (isr_mem_rtx) = 1;
+    MEM_VOLATILE_P (isr_mem_rtx) = 1;                             /* Do not optimize in flow analysis */
+    isr_reg_rtx = gen_rtx (REG, gpr_mode, MB_ABI_MSR_SAVE_REG);
+    isr_msr_rtx = gen_rtx (REG, gpr_mode, ST_REG_FIRST);
+  }
+
+  if (interrupt_handler && !prologue) {
+    emit_move_insn (isr_reg_rtx, isr_mem_rtx);
+    emit_move_insn (isr_msr_rtx, isr_reg_rtx);
+    emit_insn (gen_rtx_USE (SImode, isr_reg_rtx));                  /* Do not optimize in flow analysis */
+    emit_insn (gen_rtx_USE (SImode, isr_msr_rtx));                  /* Do not optimize in flow analysis */
+  } 
+
+  for (regno = GP_REG_FIRST; regno <= GP_REG_LAST; regno++) 
+  {
+    if (BITSET_P (mask, regno - GP_REG_FIRST))
+    {
+      if (regno == MB_ABI_SUB_RETURN_ADDR_REGNUM)             /* Don't handle here. Already handled as the first register */
+        continue;
+
+      reg_rtx = gen_rtx (REG, gpr_mode, regno);  
+      mem_rtx = gen_rtx (MEM, gpr_mode, gen_rtx (PLUS, Pmode, base_reg_rtx, GEN_INT (gp_offset)));    
+      RTX_UNCHANGING_P (mem_rtx) = 1;
+      if (interrupt_handler)
+        MEM_VOLATILE_P (mem_rtx) = 1;                         /* Do not optimize in flow analysis */    
+
+      if (prologue)
+      {
+        insn = emit_move_insn (mem_rtx, reg_rtx);
+        RTX_FRAME_RELATED_P (insn) = 1;
+      }
+      else if (!TARGET_ABICALLS 
+               || regno != (PIC_OFFSET_TABLE_REGNUM - GP_REG_FIRST))
+      {
+        emit_move_insn (reg_rtx, mem_rtx);
+      }
+      
+      gp_offset += GET_MODE_SIZE (gpr_mode);
+    }
+  }
+
+  if (interrupt_handler && prologue) {
+    emit_move_insn (isr_reg_rtx, isr_msr_rtx);
+    emit_move_insn (isr_mem_rtx, isr_reg_rtx);
+
+    emit_insn (gen_rtx_USE (SImode, isr_reg_rtx));                  /* Do not optimize in flow analysis */
+    emit_insn (gen_rtx_USE (SImode, isr_msr_rtx));                  /* Do not optimize in flow analysis */
+  }
+
+  /* Done saving and restoring */
+}
+
+
+/* Set up the stack and frame (if desired) for the function.  */
+static void
+microblaze_function_prologue (file, size)
+  FILE *file;
+  int size ATTRIBUTE_UNUSED;
+{
+#ifndef FUNCTION_NAME_ALREADY_DECLARED
+  char *fnname;
+#endif
+  long fsiz = current_frame_info.total_size;
+
+#if 0   
+  ASM_OUTPUT_SOURCE_FILENAME (file, DECL_SOURCE_FILE (current_function_decl));
+
+#ifdef SDB_DEBUGGING_INFO
+  if (debug_info_level != DINFO_LEVEL_TERSE && write_symbols == SDB_DEBUG)
+    ASM_OUTPUT_SOURCE_LINE (file, DECL_SOURCE_LINE (current_function_decl));
+#endif
+#endif
+  inside_function = 1;
+
+#ifndef FUNCTION_NAME_ALREADY_DECLARED
+  /* Get the function name the same way that toplev.c does before calling
+     assemble_start_function.  This is needed so that the name used here
+     exactly matches the name used in ASM_DECLARE_FUNCTION_NAME.  */
+  fnname = XSTR (XEXP (DECL_RTL (current_function_decl), 0), 0);
+  if (!flag_inhibit_size_directive)
+  {
+    fputs ("\t.ent\t", file);
+    if (interrupt_handler && strcmp(INTERRUPT_HANDLER_NAME,fnname))  
+      fputs ("_interrupt_handler", file); 
+    else 
+      assemble_name (file, fnname); 
+    fputs ("\n", file);
+  }
+
+  assemble_name (file, fnname);
+  fputs (":\n", file);
+
+  if (interrupt_handler && strcmp (INTERRUPT_HANDLER_NAME, fnname)) 
+    fputs ("_interrupt_handler:\n",file); 
+#endif
+
+  if (!flag_inhibit_size_directive)
+  {
+    /* .frame FRAMEREG, FRAMESIZE, RETREG */
+    fprintf (file,
+             "\t.frame\t%s,%ld,%s\t\t# vars= %ld, regs= %d, args= %d\n",
+             (reg_names[(frame_pointer_needed)
+                        ? HARD_FRAME_POINTER_REGNUM : STACK_POINTER_REGNUM]),
+             fsiz,
+             reg_names[MB_ABI_SUB_RETURN_ADDR_REGNUM + GP_REG_FIRST],
+             current_frame_info.var_size,
+             current_frame_info.num_gp,
+             current_function_outgoing_args_size);
+    fprintf (file, "\t.mask\t0x%08lx\n", current_frame_info.mask);
+  }
+}
+
+/* Output extra assembler code at the end of a prologue */
+void
+microblaze_function_end_prologue (FILE *file)
+{
+  if (TARGET_STACK_CHECK) {
+    fprintf (file, "\t# Stack Check Stub -- Start.\n\t");
+    fprintf (file, "ori\tr18,r0,_stack_end\n\t");
+    fprintf (file, "cmpu\tr18,r1,r18\n\t");
+    fprintf (file, "bgei\tr18,_stack_overflow_exit\n\t");
+    fprintf (file, "# Stack Check Stub -- End.\n");        
+  }
+}
+
+/* Expand the prologue into a bunch of separate insns.  */
+
+void
+microblaze_expand_prologue ()
+{
+  int regno;
+  HOST_WIDE_INT fsiz;
+  char *arg_name = 0;
+  tree fndecl = current_function_decl;
+  tree fntype = TREE_TYPE (fndecl);
+  tree fnargs = DECL_ARGUMENTS (fndecl);
+  rtx next_arg_reg;
+  int i;
+  tree next_arg;
+  tree cur_arg;
+  CUMULATIVE_ARGS args_so_far;
+  rtx reg_18_save = NULL_RTX;
+  rtx mem_rtx, reg_rtx, insn;
+
+  /* If struct value address is treated as the first argument, make it so.  */
+  if (aggregate_value_p (DECL_RESULT (fndecl), fntype)
+      && ! current_function_returns_pcc_struct)                                               
+  {
+    tree type = build_pointer_type (fntype);
+    tree function_result_decl = build_decl (PARM_DECL, NULL_TREE, type);
+
+    DECL_ARG_TYPE (function_result_decl) = type;
+    TREE_CHAIN (function_result_decl) = fnargs;
+    fnargs = function_result_decl;
+  }
+
+  /* Determine the last argument, and get its name.  */
+
+  INIT_CUMULATIVE_ARGS (args_so_far, fntype, NULL_RTX, 0, 0);
+  regno = GP_ARG_FIRST;
+
+  for (cur_arg = fnargs; cur_arg != 0; cur_arg = next_arg)
+  {
+    tree passed_type = DECL_ARG_TYPE (cur_arg);
+    enum machine_mode passed_mode = TYPE_MODE (passed_type);
+    rtx entry_parm;
+
+    if (TREE_ADDRESSABLE (passed_type))
+    {
+      passed_type = build_pointer_type (passed_type);
+      passed_mode = Pmode;
+    }
+
+    entry_parm = FUNCTION_ARG (args_so_far, passed_mode, passed_type, 1);
+
+    if (entry_parm)
+    {
+      int words;
+
+      /* passed in a register, so will get homed automatically */
+      if (GET_MODE (entry_parm) == BLKmode)
+        words = (int_size_in_bytes (passed_type) + 3) / 4;
+      else
+        words = (GET_MODE_SIZE (GET_MODE (entry_parm)) + 3) / 4;
+
+      regno = REGNO (entry_parm) + words - 1;
+    }
+    else
+    {
+      regno = GP_ARG_LAST+1;
+      break;
+    }
+
+    FUNCTION_ARG_ADVANCE (args_so_far, passed_mode, passed_type, 1);
+
+    next_arg = TREE_CHAIN (cur_arg);
+    if (next_arg == 0)
+    {
+      if (DECL_NAME (cur_arg))
+        arg_name = IDENTIFIER_POINTER (DECL_NAME (cur_arg));
+
+      break;
+    }
+  }
+
+  /* In order to pass small structures by value in registers compatibly with
+     the MicroBlaze compiler, we need to shift the value into the high part of the
+     register.  Function_arg has encoded a PARALLEL rtx, holding a vector of
+     adjustments to be made as the next_arg_reg variable, so we split up the
+     insns, and emit them separately.  */
+
+  /* IN MicroBlaze shift has been modified to be a combination of adds
+     and shifts in other directions, Hence we need to change the code
+     a bit */
+
+  next_arg_reg = FUNCTION_ARG (args_so_far, VOIDmode, void_type_node, 1);
+  if (next_arg_reg != 0 && GET_CODE (next_arg_reg) == PARALLEL)
+  {
+    rtvec adjust = XVEC (next_arg_reg, 0);
+    int num = GET_NUM_ELEM (adjust);
+
+    for (i = 0; i < num; i++)
+    {
+      rtx pattern = RTVEC_ELT (adjust, i);
+      /* 	  if (GET_CODE (pattern) != SET */
+      /* 	      || GET_CODE (SET_SRC (pattern)) != ASHIFT) */
+      /* 	    fatal_insn ("Insn is not a shift", pattern); */
+      /* 	  if (GET_CODE (pattern) != SET )
+                  fatal_insn ("Insn is Not Set", pattern); */
+
+      /*	  PUT_CODE (SET_SRC (pattern), ASHIFTRT);*/
+      emit_insn (pattern);
+    }
+  }
+
+  fsiz = compute_frame_size (get_frame_size ());
+
+  /* If this function is a varargs function, store any registers that
+     would normally hold arguments ($5 - $10) on the stack.  */
+  if (((TYPE_ARG_TYPES (fntype) != 0
+        && (TREE_VALUE (tree_last (TYPE_ARG_TYPES (fntype)))
+            != void_type_node))
+       || (arg_name != 0
+           && ((arg_name[0] == '_'
+                && strcmp (arg_name, "__builtin_va_alist") == 0)
+               || (arg_name[0] == 'v'
+                   && strcmp (arg_name, "va_alist") == 0)))))
+  {
+    int offset = (regno - GP_ARG_FIRST + 1) * UNITS_PER_WORD;
+    rtx ptr = stack_pointer_rtx;
+
+    /* If we are doing svr4-abi, sp has already been decremented by fsiz. */
+    if (TARGET_ABICALLS)
+      offset += fsiz;
+
+    for (; regno <= GP_ARG_LAST; regno++)
+    {
+      if (offset != 0)
+        ptr = gen_rtx (PLUS, Pmode, stack_pointer_rtx, GEN_INT (offset));
+      emit_move_insn (gen_rtx (MEM, gpr_mode, ptr),
+                      gen_rtx (REG, gpr_mode, regno));
+         
+      offset += GET_MODE_SIZE (gpr_mode);
+    }
+      
+  }
+  
+  if (fsiz > 0)
+  {
+    rtx fsiz_rtx = GEN_INT (fsiz);
+
+    if (!TARGET_ABICALLS )
+    {
+      rtx insn = NULL;
+      insn = emit_insn (gen_subsi3 (stack_pointer_rtx, stack_pointer_rtx,
+                                    fsiz_rtx));
+      if (insn)
+        RTX_FRAME_RELATED_P (insn) = 1;
+    }
+
+    /* Handle SUB_RETURN_ADDR_REGNUM specially at first */
+    if (!current_function_is_leaf || interrupt_handler) {
+      mem_rtx = gen_rtx (MEM, gpr_mode,
+                         gen_rtx (PLUS, Pmode, stack_pointer_rtx, const0_rtx));
+      
+      RTX_UNCHANGING_P (mem_rtx) = 1;
+      if (interrupt_handler)
+        MEM_VOLATILE_P (mem_rtx) = 1;                     /* Do not optimize in flow analysis */
+    
+      reg_rtx = gen_rtx (REG, gpr_mode, MB_ABI_SUB_RETURN_ADDR_REGNUM);
+      insn = emit_move_insn (mem_rtx, reg_rtx);
+      RTX_FRAME_RELATED_P (insn) = 1;
+     }
+
+    save_restore_insns (1);                             /* _save_ registers for prologue */
+  
+    if (frame_pointer_needed)
+    {
+      rtx insn = 0;
+
+      insn = emit_insn (gen_movsi (hard_frame_pointer_rtx,
+                                   stack_pointer_rtx));
+      
+      if (insn)
+        RTX_FRAME_RELATED_P (insn) = 1;
+    }
+
+  }
+
+  /* If we are profiling, make sure no instructions are scheduled before
+     the call to mcount.  */
+  /* profile_block_flag killed in GCC 3.4.1 */
+   
+  /*   if (profile_flag || profile_block_flag)*/
+  if (profile_flag)
+    emit_insn (gen_blockage ());
+
+  /* [02/01/02] This section checks the stack at runtime to see if it
+     has passed the malloc_base_Addr */
+
+  /*  GCC 3.4.1
+   *  This does not work reliably. Instruction scheduling seems intent on 
+   *  killing the stack check insns and later complains about deleting
+   *  frame related stuff. Doing the stack check in function_end_prologue instead.
+   */ 
+  /* 
+     if (TARGET_STACK_CHECK){
+     rtx insn;
+     insn = emit_insn (gen_stack_check ());
+     emit_insn (gen_blockage ());
+     }
+  */
+}
+
+
+
+/* Do any necessary cleanup after a function to restore stack, frame,
+   and regs. */
+
+#define RA_MASK ((long) 0x80000000)	/* 1 << 31 */
+#define PIC_OFFSET_TABLE_MASK (1 << (PIC_OFFSET_TABLE_REGNUM - GP_REG_FIRST))
+
+void
+microblaze_function_epilogue (file, size)
+  FILE *file ATTRIBUTE_UNUSED;
+  HOST_WIDE_INT size ATTRIBUTE_UNUSED;
+{
+  char *fnname;
+
+#ifndef FUNCTION_NAME_ALREADY_DECLARED
+  /* Get the function name the same way that toplev.c does before calling
+     assemble_start_function.  This is needed so that the name used here
+     exactly matches the name used in ASM_DECLARE_FUNCTION_NAME.  */
+  fnname = XSTR (XEXP (DECL_RTL (current_function_decl), 0), 0);
+
+  if (!flag_inhibit_size_directive)
+  {
+    fputs ("\t.end\t", file);
+    if (interrupt_handler)
+      fputs("_interrupt_handler",file);
+    else
+      assemble_name (file, fnname);
+    fputs ("\n", file);
+  }
+#endif
+
+  if (TARGET_STATS)
+  {
+    int num_gp_regs = current_frame_info.gp_reg_size / 4;
+    int num_regs = num_gp_regs;
+    char *name = fnname;
+      
+    if (name[0] == '*')
+      name++;
+      
+    dslots_load_total += num_regs;
+      
+    fprintf (stderr,
+             "%-20s fp=%c leaf=%c alloca=%c setjmp=%c stack=%4ld arg=%3d reg=%2d delay=%3d/%3dL %3d/%3dJ refs=%3d/%3d/%3d",
+             name, frame_pointer_needed ? 'y' : 'n',
+             (current_frame_info.mask & RA_MASK) != 0 ? 'n' : 'y',
+             current_function_calls_alloca ? 'y' : 'n',
+             current_function_calls_setjmp ? 'y' : 'n',
+             current_frame_info.total_size,
+             current_function_outgoing_args_size, num_gp_regs,
+             dslots_load_total, dslots_load_filled,
+             dslots_jump_total, dslots_jump_filled,
+             num_refs[0], num_refs[1], num_refs[2]);
+
+    if (HALF_PIC_NUMBER_PTRS > prev_half_pic_ptrs)
+    {
+      fprintf (stderr,
+               " half-pic=%3d", HALF_PIC_NUMBER_PTRS - prev_half_pic_ptrs);
+      prev_half_pic_ptrs = HALF_PIC_NUMBER_PTRS;
+    }
+
+    if (HALF_PIC_NUMBER_REFS > prev_half_pic_refs)
+    {
+      fprintf (stderr,
+               " pic-ref=%3d", HALF_PIC_NUMBER_REFS - prev_half_pic_refs);
+      prev_half_pic_refs = HALF_PIC_NUMBER_REFS;
+    }
+
+    fputc ('\n', stderr);
+  }
+
+  /* Reset state info for each function.  */
+  inside_function = 0;
+  ignore_line_number = 0;
+  dslots_load_total = 0;
+  dslots_jump_total = 0;
+  dslots_load_filled = 0;
+  dslots_jump_filled = 0;
+  num_refs[0] = 0;
+  num_refs[1] = 0;
+  num_refs[2] = 0;
+  microblaze_load_reg = 0;
+  microblaze_load_reg2 = 0;
+  current_frame_info = zero_frame_info;
+
+  while (string_constants != NULL)
+  {
+    struct string_constant *next;
+
+    next = string_constants->next;
+    free (string_constants);
+    string_constants = next;
+  }
+
+  /* Restore the output file if optimizing the GP (optimizing the GP causes
+     the text to be diverted to a tempfile, so that data decls come before
+     references to the data).  */
+
+  if (TARGET_GP_OPT && ! TARGET_GAS)
+    asm_out_file = asm_out_data_file;
+}
+
+/* Expand the epilogue into a bunch of separate insns.  */
+
+void
+microblaze_expand_epilogue ()
+{
+  HOST_WIDE_INT fsiz = current_frame_info.total_size;
+  rtx fsiz_rtx = GEN_INT (fsiz);
+  rtx reg_rtx;
+  rtx mem_rtx;
+  
+  char *fnname = XSTR (XEXP (DECL_RTL (current_function_decl), 0), 0);
+    
+  /* In case of interrupt handlers use addki instead of addi for changing the stack pointer value */
+  
+  if (microblaze_can_use_return_insn ())
+  {
+    emit_jump_insn (gen_return_internal (gen_rtx (REG, Pmode,
+                                                  GP_REG_FIRST + MB_ABI_SUB_RETURN_ADDR_REGNUM)));
+    return;
+  }
+
+  if (fsiz > 0)
+  {
+    /* Restore SUB_RETURN_ADDR_REGNUM at first. This is to prevent the sequence of load-followed by a use (in rtsd) 
+       in every prologue. Saves a load-use stall cycle  :) 
+       This is also important to handle alloca. (See comments for if (frame_pointer_needed) below */
+
+    if (!current_function_is_leaf || interrupt_handler) {
+      mem_rtx = gen_rtx (MEM, gpr_mode, gen_rtx (PLUS, Pmode, stack_pointer_rtx, const0_rtx));
+      RTX_UNCHANGING_P (mem_rtx) = 1;
+      if (interrupt_handler)
+        MEM_VOLATILE_P (mem_rtx) = 1;                       /* Do not optimize in flow analysis */
+      reg_rtx = gen_rtx (REG, gpr_mode, MB_ABI_SUB_RETURN_ADDR_REGNUM);
+      emit_move_insn (reg_rtx, mem_rtx);
+    }
+
+    /* It is important that this is done after we restore the return address register (above).
+       When alloca is used, we want to restore the sub-routine return address only from the current
+       stack top and not from the frame pointer (which we restore below). (frame_pointer + 0) might have
+       been over-written since alloca allocates memory on the current stack */
+    if (frame_pointer_needed)
+      emit_insn (gen_movsi (stack_pointer_rtx, hard_frame_pointer_rtx));
+    
+    save_restore_insns (0);                             /* _restore_ registers for epilogue */
+    emit_insn (gen_blockage ());
+    emit_insn (gen_addsi3 (stack_pointer_rtx, stack_pointer_rtx, fsiz_rtx));
+  }
+
+  emit_jump_insn (gen_return_internal (gen_rtx (REG, Pmode, GP_REG_FIRST + MB_ABI_SUB_RETURN_ADDR_REGNUM)));
+}
+
+
+/* Return nonzero if this function is known to have a null epilogue.
+   This allows the optimizer to omit jumps to jumps if no stack
+   was created.  */
+
+int
+microblaze_can_use_return_insn ()
+{
+  if (! reload_completed)
+    return 0;
+
+  if (regs_ever_live[MB_ABI_SUB_RETURN_ADDR_REGNUM] || profile_flag)
+    return 0;
+
+  if (current_frame_info.initialized)
+    return current_frame_info.total_size == 0;
+
+  return compute_frame_size (get_frame_size ()) == 0;
+}
+
+/* This function returns the register class required for a secondary
+   register when copying between one of the registers in CLASS, and X,
+   using MODE.  If IN_P is nonzero, the copy is going from X to the
+   register, otherwise the register is the source.  A return value of
+   NO_REGS means that no secondary register is required.  */
+
+enum reg_class
+microblaze_secondary_reload_class (class, mode, x, in_p)
+  enum reg_class class;
+  enum machine_mode mode;
+  rtx x;
+  int in_p;
+{
+  enum reg_class gr_regs = GR_REGS;
+  int regno = -1;
+  int gp_reg_p;
+
+  if (GET_CODE (x) == SIGN_EXTEND)
+  {
+    int off = 0;
+
+    x = XEXP (x, 0);
+
+    /* We may be called with reg_renumber NULL from regclass.
+       ??? This is probably a bug.  */
+    if (reg_renumber)
+      regno = true_regnum (x);
+    else
+    {
+      while (GET_CODE (x) == SUBREG)
+      {
+        off += SUBREG_REG (x);
+        x = SUBREG_REG (x);
+      }
+
+      if (GET_CODE (x) == REG)
+        regno = REGNO (x) + off;
+    }
+  }
+
+  else if (GET_CODE (x) == REG || GET_CODE (x) == SUBREG)
+    regno = true_regnum (x);
+
+  gp_reg_p = GP_REG_P (regno);
+
+  if (MD_REG_P (regno))
+  {
+    return class == gr_regs ? NO_REGS : gr_regs;
+  }
+
+  /* We can only copy a value to a condition code register from a
+     floating point register, and even then we require a scratch
+     floating point register.  We can only copy a value out of a
+     condition code register into a general register.  */
+  if (class == ST_REGS)
+  {
+    if (in_p)
+      return FP_REGS;
+    return GP_REG_P (regno) ? NO_REGS : GR_REGS;
+  }
+  if (ST_REG_P (regno))
+  {
+    if (! in_p)
+      return FP_REGS;
+    return class == GR_REGS ? NO_REGS : GR_REGS;
+  }
+
+  return NO_REGS;
+}
+
+
+/* We keep a list of constants we which we have to add to internal
+   constant tables in the middle of large functions.  */
+
+struct constant
+{
+  struct constant *next;
+  rtx value;
+  rtx label;
+  enum machine_mode mode;
+};
+
+/* Add a constant to the list in *PCONSTANTS.  */
+
+static rtx
+add_constant (pconstants, val, mode)
+  struct constant **pconstants;
+  rtx val;
+  enum machine_mode mode;
+{
+  struct constant *c;
+
+  for (c = *pconstants; c != NULL; c = c->next)
+    if (mode == c->mode && rtx_equal_p (val, c->value))
+      return c->label;
+
+  c = (struct constant *) xmalloc (sizeof *c);
+  c->value = val;
+  c->mode = mode;
+  c->label = gen_label_rtx ();
+  c->next = *pconstants;
+  *pconstants = c;
+  return c->label;
+}
+
+
+/* Exported to toplev.c.
+   Do a final pass over the function, just before delayed branch scheduling.  */
+void
+machine_dependent_reorg ()
+{
+  return;
+
+}
+
+/* Return nonzero if X is a SIGN or ZERO extend operator.  */
+int
+extend_operator (x, mode)
+  rtx x;
+  enum machine_mode mode ATTRIBUTE_UNUSED;
+{
+  enum rtx_code code = GET_CODE (x);
+  return code == SIGN_EXTEND || code == ZERO_EXTEND;
+}
+
+/* Accept any operator that can be used to shift the high half of the
+   input value to the lower half, suitable for truncation.  The
+   remainder (the lower half of the input, and the upper half of the
+   output) will be discarded.  */
+int
+highpart_shift_operator (x, mode)
+  rtx x;
+  enum machine_mode mode ATTRIBUTE_UNUSED;
+{
+  enum rtx_code code = GET_CODE (x);
+  return (code == LSHIFTRT
+          || code == ASHIFTRT
+          || code == ROTATERT
+          || code == ROTATE);
+}
+
+/* Get the base register for accessing a value from the memory or
+   Symbol ref. Used for Microblaze Small Data Area Pointer Optimization */
+
+int
+get_base_reg(x)
+  rtx x;
+{
+  int base_reg = (flag_pic ? 
+                  MB_ABI_PIC_ADDR_REGNUM : 
+                  MB_ABI_BASE_REGNUM );
+  
+  if (TARGET_XLGP_OPT){
+    if (VAR_SECTION(x) ==  SDATA_VAR || VAR_SECTION(x) ==  SBSS_VAR )
+      base_reg = MB_ABI_GPRW_REGNUM ;
+    else if (VAR_SECTION(x) ==  SDATA2_VAR)
+      base_reg = MB_ABI_GPRO_REGNUM ;
+  }
+
+  return base_reg;
+}
+
+#define SIZE_FMT 70
+
+
+char *
+format_load_store (char* ls_fmt, 
+                   enum load_store ls_type, 
+                   enum machine_mode ls_mode,
+                   rtx operand,
+                   int offset)
+{
+
+  int base_reg = 0;
+  int i;
+  const char* ld_mode_text,*st_mode_text;
+  rtx temp;
+
+  /*  if(ls_fmt) free(ls_fmt);*/
+  ls_fmt = (char*)xmalloc(SIZE_FMT);
+  for(i = 0 ;i < SIZE_FMT; i++) ls_fmt[i] = 0;
+  /* certain operands are mem */
+  while (GET_CODE(operand)== MEM)
+    operand = XEXP (operand, 0);
+  /* set the mode_text */
+  switch(ls_mode){
+    case SImode:
+    case SFmode: 
+      st_mode_text="w";
+      ld_mode_text="w";
+      break;
+    case HImode: 
+      st_mode_text="h";
+      ld_mode_text="hu";
+      break;
+    case QImode: 
+      st_mode_text="b";
+      ld_mode_text="bu";
+      break;
+    default:
+      break;
+  }
+
+
+  if (ls_type == LOAD){
+    /* Get the base register for Memory operations */  
+    if(GET_CODE(operand) != PLUS ){
+      base_reg = get_base_reg(operand);
+    }
+    switch(GET_CODE(operand)){
+      /* Assumed PLUS will always be reg + constant */ 
+      case PLUS:
+        temp = XEXP (operand, 1);
+        if(INTVAL(temp) + offset)
+          sprintf(ls_fmt,"l%si\t%%0,%%1",
+                  ld_mode_text);
+        break; 
+      case SYMBOL_REF:
+        sprintf(ls_fmt,"l%si\t%%0,%%1",
+                ld_mode_text);
+      
+        break;
+      case CONST:
+      case CONST_INT:
+        sprintf(ls_fmt,"l%si\t%%0,%%1",
+                ld_mode_text);
+        break; 
+      case REG:
+          sprintf(ls_fmt,"l%si\t%%0,%%1",
+                  ld_mode_text);
+        break; 
+      default:
+        fprintf(stderr,"Error!! unknown block move %d\n",GET_CODE(operand));
+        return "MicroBlaze Code error here %0 %1 LOAD";
+    } /* operands1 switch */
+    return ls_fmt;
+  }
+  else
+  {
+    /* Get the base register for Memory operations */  
+    if(GET_CODE(operand) != PLUS)
+      base_reg = get_base_reg(operand);
+
+    switch(GET_CODE(operand)){
+      /* Assumed PLUS will always be reg + constant */ 
+      case PLUS:
+        temp = XEXP (operand, 1);
+        if(INTVAL(temp) + offset)
+          sprintf(ls_fmt,"s%si\t%%0,%%1",
+                  st_mode_text);
+        break;
+         
+      case SYMBOL_REF:
+        sprintf(ls_fmt,"s%si\t%%0,%%1",
+                st_mode_text);        
+        break;
+         
+      case CONST:
+      case CONST_INT:
+        sprintf(ls_fmt,"s%si\t%%0,%%1",
+                st_mode_text);
+        break;    
+      case REG:
+        sprintf(ls_fmt,"s%si\t%%0,%%1",
+                st_mode_text);
+        break;
+      default:
+        fprintf(stderr,"Error!! unknown block move %d\n",GET_CODE(operand));
+        return "MicroBlaze Code error here %0 %1 STORE";
+    } /* operands0 switch */
+    return ls_fmt;
+      
+  } /* store part*/
+}
+
+
+/* Added to handle Xilinx interrupt handler for MicroBlaze */
+
+int
+microblaze_valid_machine_decl_attribute (decl, attributes, attr, args)
+  tree decl;
+  tree attributes;
+  tree attr;
+  tree args;
+{
+  if (args != NULL_TREE)
+    return 0;
+
+  if (is_attribute_p ("interrupt_handler", attr) ||
+      is_attribute_p ("save_volatiles", attr)){
+    /*    fprintf(stderr,"INTERRUPT HANDLER RECOGNIZED\n");*/
+    return TREE_CODE (decl) == FUNCTION_DECL;
+  }
+ 
+  return 0;
+}
+
+/* Return nonzero if FUNC is an interrupt function as specified
+   by the "interrupt" attribute.  */
+
+/* Xilinx
+ * Eventually remove both the functions below 
+ */
+static int
+microblaze_interrupt_function_p (func)
+  tree func;
+{
+  tree a;
+
+  if (TREE_CODE (func) != FUNCTION_DECL)
+    return 0;
+
+  a = lookup_attribute ("interrupt_handler", DECL_ATTRIBUTES (func));
+  return a != NULL_TREE;
+}
+
+static int
+microblaze_save_volatiles (func)
+  tree func;
+{
+  tree a;
+
+  if (TREE_CODE (func) != FUNCTION_DECL)
+    return 0;
+
+  a = lookup_attribute ("save_volatiles", DECL_ATTRIBUTES (func)); 
+  return a != NULL_TREE;
+}
+
+int microblaze_is_interrupt_handler(){
+  return interrupt_handler;
+}
+
+static void
+microblaze_globalize_label (stream, name)
+  FILE *stream;
+  const char *name;
+{									
+  fputs ("\t.globl\t", stream);					
+  if (interrupt_handler && strcmp (name, INTERRUPT_HANDLER_NAME)){      
+    fputs (INTERRUPT_HANDLER_NAME, stream);                          
+    fputs ("\n\t.globl\t", stream);                                  
+  }									
+  assemble_name (stream, name);					
+  fputs ("\n", stream);						
+} 
+
+
+
+/* Choose the section to use for the constant rtx expression X that has
+   mode MODE.  */
+
+/* This macro is not defined any more. The constants will be moved to
+   Readonly section */
+void
+microblaze_select_rtx_section (mode, x, align)
+  enum machine_mode mode;
+  rtx x ATTRIBUTE_UNUSED;
+  unsigned HOST_WIDE_INT align;
+{
+  READONLY_DATA_SECTION ();
+}
+
+/* Choose the section to use for DECL.  RELOC is true if its value contains
+   any relocatable expression.
+
+   Some of the logic used here needs to be replicated in
+   ENCODE_SECTION_INFO in microblaze.h so that references to these symbols
+   are done correctly.  Specifically, at least all symbols assigned
+   here to rom (.text and/or .rodata) must not be referenced via
+   ENCODE_SECTION_INFO with %gprel, as the rom might be too far away.
+
+   If you need to make a change here, you probably should check
+   ENCODE_SECTION_INFO to see if it needs a similar change.  */
+
+void
+microblaze_select_section (decl, reloc, align)
+  tree decl;
+  int reloc;
+  unsigned HOST_WIDE_INT align;
+{
+  int size = int_size_in_bytes (TREE_TYPE (decl));
+
+
+  /* 
+     if (DECL_NAME (decl))
+     fprintf (stderr, "microblaze_select_section: %s: \n", IDENTIFIER_POINTER (DECL_NAME (decl)));   
+  */
+    
+
+  /* 07/23/01 XLNX : Set the section to be either .data or .sdata */
+
+  /*  rtx decl_rtx = XEXP(DECL_RTL(decl),0);
+  
+  if (TREE_CODE(decl) != STRING_CST)
+  if (!(TREE_READONLY(decl)))
+  VAR_SECTION(decl_rtx)= (size <=microblaze_section_threshold && size > 0 && TARGET_XLGP_OPT) ? SDATA_VAR : DATA_VAR;
+  */
+  
+  if ((TARGET_EMBEDDED_PIC)
+      && TREE_CODE (decl) == STRING_CST
+      && !flag_writable_strings)
+    /* For embedded position independent code, put constant strings in the
+       text section, because the data section is limited to 64K in size.
+    */
+    text_section ();
+
+  /* For embedded applications, always put an object in read-only data
+     if possible, in order to reduce RAM usage.  */
+
+  if (((TREE_CODE (decl) == VAR_DECL
+        && TREE_READONLY (decl) && !TREE_SIDE_EFFECTS (decl)
+        && DECL_INITIAL (decl)
+        && (DECL_INITIAL (decl) == error_mark_node
+            || TREE_CONSTANT (DECL_INITIAL (decl))))
+       /* Deal with calls from output_constant_def_contents.  */
+       || (TREE_CODE (decl) != VAR_DECL
+           && (TREE_CODE (decl) != STRING_CST
+               || !flag_writable_strings)))
+      && ! (flag_pic && reloc)){
+    if(size > 0 && size <= microblaze_section_threshold && TARGET_XLGP_OPT)
+      READONLY_SDATA_SECTION ();
+    else
+      READONLY_DATA_SECTION ();
+  }
+  else if (size > 0 && size <= microblaze_section_threshold && TARGET_XLGP_OPT)
+    SDATA_SECTION ();
+  else
+    data_section ();
+}
+
+
+
+static void
+microblaze_unique_section(decl, reloc)
+  tree decl;
+  int reloc;
+{
+
+  int len, size, sec;
+  char *name, *string, *prefix;
+  const char *prefixes[4][2] = {
+    { ".text.", ".gnu.linkonce.t." },
+    { ".rodata.", ".gnu.linkonce.r." },
+    { ".data.", ".gnu.linkonce.d." },
+    { ".sdata.", ".gnu.linkonce.s." }
+  };
+    
+  name = IDENTIFIER_POINTER (DECL_ASSEMBLER_NAME (decl));
+  size = int_size_in_bytes (TREE_TYPE (decl));
+
+  /* Determine the base section we are interested in:
+     0=text, 1=rodata, 2=data, 3=sdata.  */
+  if (TREE_CODE (decl) == FUNCTION_DECL)
+    sec = 0;
+  else if ((TARGET_EMBEDDED_PIC)
+           && TREE_CODE (decl) == STRING_CST
+           && !flag_writable_strings)
+  {
+    /* For embedded position independent code, put constant strings
+       in the text section, because the data section is limited to
+       64K in size.  For microblaze16 code, put strings in the text
+       section so that a PC relative load instruction can be used to
+       get their address.  */
+    sec = 0;
+  }
+  else if (TARGET_EMBEDDED_DATA)
+  {
+    /* For embedded applications, always put an object in read-only data
+       if possible, in order to reduce RAM usage.  */
+
+    if (decl_readonly_section (decl, reloc))
+      sec = 1;
+    else if (size > 0 && size <= microblaze_section_threshold)
+      sec = 3;
+    else
+      sec = 2;
+  }
+  else
+  {
+    /* For hosted applications, always put an object in small data if
+       possible, as this gives the best performance.  */
+
+    if (size > 0 && size <= microblaze_section_threshold)
+      sec = 3;
+    else if (decl_readonly_section (decl, reloc))
+      sec = 1;
+    else
+      sec = 2;
+  }
+
+  prefix = prefixes[sec][DECL_ONE_ONLY (decl)];
+  len = strlen (name) + strlen (prefix);
+  /*  len =  strlen (prefix);                         */
+  string = alloca (len + 1);
+  sprintf (string, "%s%s", prefix, name);
+  DECL_SECTION_NAME(decl) = build_string(len,string);
+  /*  sprintf (string, "%s", prefix);                           */
+  /*  DECL_SECTION_NAME(DECL) = build_string(len,string);*/
+}
+
+
+
+/* GCC 3.4.1
+ * Re-write this. Can use default_encode_section_info. Refer to rs6000/mips.
+ */
+
+static void
+microblaze_encode_section_info(DECL, rtl, new_decl_p)
+  tree DECL;
+  rtx rtl;
+  int new_decl_p;
+{
+   
+  if (TARGET_EMBEDDED_DATA						
+      && (TREE_CODE (DECL) == VAR_DECL				
+          && TREE_READONLY (DECL) && !TREE_SIDE_EFFECTS (DECL))	
+      && (!DECL_INITIAL (DECL)					
+          || TREE_CONSTANT (DECL_INITIAL (DECL))))		
+  {									
+    SYMBOL_REF_FLAG (XEXP (rtl, 0)) = 0;		
+  }									
+									
+  else if (TARGET_EMBEDDED_PIC)					
+  {									
+    if (TREE_CODE (DECL) == VAR_DECL)				
+      SYMBOL_REF_FLAG (XEXP (rtl, 0)) = 1;		
+    else if (TREE_CODE (DECL) == FUNCTION_DECL)			
+      SYMBOL_REF_FLAG (XEXP (rtl, 0)) = 0;		
+    else if (TREE_CODE (DECL) == STRING_CST				
+             && ! flag_writable_strings)				
+      SYMBOL_REF_FLAG (XEXP (rtl, 0)) = 0;		
+    else								
+      SYMBOL_REF_FLAG (XEXP (rtl, 0)) = 1;		
+  }									
+									
+  else if (TREE_CODE (DECL) == VAR_DECL				
+           && DECL_SECTION_NAME (DECL) != NULL_TREE                   
+           && (0 == strcmp (TREE_STRING_POINTER (DECL_SECTION_NAME (DECL)), 
+                            ".sdata")                                 
+               || 0 == strcmp (TREE_STRING_POINTER (DECL_SECTION_NAME (DECL)),
+                               ".sbss")))                                
+  {									
+    SYMBOL_REF_FLAG (XEXP (rtl, 0)) = 1;		
+  }									
+									
+  /* We can not perform GP optimizations on variables which are in	
+     specific sections, except for .sdata and .sbss which are		
+     handled above.  */						
+  else if (TARGET_GP_OPT && TREE_CODE (DECL) == VAR_DECL		
+           && DECL_SECTION_NAME (DECL) == NULL_TREE)			
+  {									
+    int size = int_size_in_bytes (TREE_TYPE (DECL));		
+    if (size > 0 && size <= microblaze_section_threshold)		
+      SYMBOL_REF_FLAG (XEXP (rtl, 0)) = 1;		
+  }									
+									
+  else if (HALF_PIC_P ())						
+  {									
+    HALF_PIC_ENCODE (DECL);						
+  }					
+				
+  /* XLNX[07.24.01] */						        
+  /*Added for setting the var_section in different cases */       	        
+  {  								
+    int size = int_size_in_bytes (TREE_TYPE (DECL));                    
+    int small_size = (size > 0 && size <= microblaze_section_threshold   
+                      && TARGET_XLGP_OPT) ? 1 : 0;      	        
+    int read_only = (TREE_READONLY (DECL)) ;                             
+                                                                        
+    if ((TREE_CODE (DECL)) == VAR_DECL) {                            
+      int init_val = (DECL_INITIAL (DECL) == NULL) ? 0 : 1;              
+      int init_val_override = init_val ? (!initializer_zerop (DECL_INITIAL (DECL))): 1;
+      int value = (small_size & init_val_override) | (read_only << 1) | (init_val << 2);       
+            
+      if (DECL_EXTERNAL (DECL))                                            
+        VAR_SECTION (XEXP (rtl,0)) = DATA_VAR;                 
+      else {
+        if (init_val)          
+          VAR_SECTION (XEXP (rtl,0)) = value;                  
+        else                                                             
+          VAR_SECTION (XEXP (rtl,0)) = (value % 2) + 1;        
+      }
+            
+      /* 
+         if (DECL_NAME (DECL))
+         fprintf (stderr, "microblaze_encode_section_info: %s. size: %d, init_val: %d, read_only: %d, VAR_SECTION: %d\n", 
+         IDENTIFIER_POINTER (DECL_NAME (DECL)), size, init_val, read_only, VAR_SECTION (XEXP (rtl, 0)));
+      */
+            
+    }                                                                   
+    else if ((TREE_CODE (DECL)) == STRING_CST){                         
+      if ((XEXP (rtl, 0)) != NULL) {                      
+        if (TREE_STRING_LENGTH (DECL) <= microblaze_section_threshold) {     
+          VAR_SECTION (XEXP (rtl,0)) = SDATA2_VAR;   
+        }     
+        else {                                                              
+          VAR_SECTION (XEXP (rtl,0)) = RODATA_VAR;    
+        }    
+      } else {
+        fprintf(stderr,"Some Problem with the string\n");
+      }          
+    }									
+  }									
+}
--- /dev/null
+++ b/gcc/config/microblaze/microblaze.h
@@ -0,0 +1,3688 @@
+/* Definitions of target machine for GNU compiler.  MicroBlaze version.
+   Copyright (C) 1989, 90-98, 1999 Free Software Foundation, Inc.
+   This file is part of GNU CC.
+
+   GNU CC is free software; you can redistribute it and/or modify
+   it under the terms of the GNU General Public License as published by
+   the Free Software Foundation; either version 2, or (at your option)
+   any later version.
+
+   GNU CC is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+   GNU General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with GNU CC; see the file COPYING.  If not, write to
+   the Free Software Foundation, 59 Temple Place - Suite 330,
+   Boston, MA 02111-1307, USA.  */
+
+/*
+ *
+ * Copyright (c) 2001 Xilinx, Inc.  All rights reserved. 
+ * 
+ * microblaze.h
+ * 
+ * MicroBlaze specific file. Contains functions for generating MicroBlaze code. 
+ * Certain lines of code are from Free Software Foundation
+ * 
+ * $Header: /devl/xcs/repo/env/Jobs/MDT/sw/ThirdParty/gnu/src/gcc/src-3.4/gcc/config/microblaze/microblaze.h,v 1.14.2.15 2006/05/22 15:25:28 vasanth Exp $
+ * 
+ */
+
+/* Standard GCC variables that we reference.  */
+
+/* MICROBLAZE external variables defined in microblaze.c.  */
+
+/* comparison type */
+enum cmp_type {
+    CMP_SI,				/* compare four byte integers */
+    CMP_DI,				/* compare eight byte integers */
+    CMP_SF,				/* compare single precision floats */
+    CMP_DF,				/* compare double precision floats */
+    CMP_MAX				/* max comparison type */
+};
+
+/* types of delay slot */
+enum delay_type {
+    DELAY_NONE,				/* no delay slot */
+    DELAY_LOAD,				/* load from memory delay */
+    DELAY_HILO,				/* move from/to hi/lo registers */
+    DELAY_FCMP				/* delay after doing c.<xx>.{d,s} */
+};
+
+/* Which pipeline to schedule for. */
+enum pipeline_type {
+  MICROBLAZE_PIPE_3 = 0,
+  MICROBLAZE_PIPE_5 = 1
+};
+
+
+
+
+/* Which ABI to use.  */
+
+#define ABI_32  0
+
+#define microblaze_abi ABI_32
+
+/* Whether to emit abicalls code sequences or not.  */
+
+enum microblaze_abicalls_type {
+    MICROBLAZE_ABICALLS_NO,
+    MICROBLAZE_ABICALLS_YES
+};
+
+/* Recast the abicalls class to be the abicalls attribute.  */
+#define microblaze_abicalls_attr ((enum attr_abicalls)microblaze_abicalls)
+
+/* Which type of block move to do (whether or not the last store is
+   split out so it can fill a branch delay slot).  */
+
+enum block_move_type {
+    BLOCK_MOVE_NORMAL,			/* generate complete block move */
+    BLOCK_MOVE_NOT_LAST,			/* generate all but last store */
+    BLOCK_MOVE_LAST			/* generate just the last store */
+};
+
+/* microblaze_cpu_select contains CPU info */
+struct microblaze_cpu_select
+{
+    const char *cpu;
+    const char *tune;
+    unsigned int flags;
+};
+
+#define MICROBLAZE_MASK_NO_UNSAFE_DELAY         0x00000001
+
+extern char microblaze_reg_names[][8];          /* register names (a0 vs. $4). */
+extern char microblaze_print_operand_punct[];	/* print_operand punctuation chars */
+extern const char *current_function_file;       /* filename current function is in */
+extern int num_source_filenames;                /* current .file # */
+extern int inside_function;                     /* != 0 if inside of a function */
+extern int ignore_line_number;                  /* != 0 if we are to ignore next .loc */
+extern int file_in_function_warning;            /* warning given about .file in func */
+extern int sdb_label_count;                     /* block start/end next label # */
+extern int sdb_begin_function_line;             /* Starting Line of current function */
+extern int microblaze_section_threshold;	/* # bytes of data/sdata cutoff */
+/* extern unsigned int g_switch_value;             /\* value of the -G xx switch *\/ */
+/* extern int g_switch_set;                        /\* whether -G xx was passed.  *\/ */
+extern int sym_lineno;                          /* sgi next label # for each stmt */
+extern int set_noreorder;                       /* # of nested .set noreorder's  */
+extern int set_nomacro;                         /* # of nested .set nomacro's  */
+extern int set_noat;                            /* # of nested .set noat's  */
+extern int set_volatile;                        /* # of nested .set volatile's  */
+extern int microblaze_dbx_regno[];		/* Map register # to debug register # */
+extern struct rtx_def *branch_cmp[2];           /* operands for compare */
+extern enum microblaze_abicalls_type microblaze_abicalls;/* for svr4 abi pic calls */
+extern int microblaze_isa;			/* architectural level */
+extern int dslots_load_total;                   /* total # load related delay slots */
+extern int dslots_load_filled;                  /* # filled load delay slots */
+extern int dslots_jump_total;                   /* total # jump related delay slots */
+extern int dslots_jump_filled;                  /* # filled jump delay slots */
+extern int dslots_number_nops;                  /* # of nops needed by previous insn */
+extern int num_refs[3];                         /* # 1/2/3 word references */
+extern struct rtx_def *microblaze_load_reg;	/* register to check for load delay */
+extern struct rtx_def *microblaze_load_reg2;	/* 2nd reg to check for load delay */
+extern struct rtx_def *microblaze_load_reg3;	/* 3rd reg to check for load delay */
+extern struct rtx_def *microblaze_load_reg4;	/* 4th reg to check for load delay */
+extern struct rtx_def *embedded_pic_fnaddr_rtx;	/* function address */
+extern const char       *asm_file_name;
+extern char             call_used_regs[];
+extern int              current_function_calls_alloca;
+extern char             *language_string;
+extern int              may_call_alloca;
+extern int              target_flags;
+extern struct microblaze_cpu_select 
+                        microblaze_select;
+extern int microblaze_no_unsafe_delay;
+extern enum pipeline_type microblaze_pipe;
+extern enum cmp_type branch_type;
+extern char *microblaze_no_clearbss;
+
+/*extern char    *version_string;*/
+
+/* Functions within microblaze.c that we reference.  Some of these return
+   type HOST_WIDE_INT, so define that here.  */
+
+#include "hwint.h"
+
+/* This is the only format we support */
+#define OBJECT_FORMAT_ELF
+
+
+/* Stubs for half-pic support if not OSF/1 reference platform.  */
+
+#ifndef HALF_PIC_P
+#define HALF_PIC_P() 0
+#define HALF_PIC_NUMBER_PTRS 0
+#define HALF_PIC_NUMBER_REFS 0
+#define HALF_PIC_ENCODE(DECL)
+#define HALF_PIC_DECLARE(NAME)
+#define HALF_PIC_INIT()	error ("half-pic init called on systems that don't support it.")
+#define HALF_PIC_ADDRESS_P(X) 0
+#define HALF_PIC_PTR(X) X
+#define HALF_PIC_FINISH(STREAM)
+#endif
+
+
+/* Run-time compilation parameters selecting different hardware subsets.  */
+
+/* Macros used in the machine description to test the flags.  */
+
+/* Bits for real switches */
+#define MASK_SOFT_DIV           0x00000002      /* Mask for Soft Divide                         */
+#define MASK_MEMCPY             0x00000008      /* Call memcpy instead of inline code           */
+#define MASK_GAS                0x00000010	/* Gas used instead of MICROBLAZE as            */
+#define MASK_EMPTY1             0x00000020	/* Empty 1 */
+#define MASK_STATS              0x00000040	/* print statistics to stderr                   */
+#define MASK_PATTERN_COMPARE    0x00000080      /* Use extended pattern compare instructions    */
+#define MASK_SOFT_FLOAT         0x00000100	/* software floating point                      */
+#define MASK_STACK_CHECK        0x00000200      /* Do Stack checking                            */ 
+#define MASK_ABICALLS           0x00000400	/* emit .abicalls/.cprestore/.cpload            */
+#define MASK_HALF_PIC           0x00000800	/* Emit OSF-style pic refs to externs           */
+#define MASK_LONG_CALLS         0x00001000	/* Always call through a register               */
+#define MASK_BARREL_SHIFT       0x00002000      /* Use a barrel shifter as this is being provided with MicroBlaze */
+#define MASK_EMBEDDED_PIC       0x00004000	/* Generate embedded PIC code                   */
+#define MASK_EMBEDDED_DATA      0x00008000	/* Reduce RAM usage, not fast code              */
+#define MASK_BIG_ENDIAN         0x00010000	/* Generate big endian code                     */
+#define MASK_XLGPOPT            0x00020000	/* Optimize for Xilinx global pointer           */ 
+#define MASK_SOFT_MUL           0x00040000      /* Use software multiply instead of hardware    */
+#define MASK_SMALL_DIVIDES      0x10000000      /* Use table lookup for divides.                */
+                                                /* Dummy switches used only in spec's           */
+/* Debug & special switches for internal testing. Not documented  */
+#define MASK_DEBUG              0                       /* Eliminate version # in .s file               */
+#define MASK_DEBUG_A            0x40000000              /* don't allow <label>($reg) addrs              */
+#define MASK_DEBUG_B            0x20000000              /* GO_IF_LEGITIMATE_ADDRESS debug               */
+#define MASK_DEBUG_D            0                       /* don't do define_split's                      */
+#define MASK_DEBUG_E            0                       /* function_arg debug                           */
+#define MASK_DEBUG_F            0
+#define MASK_DEBUG_G            0                       /* don't support 64 bit arithmetic              */
+#define MASK_DEBUG_H            0                       /* allow ints in FP registers                   */
+#define MASK_DEBUG_I            0                       /* unused                                       */
+
+
+#define TARGET_GAS		(target_flags & MASK_GAS)
+#define TARGET_UNIX_ASM		(!TARGET_GAS)
+#define TARGET_MICROBLAZE_AS		TARGET_UNIX_ASM
+/*#define TARGET_MICROBLAZE_ASM         (target_flags & MASK_LCC_ASM)  */
+#define TARGET_MICROBLAZE_ASM   0
+#define TARGET_STACK_CHECK      (target_flags & MASK_STACK_CHECK)
+
+/* Debug Mode */
+#define TARGET_DEBUG_MODE               (target_flags & MASK_DEBUG)
+#define TARGET_DEBUG_A_MODE             (target_flags & MASK_DEBUG_A)
+#define TARGET_DEBUG_B_MODE             (target_flags & MASK_DEBUG_B)
+#define TARGET_DEBUG_D_MODE             (target_flags & MASK_DEBUG_D)
+#define TARGET_DEBUG_E_MODE             (target_flags & MASK_DEBUG_E)
+#define TARGET_DEBUG_F_MODE             (target_flags & MASK_DEBUG_F)
+#define TARGET_DEBUG_G_MODE             (target_flags & MASK_DEBUG_G)
+#define TARGET_DEBUG_H_MODE             (target_flags & MASK_DEBUG_H)
+#define TARGET_DEBUG_I_MODE             (target_flags & MASK_DEBUG_I)
+
+#define TARGET_GP_OPT            0       /* Vasanth: Cleanup */
+
+/* call memcpy instead of inline code */
+#define TARGET_MEMCPY		(target_flags & MASK_MEMCPY)
+
+/* Optimize for Sdata/Sbss */
+#define TARGET_XLGP_OPT		(target_flags & MASK_XLGPOPT)
+/* print program statistics */
+#define TARGET_STATS		(target_flags & MASK_STATS)
+
+/* .abicalls, etc from Pyramid V.4 */
+#define TARGET_ABICALLS		(target_flags & MASK_ABICALLS)
+
+/* OSF pic references to externs */
+#define TARGET_HALF_PIC		(target_flags & MASK_HALF_PIC)
+
+/* Use software floating point routines */
+#define TARGET_SOFT_FLOAT	(target_flags & MASK_SOFT_FLOAT)
+
+/* Use hardware FPU instructions */
+#define TARGET_HARD_FLOAT       (!TARGET_SOFT_FLOAT)
+
+/* always call through a register */
+#define TARGET_LONG_CALLS	(target_flags & MASK_LONG_CALLS)
+
+/* generate embedded PIC code;
+   requires gas.  */
+#define TARGET_EMBEDDED_PIC	(target_flags & MASK_EMBEDDED_PIC)
+
+/* For embedded systems, optimize for reduced RAM space instead of for
+   fastest code.  */
+#define TARGET_EMBEDDED_DATA	(target_flags & MASK_EMBEDDED_DATA)
+
+/* Generate big endian code.  */
+#define TARGET_BIG_ENDIAN	(target_flags & MASK_BIG_ENDIAN)
+
+/* Use software multiply routines */
+#define TARGET_SOFT_MUL         (target_flags & MASK_SOFT_MUL)
+
+/* Use software divide routines */
+#define TARGET_SOFT_DIV         (target_flags & MASK_SOFT_DIV)
+
+/* Use hardware barrel shifter */
+#define TARGET_BARREL_SHIFT     (target_flags & MASK_BARREL_SHIFT)
+
+/* Use extended compare instructions */
+#define TARGET_PATTERN_COMPARE  (target_flags & MASK_PATTERN_COMPARE)
+
+#define TARGET_SMALL_DIVIDES    (target_flags & MASK_SMALL_DIVIDES)
+
+/* This is true if we must enable the assembly language file switching
+   code.  */
+/* [Changed to False for microblaze {04/17/02}]
+   #define TARGET_FILE_SWITCHING	(TARGET_GP_OPT && ! TARGET_GAS)*/
+#define TARGET_FILE_SWITCHING	0
+
+/* We must disable the function end stabs when doing the file switching trick,
+   because the Lscope stabs end up in the wrong place, making it impossible
+   to debug the resulting code.  */
+#define NO_DBX_FUNCTION_END TARGET_FILE_SWITCHING
+
+/* Added by Sid for mb-objdump problem */
+#define DBX_FUNCTION_FIRST 1
+#define DBX_BLOCKS_FUNCTION_RELATIVE 1
+
+/* Just to preserve old code */
+#define TARGET_SINGLE_FLOAT             0
+
+/* This table intercepts weirdo options whose names would interfere
+   with normal driver conventions, and either translates them into
+   standardly-named options, or adds a 'Z' so that they can get to
+   specs processing without interference.
+
+   Do not expand a linker option to "-Xlinker -<option>", since that
+   forfeits the ability to control via spec strings later.  However,
+   as a special exception, do this translation with -filelist, because
+   otherwise the driver will think there are no input files and quit.
+   (The alternative would be to hack the driver to recognize -filelist
+   specially, but it's simpler to use the translation table.)
+
+   Note that an option name with a prefix that matches another option
+   name, that also takes an argument, needs to be modified so the
+   prefix is different, otherwise a '*' after the shorter option will
+   match with the longer one.  */
+#define TARGET_OPTION_TRANSLATE_TABLE \
+  { "-xl-mode-executable", "-Zxl-mode-executable" }, \
+  { "-xl-mode-xmdstub", "-Zxl-mode-xmdstub" },  \
+  { "-xl-mode-bootstrap", "-Zxl-mode-bootstrap" }, \
+  { "-xl-mode-novectors", "-Zxl-mode-novectors" }, \
+  { "-xl-mode-xilkernel", "-Zxl-mode-xilkernel" },  \
+  { "-xl-blazeit", "-Zxl-blazeit" },    \
+  { "-xl-no-libxil", "-Zxl-no-libxil" }
+
+
+/* Macro to define tables used to set the flags.
+   This is a list in braces of pairs in braces,
+   each pair being { "NAME", VALUE }
+   where VALUE is the bits to set or minus the bits to clear.
+   An empty string NAME is used to identify the default VALUE.  */
+
+#define TARGET_SWITCHES							\
+{									\
+  {"xl-soft-mul",	  MASK_SOFT_MUL,				\
+     "Use the soft multiply emulation"},				\
+  {"no-xl-soft-mul",	  -MASK_SOFT_MUL,				\
+     "Use the hardware multiplier instead of emulation"},		\
+  {"xl-soft-div",	  MASK_SOFT_DIV,				\
+     "Use the soft divide emulation"},                                  \
+  {"no-xl-soft-div",	  -MASK_SOFT_DIV,				\
+     "Use the hardware divider instead of emulation"},	         	\
+  {"xl-barrel-shift",	  MASK_BARREL_SHIFT,				\
+     "Use the hardware barrel shifter instead of emulation"},           \
+  {"soft-float",	  MASK_SOFT_FLOAT,				\
+     "Use software floating point"},					\
+  {"hard-float",          -MASK_SOFT_FLOAT,                             \
+     "Don't use software floating point"},                              \
+  {"xl-pattern-compare",  MASK_PATTERN_COMPARE,                         \
+     "Use pattern compare instructions"},                               \
+  {"small-divides",	  MASK_SMALL_DIVIDES,				\
+     "Use table lookup optimization for small signed integer divisions"},\
+  {"xl-stack-check",	  MASK_STACK_CHECK,				\
+     "Check Stack at runtime"},						\
+  {"memcpy",		  MASK_MEMCPY,					\
+     "Don't optimize block moves"},					\
+  {"no-memcpy",		 -MASK_MEMCPY,					\
+     "Optimize block moves"},						\
+  {"xl-gp-opt",		 MASK_XLGPOPT,					\
+     "Use GP relative sdata/sbss sections[for xlnx]"},			\
+  {"no-xl-gp-opt",	 -MASK_XLGPOPT,			 	        \
+     "Use GP relative sdata/sbss sections[for xlnx]"},			\
+  {"stats",		  MASK_STATS,					\
+     "Output compiler statistics"},					\
+  {"no-stats",		 -MASK_STATS,					\
+     "Don't output compiler statistics"},				\
+  {"debug",		  MASK_DEBUG,					\
+     NULL},								\
+  {"debuga",		  MASK_DEBUG_A,					\
+     NULL},								\
+  {"debugb",		  MASK_DEBUG_B,					\
+     NULL},								\
+  {"debugd",		  MASK_DEBUG_D,					\
+     NULL},								\
+  {"debuge",		  MASK_DEBUG_E,					\
+     NULL},								\
+  {"debugf",		  MASK_DEBUG_F,					\
+     NULL},								\
+  {"debugg",		  MASK_DEBUG_G,					\
+     NULL},								\
+  {"debugh",		  MASK_DEBUG_H,					\
+     NULL},								\
+  {"debugi",		  MASK_DEBUG_I,					\
+     NULL},								\
+  {"",			  (TARGET_DEFAULT				\
+			   | TARGET_CPU_DEFAULT				\
+			   | TARGET_ENDIAN_DEFAULT),			\
+     NULL},								\
+}     
+
+/* Default target_flags if no switches are specified  */
+#define TARGET_DEFAULT      (0)
+
+#ifndef TARGET_CPU_DEFAULT
+#define TARGET_CPU_DEFAULT 0
+#endif
+
+#ifndef TARGET_ENDIAN_DEFAULT
+#define TARGET_ENDIAN_DEFAULT MASK_BIG_ENDIAN
+#endif
+
+#ifndef MULTILIB_DEFAULTS
+#if TARGET_ENDIAN_DEFAULT == 0
+#define MULTILIB_DEFAULTS { "EL", "microblaze" }
+#else
+#define MULTILIB_DEFAULTS { "EB", "microblaze" }
+#endif
+#endif
+
+/* What is the default setting for -mcpu= . We set it to v4.00.a even though 
+   we are actually ahead. This is safest version that has generate code compatible 
+   for the original ISA */
+#define MICROBLAZE_DEFAULT_CPU      "v4.00.a"               
+
+/* We must pass -EL to the linker by default for little endian embedded
+   targets using linker scripts with a OUTPUT_FORMAT line.  Otherwise, the
+   linker will default to using big-endian output files.  The OUTPUT_FORMAT
+   line must be in the linker script, otherwise -EB/-EL will not work.  */
+
+#ifndef LINKER_ENDIAN_SPEC
+#if TARGET_ENDIAN_DEFAULT == 0
+#define LINKER_ENDIAN_SPEC "%{!EB:%{!meb:-EL}}"
+#else
+#define LINKER_ENDIAN_SPEC ""
+#endif
+#endif
+
+/* This macro is similar to `TARGET_SWITCHES' but defines names of
+   command options that have values.  Its definition is an
+   initializer with a subgrouping for each command option.
+
+   Each subgrouping contains a string constant, that defines the
+   fixed part of the option name, and the address of a variable. 
+   The variable, type `char *', is set to the variable part of the
+   given option if the fixed part matches.  The actual option name
+   is made by appending `-m' to the specified name.
+
+   Here is an example which defines `-mshort-data-NUMBER'.  If the
+   given option is `-mshort-data-512', the variable `m88k_short_data'
+   will be set to the string `"512"'.
+
+   extern char *m88k_short_data;
+   #define TARGET_OPTIONS { { "short-data-", &m88k_short_data } }  */
+
+#define TARGET_OPTIONS                                                                  \
+{                                                                                       \
+  {"cpu=",  &microblaze_select.cpu,                                                     \
+     N_("Use features of and schedule code for given CPU"), NULL},                      \
+  {"tune=", &microblaze_select.tune,                                                    \
+     N_("Schedule code for given CPU"), NULL},                                          \
+  {"no-clearbss", &microblaze_no_clearbss,                                              \
+     N_("Do not clear the BSS to zero and do not place zero initialized in BSS"), "yes"}\
+}
+
+/* Macros to decide whether certain features are available or not,
+   depending on the instruction set architecture level.  */
+
+#define HAVE_SQRT_P()		0
+
+/* 
+   The gen* programs link code that refers to MASK_64BIT.  They don't
+   actually use the information in target_flags; they just refer to
+   it.  */
+
+/* Switch  Recognition by gcc.c.  Add -G xx support */
+
+#ifdef SWITCH_TAKES_ARG
+#undef SWITCH_TAKES_ARG
+#endif
+
+#define SWITCH_TAKES_ARG(CHAR)						\
+  (DEFAULT_SWITCH_TAKES_ARG (CHAR) || (CHAR) == 'G')
+
+/* Sometimes certain combinations of command options do not make sense
+   on a particular target machine.  You can define a macro
+   `OVERRIDE_OPTIONS' to take account of this.  This macro, if
+   defined, is executed once just after all the command options have
+   been parsed.
+
+   On the MICROBLAZE, it is used to handle -G.  We also use it to set up all
+   of the tables referenced in the other macros.  */
+
+#define OVERRIDE_OPTIONS override_options ()
+
+/* Zero or more C statements that may conditionally modify two
+   variables `fixed_regs' and `call_used_regs' (both of type `char
+   []') after they have been initialized from the two preceding
+   macros.
+
+   This is necessary in case the fixed or call-clobbered registers
+   depend on target flags.
+
+   You need not define this macro if it has no work to do.
+
+   If the usage of an entire class of registers depends on the target
+   flags, you may indicate this to GCC by using this macro to modify
+   `fixed_regs' and `call_used_regs' to 1 for each of the registers in
+   the classes which should not be used by GCC.  Also define the macro
+   `REG_CLASS_FROM_LETTER' to return `NO_REGS' if it is called with a
+   letter for a class that shouldn't be used.
+
+   (However, if this class is not included in `GENERAL_REGS' and all
+   of the insn patterns whose constraints permit this class are
+   controlled by target switches, then GCC will automatically avoid
+   using these registers when the target switches are opposed to
+   them.)  */
+
+#define CONDITIONAL_REGISTER_USAGE					\
+do									\
+  {									\
+	int regno;							\
+	for (regno = FP_REG_FIRST; regno <= FP_REG_LAST; regno++)	\
+	  fixed_regs[regno] = call_used_regs[regno] = 1;		\
+	for (regno = ST_REG_FIRST; regno <= ST_REG_LAST; regno++)	\
+	  fixed_regs[regno] = call_used_regs[regno] = 1;		\
+    SUBTARGET_CONDITIONAL_REGISTER_USAGE				\
+  }									\
+while (0)
+
+/* This is meant to be redefined in the host dependent files.  */
+#define SUBTARGET_CONDITIONAL_REGISTER_USAGE
+
+/* Show we can debug even without a frame pointer.  */
+#define CAN_DEBUG_WITHOUT_FP
+
+/* Complain about missing specs and predefines that should be defined in each
+   of the target tm files to override the defaults.  This is mostly a place-
+   holder until I can get each of the files updated [mm].  */
+
+#if defined(OSF_OS) \
+    || defined(MICROBLAZE_SYSV) \
+    || defined(MICROBLAZE_SVR4) \
+    || defined(MICROBLAZE_BSD43)
+
+#ifndef STARTFILE_SPEC
+#error "Define STARTFILE_SPEC in the appropriate tm.h file"
+#endif
+
+#ifndef MACHINE_TYPE
+#error "Define MACHINE_TYPE in the appropriate tm.h file"
+#endif
+#endif
+
+/* Tell collect what flags to pass to nm.  */
+#ifndef NM_FLAGS
+#define NM_FLAGS "-Bn"
+#endif
+
+/* Names to predefine in the preprocessor for this target machine.  */
+
+/* Target CPU builtins.  */
+#define TARGET_CPU_CPP_BUILTINS()				\
+  do								\
+    {								\
+        builtin_define ("microblaze");                          \
+        builtin_define ("_BIG_ENDIAN");                         \
+        builtin_define ("__MICROBLAZE__");                      \
+                                                                \
+        builtin_assert ("system=unix");                         \
+        builtin_assert ("system=bsd");                          \
+        builtin_assert ("cpu=microblaze");                      \
+        builtin_assert ("machine=microblaze");                  \
+} while (0)
+
+/* Assembler specs.  */
+
+/* MICROBLAZE_AS_ASM_SPEC is passed when using the MICROBLAZE assembler rather
+   than gas.  */
+
+#define MICROBLAZE_AS_ASM_SPEC "\
+%{!.s:-nocpp} %{.s: %{cpp} %{nocpp}} \
+%{pipe: %e-pipe is not supported.} \
+%{K} %(subtarget_microblaze_as_asm_spec)"
+
+/* SUBTARGET_MICROBLAZE_AS_ASM_SPEC is passed when using the MICROBLAZE assembler
+   rather than gas.  It may be overridden by subtargets.  */
+
+#ifndef SUBTARGET_MICROBLAZE_AS_ASM_SPEC
+#define SUBTARGET_MICROBLAZE_AS_ASM_SPEC "%{v}"
+#endif
+
+/* GAS_ASM_SPEC is passed when using gas, rather than the MICROBLAZE
+   assembler.  */
+
+#define GAS_ASM_SPEC "%{v}"
+
+/* TARGET_ASM_SPEC is used to select either MICROBLAZE_AS_ASM_SPEC or
+   GAS_ASM_SPEC as the default, depending upon the value of
+   TARGET_DEFAULT.  */
+
+#if ((TARGET_CPU_DEFAULT | TARGET_DEFAULT) & MASK_GAS) != 0
+/* GAS */
+
+#define TARGET_ASM_SPEC "\
+%{mmicroblaze-as: %(microblaze_as_asm_spec)} \
+%{!mmicroblaze-as: %(gas_asm_spec)}"
+
+#else /* not GAS */
+
+#define TARGET_ASM_SPEC ""
+/*#define TARGET_ASM_SPEC "\
+  %{!mgas: %(microblaze_as_asm_spec)} \
+  %{mgas: %(gas_asm_spec)}"
+*/
+#endif /* not GAS */
+
+/* SUBTARGET_ASM_OPTIMIZING_SPEC handles passing optimization options
+   to the assembler.  It may be overridden by subtargets.  */
+#ifndef SUBTARGET_ASM_OPTIMIZING_SPEC
+#define SUBTARGET_ASM_OPTIMIZING_SPEC " "
+#endif
+
+/* SUBTARGET_ASM_DEBUGGING_SPEC handles passing debugging options to
+   the assembler.  It may be overridden by subtargets.  */
+#ifndef SUBTARGET_ASM_DEBUGGING_SPEC
+#define SUBTARGET_ASM_DEBUGGING_SPEC "\
+%{g} %{g0} %{g1} %{g2} %{g3} \
+%{ggdb:-g} %{ggdb0:-g0} %{ggdb1:-g1} %{ggdb2:-g2} %{ggdb3:-g3} \
+%{gstabs:-g} %{gstabs0:-g0} %{gstabs1:-g1} %{gstabs2:-g2} %{gstabs3:-g3} \
+%{gstabs+:-g} %{gstabs+0:-g0} %{gstabs+1:-g1} %{gstabs+2:-g2} %{gstabs+3:-g3}"
+/* Old Specs */
+/*#define SUBTARGET_ASM_DEBUGGING_SPEC "\
+  %{g} %{g0} %{g1} %{g2} %{g3} \
+  %{ggdb:-g} %{ggdb0:-g0} %{ggdb1:-g1} %{ggdb2:-g2} %{ggdb3:-g3} \
+  %{gstabs:-g} %{gstabs0:-g0} %{gstabs1:-g1} %{gstabs2:-g2} %{gstabs3:-g3} \
+  %{gstabs+:-g} %{gstabs+0:-g0} %{gstabs+1:-g1} %{gstabs+2:-g2} %{gstabs+3:-g3} \
+  %{gcoff:-g} %{gcoff0:-g0} %{gcoff1:-g1} %{gcoff2:-g2} %{gcoff3:-g3}"
+*/
+#endif
+
+
+/* SUBTARGET_ASM_SPEC is always passed to the assembler.  It may be
+   overridden by subtargets.  */
+
+#ifndef SUBTARGET_ASM_SPEC
+#define SUBTARGET_ASM_SPEC ""
+#endif
+
+/* ASM_SPEC is the set of arguments to pass to the assembler.  */
+
+#define ASM_SPEC "\
+%{microblaze1} \
+%(target_asm_spec) \
+%(subtarget_asm_spec)"
+
+/* old asm spec */
+/*#define ASM_SPEC "\
+  %{G*} %{EB} %{EL} %{microblaze1} \
+  %{membedded-pic} \
+  %(target_asm_spec) \
+  %(subtarget_asm_spec)"
+*/
+
+/* Specify to run a post-processor, microblaze-tfile after the assembler
+   has run to stuff the microblaze debug information into the object file.
+   This is needed because the $#!%^ MICROBLAZE assembler provides no way
+   of specifying such information in the assembly file.  If we are
+   cross compiling, disable microblaze-tfile unless the user specifies
+   -mmicroblaze-tfile.  */
+
+#ifndef ASM_FINAL_SPEC
+#define ASM_FINAL_SPEC ""
+#endif	/* ASM_FINAL_SPEC */
+
+/* Extra switches sometimes passed to the linker.  */
+/* ??? The bestGnum will never be passed to the linker, because the gcc driver
+   will interpret it as a -b option.  */
+
+#ifndef LINK_SPEC
+/*#define LINK_SPEC "\
+  %{G*} %{EB} %{EL} \
+  %{bestGnum} %{shared} %{non_shared} \
+  %(linker_endian_spec) -relax -N \
+  %{intrusive-debug:-defsym _TEXT_START_ADDR=0x400}"
+*/
+#define LINK_SPEC "-relax -N %{Zxl-mode-xmdstub:-defsym _TEXT_START_ADDR=0x800}"
+#endif	/* LINK_SPEC defined */
+
+/* Specs for the compiler proper */
+
+/* SUBTARGET_CC1_SPEC is passed to the compiler proper.  It may be
+   overridden by subtargets.  */
+#ifndef SUBTARGET_CC1_SPEC
+#define SUBTARGET_CC1_SPEC ""
+#endif
+
+/* CC1_SPEC is the set of arguments to pass to the compiler proper.  */
+
+#ifndef CC1_SPEC
+#define CC1_SPEC "\
+%{G*} %{gline:%{!g:%{!g0:%{!g1:%{!g2: -g1}}}}} \
+%{save-temps: } \
+%(subtarget_cc1_spec)\
+%{Zxl-blazeit: %{!mxl-soft-mul: -mno-xl-soft-mul} %{!mxl-soft-div: -mno-xl-soft-div} -mxl-barrel-shift}\
+%{!mno-xl-soft-mul: %{!Zxl-blazeit: -mxl-soft-mul}}\
+%{!mno-xl-soft-div: %{!Zxl-blazeit: -mxl-soft-div}}\
+%{!mhard-float: -msoft-float}\
+"
+#endif
+
+/* Preprocessor specs.  */
+
+/* SUBTARGET_CPP_SIZE_SPEC defines SIZE_TYPE and PTRDIFF_TYPE.  It may
+   be overridden by subtargets.  */
+/* GCC 3.4.1 
+ * Removed
+ */
+
+#ifndef SUBTARGET_CPP_SIZE_SPEC
+#define SUBTARGET_CPP_SIZE_SPEC "-D__SIZE_TYPE__=unsigned\\ int -D__PTRDIFF_TYPE__=int"
+/*#define SUBTARGET_CPP_SIZE_SPEC ""*/
+#endif
+
+
+/* SUBTARGET_CPP_SPEC is passed to the preprocessor.  It may be
+   overridden by subtargets.  */
+#ifndef SUBTARGET_CPP_SPEC
+#define SUBTARGET_CPP_SPEC ""
+#endif
+
+/* CPP_SPEC is the set of arguments to pass to the preprocessor.  */
+
+#ifndef CPP_SPEC
+#define CPP_SPEC "\
+%{.S:	-D__LANGUAGE_ASSEMBLY -D_LANGUAGE_ASSEMBLY %{!ansi:-DLANGUAGE_ASSEMBLY}} \
+%{.s:	-D__LANGUAGE_ASSEMBLY -D_LANGUAGE_ASSEMBLY %{!ansi:-DLANGUAGE_ASSEMBLY}} \
+%{!.S: %{!.s: %{!.cc: %{!.cxx: %{!.C: %{!.m: -D__LANGUAGE_C -D_LANGUAGE_C %{!ansi:-DLANGUAGE_C}}}}}}} \
+%{mno-xl-soft-mul: -DHAVE_HW_MUL}       \
+%{mno-xl-soft-div: -DHAVE_HW_DIV}       \
+%{mxl-barrel-shift: -DHAVE_HW_BSHIFT}   \
+%{mxl-pattern-compare: -DHAVE_HW_PCMP}  \
+%{mhard-float: -DHAVE_HW_FPU}           \
+"
+#endif
+
+/* This macro defines names of additional specifications to put in the specs
+   that can be used in various specifications like CC1_SPEC.  Its definition
+   is an initializer with a subgrouping for each command option.
+
+   Each subgrouping contains a string constant, that defines the
+   specification name, and a string constant that used by the GNU CC driver
+   program.
+
+   Do not define this macro if it does not need to do anything.  */
+
+#define EXTRA_SPECS							\
+  { "subtarget_cc1_spec", SUBTARGET_CC1_SPEC },				\
+  { "subtarget_cpp_spec", SUBTARGET_CPP_SPEC },				\
+  { "subtarget_cpp_size_spec", SUBTARGET_CPP_SIZE_SPEC },		\
+  { "microblaze_as_asm_spec", MICROBLAZE_AS_ASM_SPEC },				\
+  { "gas_asm_spec", GAS_ASM_SPEC },					\
+  { "target_asm_spec", TARGET_ASM_SPEC },				\
+  { "subtarget_microblaze_as_asm_spec", SUBTARGET_MICROBLAZE_AS_ASM_SPEC }, 	\
+  { "subtarget_asm_optimizing_spec", SUBTARGET_ASM_OPTIMIZING_SPEC },	\
+  { "subtarget_asm_debugging_spec", SUBTARGET_ASM_DEBUGGING_SPEC },	\
+  { "subtarget_asm_spec", SUBTARGET_ASM_SPEC },				\
+  { "linker_endian_spec", LINKER_ENDIAN_SPEC },				\
+  SUBTARGET_EXTRA_SPECS
+
+/* If defined, this macro is an additional prefix to try after
+   `STANDARD_EXEC_PREFIX'.  */
+
+#ifndef MD_EXEC_PREFIX
+#define MD_EXEC_PREFIX "/usr/lib/cmplrs/cc/"
+#endif
+
+#ifndef MD_STARTFILE_PREFIX
+/*#define MD_STARTFILE_PREFIX "/usr/lib/cmplrs/cc/"*/
+#define MD_STARTFILE_PREFIX "/home/sid/comp/tests/gcctest/"
+#endif
+
+
+/* Print subsidiary information on the compiler version in use.  */
+#define MICROBLAZE_VERSION MICROBLAZE_DEFAULT_CPU
+
+#ifndef MACHINE_TYPE
+#define MACHINE_TYPE "MicroBlaze/ELF"
+#endif
+
+#ifndef TARGET_VERSION_INTERNAL
+#define TARGET_VERSION_INTERNAL(STREAM)					\
+  fprintf (STREAM, " %s %s", MACHINE_TYPE, MICROBLAZE_VERSION)
+#endif
+
+#ifndef TARGET_VERSION
+#define TARGET_VERSION TARGET_VERSION_INTERNAL (stderr)
+#endif
+
+
+/* If we are passing smuggling stabs through the MICROBLAZE ECOFF object
+   format, put a comment in front of the .stab<x> operation so
+   that the MICROBLAZE assembler does not choke.  The microblaze-tfile program
+   will correctly put the stab into the object file.  */
+
+/* #define ASM_STABS_OP	((TARGET_GAS) ? ".stabs" : " #.stabs") */
+/* #define ASM_STABN_OP	((TARGET_GAS) ? ".stabn" : " #.stabn") */
+/* #define ASM_STABD_OP	((TARGET_GAS) ? ".stabd" : " #.stabd") */
+
+#define ASM_STABS_OP	".stabs "  
+#define ASM_STABN_OP	".stabn " 
+#define ASM_STABD_OP	".stabd " 
+
+/* Local compiler-generated symbols must have a prefix that the assembler
+   understands.   By default, this is $, although some targets (e.g.,
+   NetBSD-ELF) need to override this. */
+
+#ifndef LOCAL_LABEL_PREFIX
+#define LOCAL_LABEL_PREFIX	"$"
+#endif
+
+/* By default on the microblaze, external symbols do not have an underscore
+   prepended, but some targets (e.g., NetBSD) require this. */
+
+#ifndef USER_LABEL_PREFIX
+#define USER_LABEL_PREFIX	""
+#endif
+
+/* fixed registers */
+#define MB_ABI_BASE_REGNUM                   0
+#define MB_ABI_STACK_POINTER_REGNUM          1
+#define MB_ABI_GPRO_REGNUM                   2
+#define MB_ABI_GPRW_REGNUM                  13
+#define MB_ABI_INTR_RETURN_ADDR_REGNUM      14
+#define MB_ABI_SUB_RETURN_ADDR_REGNUM       15
+#define MB_ABI_DEBUG_RETURN_ADDR_REGNUM     16
+#define MB_ABI_EXCEPTION_RETURN_ADDR_REGNUM 17
+#define MB_ABI_ASM_TEMP_REGNUM              18          /* This is our workhorse temporary register. We use it heavily for compares and shifts */
+#define MB_ABI_FRAME_POINTER_REGNUM         19
+#define MB_ABI_PIC_ADDR_REGNUM              20
+#define MB_ABI_PIC_FUNC_REGNUM              21
+/* volatile registers */
+#define MB_ABI_INT_RETURN_VAL_REGNUM         3
+#define MB_ABI_INT_RETURN_VAL2_REGNUM        4
+#define MB_ABI_FIRST_ARG_REGNUM              5
+#define MB_ABI_LAST_ARG_REGNUM              10
+#define MB_ABI_MAX_ARG_REGS                 (MB_ABI_LAST_ARG_REGNUM - MB_ABI_FIRST_ARG_REGNUM + 1)
+#define MB_ABI_STATIC_CHAIN_REGNUM           3
+#define MB_ABI_TEMP1_REGNUM                 11
+#define MB_ABI_TEMP2_REGNUM                 12
+#define MB_ABI_MSR_SAVE_REG                 11          /* Volatile register used to save MSR in interrupt handlers */
+
+
+/* Debug stuff */
+
+/* #define SDB_DEBUGGING_INFO		/\* generate info for microblaze-tfile *\/ */
+/* /\* Forward references to tags are allowed.  *\/ */
+/* #define SDB_ALLOW_FORWARD_REFERENCES */
+
+/* /\* Unknown tags are also allowed.  *\/ */
+/* #define SDB_ALLOW_UNKNOWN_REFERENCES */
+
+#define DBX_DEBUGGING_INFO		/* generate stabs (OSF/rose) */
+
+/* On Sun 4, this limit is 2048.  We use 1500 to be safe,
+   since the length can run past this up to a continuation point.  */
+#define DBX_CONTIN_LENGTH 1500
+
+/* How to renumber registers for dbx and gdb. */
+#define DBX_REGISTER_NUMBER(REGNO) microblaze_dbx_regno[ (REGNO) ]
+
+#define DWARF2_UNWIND_INFO 0
+/* The mapping from gcc register number to DWARF 2 CFA column number.
+ */
+#define DWARF_FRAME_REGNUM(REG)				\
+  (REG == GP_REG_FIRST + MB_ABI_SUB_RETURN_ADDR_REGNUM ? DWARF_FRAME_RETURN_COLUMN : REG)
+
+/* The DWARF 2 CFA column which tracks the return address.  */
+#define DWARF_FRAME_RETURN_COLUMN (FP_REG_LAST + 1)
+#define INCOMING_RETURN_ADDR_RTX  gen_rtx (REG, VOIDmode, GP_REG_FIRST + MB_ABI_SUB_RETURN_ADDR_REGNUM)
+
+
+/* /\* Overrides for the COFF debug format.  *\/ */
+/* #define PUT_SDB_SCL(a)					\ */
+/* do {							\ */
+/*   extern FILE *asm_out_text_file;			\ */
+/*   fprintf (asm_out_text_file, "\t.scl\t%d;", (a));	\ */
+/* } while (0) */
+
+/* #define PUT_SDB_INT_VAL(a)				\ */
+/* do {							\ */
+/*   extern FILE *asm_out_text_file;			\ */
+/*   fprintf (asm_out_text_file, "\t.val\t%d;", (a));	\ */
+/* } while (0) */
+
+/* #define PUT_SDB_VAL(a)					\ */
+/* do {							\ */
+/*   extern FILE *asm_out_text_file;			\ */
+/*   fputs ("\t.val\t", asm_out_text_file);		\ */
+/*   output_addr_const (asm_out_text_file, (a));		\ */
+/*   fputc (';', asm_out_text_file);			\ */
+/* } while (0) */
+
+/* #define PUT_SDB_DEF(a)					\ */
+/* do {							\ */
+/*   extern FILE *asm_out_text_file;			\ */
+/*   fprintf (asm_out_text_file, "\t%s.def\t",		\ */
+/* 	   (TARGET_GAS) ? "" : "#");			\ */
+/*   ASM_OUTPUT_LABELREF (asm_out_text_file, a); 		\ */
+/*   fputc (';', asm_out_text_file);			\ */
+/* } while (0) */
+
+/* #define PUT_SDB_PLAIN_DEF(a)				\ */
+/* do {							\ */
+/*   extern FILE *asm_out_text_file;			\ */
+/*   fprintf (asm_out_text_file, "\t%s.def\t.%s;",		\ */
+/* 	   (TARGET_GAS) ? "" : "#", (a));		\ */
+/* } while (0) */
+
+/* #define PUT_SDB_ENDEF					\ */
+/* do {							\ */
+/*   extern FILE *asm_out_text_file;			\ */
+/*   fprintf (asm_out_text_file, "\t.endef\n");		\ */
+/* } while (0) */
+
+/* #define PUT_SDB_TYPE(a)					\ */
+/* do {							\ */
+/*   extern FILE *asm_out_text_file;			\ */
+/*   fprintf (asm_out_text_file, "\t.type\t0x%x;", (a));	\ */
+/* } while (0) */
+
+/* #define PUT_SDB_SIZE(a)					\ */
+/* do {							\ */
+/*   extern FILE *asm_out_text_file;			\ */
+/*   fprintf (asm_out_text_file, "\t.size\t%d;", (a));	\ */
+/* } while (0) */
+
+/* #define PUT_SDB_DIM(a)					\ */
+/* do {							\ */
+/*   extern FILE *asm_out_text_file;			\ */
+/*   fprintf (asm_out_text_file, "\t.dim\t%d;", (a));	\ */
+/* } while (0) */
+
+/* #ifndef PUT_SDB_START_DIM */
+/* #define PUT_SDB_START_DIM				\ */
+/* do {							\ */
+/*   extern FILE *asm_out_text_file;			\ */
+/*   fprintf (asm_out_text_file, "\t.dim\t");		\ */
+/* } while (0) */
+/* #endif */
+
+/* #ifndef PUT_SDB_NEXT_DIM */
+/* #define PUT_SDB_NEXT_DIM(a)				\ */
+/* do {							\ */
+/*   extern FILE *asm_out_text_file;			\ */
+/*   fprintf (asm_out_text_file, "%d,", a);		\ */
+/* } while (0) */
+/* #endif */
+
+/* #ifndef PUT_SDB_LAST_DIM */
+/* #define PUT_SDB_LAST_DIM(a)				\ */
+/* do {							\ */
+/*   extern FILE *asm_out_text_file;			\ */
+/*   fprintf (asm_out_text_file, "%d;", a);		\ */
+/* } while (0) */
+/* #endif */
+
+/* #define PUT_SDB_TAG(a)					\ */
+/* do {							\ */
+/*   extern FILE *asm_out_text_file;			\ */
+/*   fprintf (asm_out_text_file, "\t.tag\t");		\ */
+/*   ASM_OUTPUT_LABELREF (asm_out_text_file, a); 		\ */
+/*   fputc (';', asm_out_text_file);			\ */
+/* } while (0) */
+
+/* /\* For block start and end, we create labels, so that */
+/*    later we can figure out where the correct offset is. */
+/*    The normal .ent/.end serve well enough for functions, */
+/*    so those are just commented out.  *\/ */
+
+/* #define PUT_SDB_BLOCK_START(LINE)			\ */
+/* do {							\ */
+/*   extern FILE *asm_out_text_file;			\ */
+/*   fprintf (asm_out_text_file,				\ */
+/* 	   "%sLb%d:\n\t%s.begin\t%sLb%d\t%d\n",		\ */
+/* 	   LOCAL_LABEL_PREFIX,				\ */
+/* 	   sdb_label_count,				\ */
+/* 	   (TARGET_GAS) ? "" : "#",			\ */
+/* 	   LOCAL_LABEL_PREFIX,				\ */
+/* 	   sdb_label_count,				\ */
+/* 	   (LINE));					\ */
+/*   sdb_label_count++;					\ */
+/* } while (0) */
+
+/* #define PUT_SDB_BLOCK_END(LINE)				\ */
+/* do {							\ */
+/*   extern FILE *asm_out_text_file;			\ */
+/*   fprintf (asm_out_text_file,				\ */
+/* 	   "%sLe%d:\n\t%s.bend\t%sLe%d\t%d\n",		\ */
+/* 	   LOCAL_LABEL_PREFIX,				\ */
+/* 	   sdb_label_count,				\ */
+/* 	   (TARGET_GAS) ? "" : "#",			\ */
+/* 	   LOCAL_LABEL_PREFIX,				\ */
+/* 	   sdb_label_count,				\ */
+/* 	   (LINE));					\ */
+/*   sdb_label_count++;					\ */
+/* } while (0) */
+
+/* #define PUT_SDB_FUNCTION_START(LINE)\ */
+/* do {                                                  \ */
+/*   extern FILE *asm_out_text_file;             \ */
+/*   ASM_OUTPUT_SOURCE_LINE (asm_out_text_file, LINE + sdb_begin_function_line, 0); \ */
+/* } while (0) */
+
+/* #define PUT_SDB_FUNCTION_END(LINE)            \ */
+/* do {                                                  \ */
+/*   extern FILE *asm_out_text_file;             \ */
+/*   ASM_OUTPUT_SOURCE_LINE (asm_out_text_file, LINE + sdb_begin_function_line, 0); \ */
+/* } while (0) */
+
+/* #define PUT_SDB_EPILOGUE_END(NAME) */
+
+/* #define PUT_SDB_SRC_FILE(FILENAME) \ */
+/* do {							\ */
+/*   extern FILE *asm_out_text_file;			\ */
+/*   output_file_directive (asm_out_text_file, (FILENAME)); \ */
+/* } while (0) */
+
+/* #define SDB_GENERATE_FAKE(BUFFER, NUMBER) \ */
+/*   sprintf ((BUFFER), ".%dfake", (NUMBER)); */
+
+/* Correct the offset of automatic variables and arguments.  Note that
+   the MICROBLAZE debug format wants all automatic variables and arguments
+   to be in terms of the virtual frame pointer (stack pointer before
+   any adjustment in the function), while the MICROBLAZE linker wants
+   the frame pointer to be the stack pointer after the initial
+   adjustment.  */
+
+#define DEBUGGER_AUTO_OFFSET(X)  \
+  microblaze_debugger_offset (X, (HOST_WIDE_INT) 0)
+#define DEBUGGER_ARG_OFFSET(OFFSET, X)  \
+  microblaze_debugger_offset (X, (HOST_WIDE_INT) OFFSET)
+
+/* /\* Tell collect that the object format is ECOFF *\/ */
+/* #define OBJECT_FORMAT_COFF	/\* Object file looks like COFF *\/ */
+/* #define EXTENDED_COFF		/\* ECOFF, not normal coff *\/ */
+
+
+/* Target machine storage layout */
+
+/* Define in order to support both big and little endian float formats
+   in the same gcc binary.  */
+/*#define REAL_ARITHMETIC*/
+
+/* Define this if most significant bit is lowest numbered
+   in instructions that operate on numbered bit-fields.
+*/
+#define BITS_BIG_ENDIAN 0
+
+/* Define this if most significant byte of a word is the lowest numbered. */
+#define BYTES_BIG_ENDIAN (TARGET_BIG_ENDIAN != 0)
+
+/* Define this if most significant word of a multiword number is the lowest. */
+#define WORDS_BIG_ENDIAN (TARGET_BIG_ENDIAN != 0)
+
+/* Define this to set the endianness to use in libgcc2.c, which can
+   not depend on target_flags.  */
+#define LIBGCC2_WORDS_BIG_ENDIAN 1
+
+/* Number of bits in an addressable storage unit */
+#define BITS_PER_UNIT           8
+
+/* Width in bits of a "word", which is the contents of a machine register.
+   Note that this is not necessarily the width of data type `int';
+   if using 16-bit ints on a 68000, this would still be 32.
+   But on a machine with 16-bit registers, this would be 16.  */
+#define BITS_PER_WORD           32
+
+/* Width of a word, in units (bytes).  */
+#define UNITS_PER_WORD          4
+#define MIN_UNITS_PER_WORD      4
+
+/* For MICROBLAZE, width of a floating point register.  */
+#define UNITS_PER_FPREG         4
+
+/* A C expression for the size in bits of the type `int' on the
+   target machine.  If you don't define this, the default is one
+   word.  */
+#define INT_TYPE_SIZE           32
+/* GCC 3.4.1 
+ * Removed the MAX_INT_TYPE_SIZE 
+ */
+/* #define MAX_INT_TYPE_SIZE 64 */
+
+/* Tell the preprocessor the maximum size of wchar_t.  */
+/* GCC 3.4.1 
+ * Removed the MAX_WCHAR_TYPE_SIZE 
+ */
+/*
+  #ifndef MAX_WCHAR_TYPE_SIZE
+  #ifndef WCHAR_TYPE_SIZE
+  #define MAX_WCHAR_TYPE_SIZE MAX_INT_TYPE_SIZE
+  #endif
+  #endif
+*/
+
+/* A C expression for the size in bits of the type `short' on the
+   target machine.  If you don't define this, the default is half a
+   word.  (If this would be less than one storage unit, it is
+   rounded up to one unit.)  */
+#define SHORT_TYPE_SIZE         16
+
+/* A C expression for the size in bits of the type `long' on the
+   target machine.  If you don't define this, the default is one
+   word.  */
+#define LONG_TYPE_SIZE          32
+#define MAX_LONG_TYPE_SIZE      64
+
+/* A C expression for the size in bits of the type `long long' on the
+   target machine.  If you don't define this, the default is two
+   words.  */
+#define LONG_LONG_TYPE_SIZE     64
+
+/* A C expression for the size in bits of the type `char' on the
+   target machine.  If you don't define this, the default is one
+   quarter of a word.  (If this would be less than one storage unit,
+   it is rounded up to one unit.)  */
+#define CHAR_TYPE_SIZE BITS_PER_UNIT
+
+/* A C expression for the size in bits of the type `float' on the
+   target machine.  If you don't define this, the default is one
+   word.  */
+#define FLOAT_TYPE_SIZE         32
+
+/* A C expression for the size in bits of the type `double' on the
+   target machine.  If you don't define this, the default is two
+   words.  */
+#define DOUBLE_TYPE_SIZE        64
+
+/* A C expression for the size in bits of the type `long double' on
+   the target machine.  If you don't define this, the default is two
+   words.  */
+#define LONG_DOUBLE_TYPE_SIZE   64
+
+/* Width in bits of a pointer.
+   See also the macro `Pmode' defined below.  */
+#ifndef POINTER_SIZE
+#define POINTER_SIZE            32
+#endif
+
+/* Allocation boundary (in *bits*) for storing arguments in argument list.  */
+#define PARM_BOUNDARY           32
+
+/* Allocation boundary (in *bits*) for the code of a function.  */
+#define FUNCTION_BOUNDARY       32
+
+/* Alignment of field after `int : 0' in a structure.  */
+#define EMPTY_FIELD_BOUNDARY    32
+
+/* Every structure's size must be a multiple of this.  */
+/* 8 is observed right on a DECstation and on riscos 4.02.  */
+#define STRUCTURE_SIZE_BOUNDARY 8
+
+/* There is no point aligning anything to a rounder boundary than this.  */
+#define BIGGEST_ALIGNMENT       32
+
+/* Set this nonzero if move instructions will actually fail to work
+   when given unaligned data.  */
+#define STRICT_ALIGNMENT        1
+
+/* Define this if you wish to imitate the way many other C compilers
+   handle alignment of bitfields and the structures that contain
+   them.
+
+   The behavior is that the type written for a bitfield (`int',
+   `short', or other integer type) imposes an alignment for the
+   entire structure, as if the structure really did contain an
+   ordinary field of that type.  In addition, the bitfield is placed
+   within the structure so that it would fit within such a field,
+   not crossing a boundary for it.
+
+   Thus, on most machines, a bitfield whose type is written as `int'
+   would not cross a four-byte boundary, and would force four-byte
+   alignment for the whole structure.  (The alignment used may not
+   be four bytes; it is controlled by the other alignment
+   parameters.)
+
+   If the macro is defined, its definition should be a C expression;
+   a nonzero value for the expression enables this behavior.  */
+
+#define PCC_BITFIELD_TYPE_MATTERS 1
+
+/* If defined, a C expression to compute the alignment given to a
+   constant that is being placed in memory.  CONSTANT is the constant
+   and ALIGN is the alignment that the object would ordinarily have.
+   The value of this macro is used instead of that alignment to align
+   the object.
+
+   If this macro is not defined, then ALIGN is used.
+
+   The typical use of this macro is to increase alignment for string
+   constants to be word aligned so that `strcpy' calls that copy
+   constants can be done inline.  */
+
+#define CONSTANT_ALIGNMENT(EXP, ALIGN)					\
+  ((TREE_CODE (EXP) == STRING_CST  || TREE_CODE (EXP) == CONSTRUCTOR)	\
+   && (ALIGN) < BITS_PER_WORD						\
+	? BITS_PER_WORD							\
+	: (ALIGN))
+
+/* If defined, a C expression to compute the alignment for a static
+   variable.  TYPE is the data type, and ALIGN is the alignment that
+   the object would ordinarily have.  The value of this macro is used
+   instead of that alignment to align the object.
+
+   If this macro is not defined, then ALIGN is used.
+
+   One use of this macro is to increase alignment of medium-size
+   data to make it all fit in fewer cache lines.  Another is to
+   cause character arrays to be word-aligned so that `strcpy' calls
+   that copy constants to character arrays can be done inline.  */
+
+#undef DATA_ALIGNMENT
+#define DATA_ALIGNMENT(TYPE, ALIGN)					\
+  ((((ALIGN) < BITS_PER_WORD)						\
+    && (TREE_CODE (TYPE) == ARRAY_TYPE					\
+	|| TREE_CODE (TYPE) == UNION_TYPE				\
+	|| TREE_CODE (TYPE) == RECORD_TYPE)) ? BITS_PER_WORD : (ALIGN))
+
+
+/* Define this macro if an argument declared as `char' or `short' in a
+   prototype should actually be passed as an `int'.  In addition to
+   avoiding errors in certain cases of mismatch, it also makes for
+   better code on certain machines. */
+
+#define PROMOTE_PROTOTYPES 1
+
+/* Define if operations between registers always perform the operation
+   on the full register even if a narrower mode is specified.  */
+#define WORD_REGISTER_OPERATIONS
+
+/* Define if loading in MODE, an integral mode narrower than BITS_PER_WORD
+   will either zero-extend or sign-extend.  The value of this macro should
+   be the code that says which one of the two operations is implicitly
+   done, NIL if none.  */
+#define LOAD_EXTEND_OP(MODE)  ZERO_EXTEND
+
+/* Define this macro if it is advisable to hold scalars in registers
+   in a wider mode than that declared by the program.  In such cases, 
+   the value is constrained to be within the bounds of the declared
+   type, but kept valid in the wider mode.  The signedness of the
+   extension may differ from that of the type.
+
+   We promote any value smaller than SImode up to SImode.  We don't
+   want to promote to DImode when in 64 bit mode, because that would
+   prevent us from using the faster SImode multiply and divide
+   instructions.  */
+
+#define PROMOTE_MODE(MODE, UNSIGNEDP, TYPE)	\
+  if (GET_MODE_CLASS (MODE) == MODE_INT		\
+      && GET_MODE_SIZE (MODE) < 4)		\
+    (MODE) = SImode;
+
+/* Define this if function arguments should also be promoted using the above
+   procedure.  */
+
+#define PROMOTE_FUNCTION_ARGS
+
+/* Likewise, if the function return value is promoted.  */
+
+#define PROMOTE_FUNCTION_RETURN
+
+/* Standard register usage.  */
+
+/* Number of actual hardware registers.
+   The hardware registers are assigned numbers for the compiler
+   from 0 to just below FIRST_PSEUDO_REGISTER.
+   All registers that the compiler knows about must be given numbers,
+   even those that are not normally considered general registers.
+
+   On the MicroBlaze, we have 32 integer registers */
+
+#define FIRST_PSEUDO_REGISTER 78
+
+/* 1 for registers that have pervasive standard uses
+   and are not available for the register allocator.
+   does not include arg passing regs 
+*/
+
+#define FIXED_REGISTERS							\
+{									\
+  1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,			\
+  1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,			\
+  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,			\
+  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,			\
+  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 				\
+}
+
+
+/* 1 for registers not available across function calls.
+   i.e. all non-volatiles are 0
+   These must include the FIXED_REGISTERS and also any
+   registers that can be used without being saved.
+   The latter must include the registers where values are returned
+   and the register where structure-value addresses are passed.
+   Aside from that, you can include as many other registers as you like.  */
+
+#define CALL_USED_REGISTERS						\
+{									\
+  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,			\
+  1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,			\
+  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,			\
+  1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,			\
+  1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 				\
+}
+
+/* Internal macros to classify a register number as to whether it's a
+   general purpose register, a floating point register, a
+   multiply/divide register, or a status register.  */
+
+#define GP_REG_FIRST    0
+#define GP_REG_LAST     31
+#define GP_REG_NUM      (GP_REG_LAST - GP_REG_FIRST + 1)
+#define GP_DBX_FIRST    0
+
+#define FP_REG_FIRST    32
+#define FP_REG_LAST     63 
+#define FP_REG_NUM      (FP_REG_LAST - FP_REG_FIRST + 1)
+#define FP_DBX_FIRST    ((write_symbols == DBX_DEBUG) ? 38 : 32)
+
+#define MD_REG_FIRST    64
+#define MD_REG_LAST     66
+#define MD_REG_NUM      (MD_REG_LAST - MD_REG_FIRST + 1)
+
+#define ST_REG_FIRST    67
+#define ST_REG_LAST     74
+#define ST_REG_NUM      (ST_REG_LAST - ST_REG_FIRST + 1)
+
+#define AP_REG_NUM      75
+#define RAP_REG_NUM     76
+#define FRP_REG_NUM     77
+
+#define GP_REG_P(REGNO) ((unsigned) ((REGNO) - GP_REG_FIRST) < GP_REG_NUM)
+#define FP_REG_P(REGNO) ((unsigned) ((REGNO) - FP_REG_FIRST) < FP_REG_NUM)
+#define MD_REG_P(REGNO) ((unsigned) ((REGNO) - MD_REG_FIRST) < MD_REG_NUM)
+#define ST_REG_P(REGNO) ((unsigned) ((REGNO) - ST_REG_FIRST) < ST_REG_NUM)
+
+/* Return number of consecutive hard regs needed starting at reg REGNO
+   to hold something of mode MODE.
+   This is ordinarily the length in words of a value of mode MODE
+   but can be less for certain modes in special long registers.
+
+*/
+
+#define HARD_REGNO_NREGS(REGNO, MODE)					\
+  (! FP_REG_P (REGNO)							\
+	? ((GET_MODE_SIZE (MODE) + UNITS_PER_WORD - 1) / UNITS_PER_WORD) \
+	: ((GET_MODE_SIZE (MODE) + UNITS_PER_FPREG - 1) / UNITS_PER_FPREG))
+
+/* Value is 1 if hard register REGNO can hold a value of machine-mode
+   MODE.  In 32 bit mode, require that DImode and DFmode be in even
+   registers.  For DImode, this makes some of the insns easier to
+   write, since you don't have to worry about a DImode value in
+   registers 3 & 4, producing a result in 4 & 5.
+   FIXME: Can we avoid restricting odd-numbered register bases for DImode and DFmode ?
+
+   To make the code simpler HARD_REGNO_MODE_OK now just references an
+   array built in override_options.  Because machmodes.h is not yet
+   included before this file is processed, the MODE bound can't be
+   expressed here.  */
+extern char microblaze_hard_regno_mode_ok[][FIRST_PSEUDO_REGISTER];
+#define HARD_REGNO_MODE_OK(REGNO, MODE)					\
+            microblaze_hard_regno_mode_ok[ (int)(MODE) ][ (REGNO)]
+
+
+/* Value is 1 if it is a good idea to tie two pseudo registers
+   when one has mode MODE1 and one has mode MODE2.
+   If HARD_REGNO_MODE_OK could produce different values for MODE1 and MODE2,
+   for any hard reg, then this must be 0 for correct output.  */
+#define MODES_TIEABLE_P(MODE1, MODE2)					\
+  ((GET_MODE_CLASS (MODE1) == MODE_FLOAT ||				\
+    GET_MODE_CLASS (MODE1) == MODE_COMPLEX_FLOAT)			\
+   == (GET_MODE_CLASS (MODE2) == MODE_FLOAT ||				\
+       GET_MODE_CLASS (MODE2) == MODE_COMPLEX_FLOAT))
+
+/* MICROBLAZE pc is not overloaded on a register.	*/
+/* #define PC_REGNUM xx				*/
+
+/* Register to use for pushing function arguments.  */
+#define STACK_POINTER_REGNUM    \
+        (GP_REG_FIRST + MB_ABI_STACK_POINTER_REGNUM)
+
+/* Offset from the stack pointer to the first location for outgoing arguments. */
+#define STACK_POINTER_OFFSET    \
+        FIRST_PARM_OFFSET(FNDECL) 
+
+/* Base register for access to local variables of the function.  We
+   pretend that the frame pointer is
+   MB_ABI_INTR_RETURN_ADDR_REGNUM, and then eliminate it
+   to HARD_FRAME_POINTER_REGNUM.  We can get away with this because
+   rMB_ABI_INTR_RETUREN_ADDR_REGNUM is a fixed
+   register(return address for interrupt), and will not be used for
+   anything else.  
+   
+   [12/14/01] We need to check this out. This might be no longer true,
+   now that we have interrupt controllers working. */
+#define FRAME_POINTER_REGNUM            \
+        (FRP_REG_NUM)
+
+#define HARD_FRAME_POINTER_REGNUM       \
+        (GP_REG_FIRST + MB_ABI_FRAME_POINTER_REGNUM)
+
+/* Value should be nonzero if functions must have frame pointers.
+   Zero means the frame pointer need not be set up (and parms
+   may be accessed via the stack pointer) in functions that seem suitable.
+   This is computed in `reload', in reload1.c.  */
+#define FRAME_POINTER_REQUIRED          \
+        (current_function_calls_alloca)
+
+/* Base register for access to arguments of the function.  */
+#define ARG_POINTER_REGNUM              \
+        (AP_REG_NUM)
+
+/* Fake register that holds the address on the stack of the
+   current function's return address.  */
+#define RETURN_ADDRESS_POINTER_REGNUM   \
+        (RAP_REG_NUM)
+
+/* Register in which static-chain is passed to a function.  */
+#define STATIC_CHAIN_REGNUM             \
+        (GP_REG_FIRST + MB_ABI_STATIC_CHAIN_REGNUM)
+
+/* If the structure value address is passed in a register, then
+   `STRUCT_VALUE_REGNUM' should be the number of that register.  */
+/* #define STRUCT_VALUE_REGNUM (GP_REG_FIRST + 4) */
+
+/* If the structure value address is not passed in a register, define
+   `STRUCT_VALUE' as an expression returning an RTX for the place
+   where the address is passed.  If it returns 0, the address is
+   passed as an "invisible" first argument.  */
+#define STRUCT_VALUE    0
+
+/* registers used in prologue/epilogue code when the stack frame
+   is larger than 32K bytes.  These registers must come from the
+   scratch register set, and not used for passing and returning
+   arguments and any other information used in the calling sequence
+   (such as pic).  
+*/
+
+#define MICROBLAZE_TEMP1_REGNUM         \
+        (GP_REG_FIRST + MB_ABI_TEMP1_REGNUM)
+
+#define MICROBLAZE_TEMP2_REGNUM         \
+        (GP_REG_FIRST + MB_ABI_TEMP2_REGNUM)
+
+/* Define this macro if it is as good or better to call a constant
+   function address than to call an address kept in a register.  */
+#define NO_FUNCTION_CSE                 1
+
+/* Define this macro if it is as good or better for a function to
+   call itself with an explicit address than to call an address
+   kept in a register.  */
+#define NO_RECURSIVE_FUNCTION_CSE       1
+
+/* The register number of the register used to address a table of
+   static data addresses in memory.  In some cases this register is
+   defined by a processor's "application binary interface" (ABI). 
+   When this macro is defined, RTL is generated for this register
+   once, as with the stack pointer and frame pointer registers.  If
+   this macro is not defined, it is up to the machine-dependent
+   files to allocate such a register (if necessary).  */
+#define PIC_OFFSET_TABLE_REGNUM         \
+        (GP_REG_FIRST + MB_ABI_PIC_ADDR_REGNUM)
+
+#define PIC_FUNCTION_ADDR_REGNUM        \
+        (GP_REG_FIRST + MB_ABI_PIC_FUNC_REGNUM)
+
+/* Initialize embedded_pic_fnaddr_rtx before RTL generation for
+   each function.  We used to do this in FINALIZE_PIC, but FINALIZE_PIC
+   isn't always called for static inline functions.  */
+#define INIT_EXPANDERS			\
+do {					\
+  embedded_pic_fnaddr_rtx = NULL;	\
+} while (0)
+
+/* Define the classes of registers for register constraints in the
+   machine description.  Also define ranges of constants.
+
+   One of the classes must always be named ALL_REGS and include all hard regs.
+   If there is more than one class, another class must be named NO_REGS
+   and contain no registers.
+
+   The name GENERAL_REGS must be the name of a class (or an alias for
+   another name such as ALL_REGS).  This is the class of registers
+   that is allowed by "g" or "r" in a register constraint.
+   Also, registers outside this class are allocated only when
+   instructions express preferences for them.
+
+   The classes must be numbered in nondecreasing order; that is,
+   a larger-numbered class must never be contained completely
+   in a smaller-numbered class.
+
+   For any two classes, it is very desirable that there be another
+   class that represents their union.  */
+
+enum reg_class
+{
+    NO_REGS,			/* no registers in set */
+    GR_REGS,			/* integer registers */
+    FP_REGS,			/* floating point registers */
+    HI_REG,			/* hi register */
+    LO_REG,			/* lo register */
+    HILO_REG,			/* hilo register pair for 64 bit mode mult */
+    MD_REGS,			/* multiply/divide registers (hi/lo) */
+    HI_AND_GR_REGS,		/* union classes */
+    LO_AND_GR_REGS,
+    HILO_AND_GR_REGS,
+    ST_REGS,			/* status registers (fp status) */
+    ALL_REGS,			/* all registers */
+    LIM_REG_CLASSES		/* max value + 1 */
+};
+
+#define N_REG_CLASSES (int) LIM_REG_CLASSES
+
+#define GENERAL_REGS GR_REGS
+
+/* An initializer containing the names of the register classes as C
+   string constants.  These names are used in writing some of the
+   debugging dumps.  */
+
+#define REG_CLASS_NAMES							\
+{									\
+  "NO_REGS",								\
+  "GR_REGS",								\
+  "FP_REGS",								\
+  "HI_REG",								\
+  "LO_REG",								\
+  "HILO_REG",								\
+  "MD_REGS",								\
+  "HI_AND_GR_REGS",							\
+  "LO_AND_GR_REGS",							\
+  "HILO_AND_GR_REGS",							\
+  "ST_REGS",								\
+  "ALL_REGS"								\
+}
+
+/* An initializer containing the contents of the register classes,
+   as integers which are bit masks.  The Nth integer specifies the
+   contents of class N.  The way the integer MASK is interpreted is
+   that register R is in the class if `MASK & (1 << R)' is 1.
+
+   When the machine has more than 32 registers, an integer does not
+   suffice.  Then the integers are replaced by sub-initializers,
+   braced groupings containing several integers.  Each
+   sub-initializer must be suitable as an initializer for the type
+   `HARD_REG_SET' which is defined in `hard-reg-set.h'.  */
+
+#define REG_CLASS_CONTENTS						\
+{									\
+  { 0x00000000, 0x00000000, 0x00000000 },	/* no registers */	\
+  { 0xffffffff, 0x00000000, 0x00000000 },	/* integer registers */	\
+  { 0x00000000, 0xffffffff, 0x00000000 },	/* floating registers*/	\
+  { 0x00000000, 0x00000000, 0x00000001 },	/* hi register */	\
+  { 0x00000000, 0x00000000, 0x00000002 },	/* lo register */	\
+  { 0x00000000, 0x00000000, 0x00000004 },	/* hilo register */	\
+  { 0x00000000, 0x00000000, 0x00000003 },	/* mul/div registers */	\
+  { 0xffffffff, 0x00000000, 0x00000001 },	/* union classes */     \
+  { 0xffffffff, 0x00000000, 0x00000002 },				\
+  { 0xffffffff, 0x00000000, 0x00000004 },				\
+  { 0x00000000, 0x00000000, 0x000007f8 },	/* status registers */	\
+  { 0xffffffff, 0xffffffff, 0x000007ff }	/* all registers */	\
+}
+
+
+/* A C expression whose value is a register class containing hard
+   register REGNO.  In general there is more that one such class;
+   choose a class which is "minimal", meaning that no smaller class
+   also contains the register.  */
+
+extern enum reg_class microblaze_regno_to_class[];
+
+#define REGNO_REG_CLASS(REGNO) microblaze_regno_to_class[ (REGNO) ]
+
+/* A macro whose definition is the name of the class to which a
+   valid base register must belong.  A base register is one used in
+   an address which is the register value plus a displacement.  */
+
+#define BASE_REG_CLASS  GR_REGS
+
+/* A macro whose definition is the name of the class to which a
+   valid index register must belong.  An index register is one used
+   in an address where its value is either multiplied by a scale
+   factor or added to another register (as well as added to a
+   displacement).  */
+
+#define INDEX_REG_CLASS GR_REGS
+
+
+/* This macro is used later on in the file.  */
+#define GR_REG_CLASS_P(CLASS) ((CLASS) == GR_REGS)
+
+/* REG_ALLOC_ORDER is to order in which to allocate registers.  This
+   is the default value (allocate the registers in numeric order).  We
+   define it just so that we can override it if necessary in
+   ORDER_REGS_FOR_LOCAL_ALLOC.  */
+
+#define REG_ALLOC_ORDER							\
+{  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 16,	\
+  17, 18, 19, 20, 21, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 15,	\
+  32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47,	\
+  48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63,	\
+  64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75		        \
+}
+
+/* ORDER_REGS_FOR_LOCAL_ALLOC is a macro which permits reg_alloc_order
+   to be rearranged based on a particular function.  
+*/
+
+#define ORDER_REGS_FOR_LOCAL_ALLOC microblaze_order_regs_for_local_alloc ()
+
+/* REGISTER AND CONSTANT CLASSES */
+
+/* Get reg_class from a letter such as appears in the machine
+   description.
+
+   DEFINED REGISTER CLASSES:
+
+   'd'  General (aka integer) registers
+   Normally this is GR_REGS
+   'y'  General registers 
+   'f'	Floating point registers
+   'h'	Hi register
+   'l'	Lo register
+   'x'	Multiply/divide registers
+   'a'	HILO_REG
+   'z'	FP Status register
+   'b'	All registers */
+
+extern enum reg_class microblaze_char_to_class[];
+
+#define REG_CLASS_FROM_LETTER(C) microblaze_char_to_class[(C)]
+
+/* The letters I, J, K, L, M, N, O, and P in a register constraint
+   string can be used to stand for particular ranges of immediate
+   operands.  This macro defines what the ranges are.  C is the
+   letter, and VALUE is a constant value.  Return 1 if VALUE is
+   in the range specified by C.  */
+
+/* 
+`I'	is used for the range of constants an arithmetic insn can
+actually contain (16 bits signed integers).
+
+`J'	is used for the range which is just zero (ie, $r0).
+
+`K'	is used for the range of constants a logical insn can actually
+contain (16 bit zero-extended integers).
+
+`L'	is used for the range of constants that be loaded with lui
+(ie, the bottom 16 bits are zero).
+
+`M'	is used for the range of constants that take two words to load
+(ie, not matched by `I', `K', and `L').
+
+`N'	is used for negative 16 bit constants other than -65536.
+
+`O'	is a 15 bit signed integer.
+
+`P'	is used for positive 16 bit constants.  */
+
+#define SMALL_INT(X) ((unsigned HOST_WIDE_INT) (INTVAL (X) + 0x8000) < 0x10000)
+#define SMALL_INT_UNSIGNED(X) ((unsigned HOST_WIDE_INT) (INTVAL (X)) < 0x10000)
+
+/* Deifinition of K changed for MicroBlaze specific code */
+
+#define CONST_OK_FOR_LETTER_P(VALUE, C)					\
+  ((C) == 'I' ? ((unsigned HOST_WIDE_INT) ((VALUE) + 0x8000) < 0x10000)	\
+   : (C) == 'J' ? ((VALUE) == 0)					\
+   : (C) == 'K' ? (((VALUE) >= -32768) && ((VALUE) <= 32767))           \
+   : (C) == 'L' ? (((VALUE) & 0x0000ffff) == 0				\
+		   && (((VALUE) & ~2147483647) == 0			\
+		       || ((VALUE) & ~2147483647) == ~2147483647))	\
+   : (C) == 'M' ? ((((VALUE) & ~0x0000ffff) != 0)			\
+		   && (((VALUE) & ~0x0000ffff) != ~0x0000ffff)		\
+		   && (((VALUE) & 0x0000ffff) != 0			\
+		       || (((VALUE) & ~2147483647) != 0			\
+			   && ((VALUE) & ~2147483647) != ~2147483647)))	\
+   : (C) == 'N' ? ((unsigned HOST_WIDE_INT) ((VALUE) + 0xffff) < 0xffff) \
+   : (C) == 'O' ? ((unsigned HOST_WIDE_INT) ((VALUE) + 0x4000) < 0x8000) \
+   : (C) == 'P' ? ((VALUE) != 0 && (((VALUE) & ~0x0000ffff) == 0))	\
+   : 0)
+
+/* Similar, but for floating constants, and defining letters G and H.
+   Here VALUE is the CONST_DOUBLE rtx itself.  */
+
+/* 'G'	: Floating point 0 */
+
+#define CONST_DOUBLE_OK_FOR_LETTER_P(VALUE, C)				\
+  ((C) == 'G'								\
+   && (VALUE) == CONST0_RTX (GET_MODE (VALUE)))
+
+/* Letters in the range `Q' through `U' may be defined in a
+   machine-dependent fashion to stand for arbitrary operand types. 
+   The machine description macro `EXTRA_CONSTRAINT' is passed the
+   operand as its first argument and the constraint letter as its
+   second operand.
+
+   `R'	is for memory references which take 1 word for the instruction.
+   `S'	is for references to extern items which are PIC for OSF/rose.
+   `T'	is for memory addresses that can be used to load two words.  */
+
+#define EXTRA_CONSTRAINT(OP,CODE)					\
+  (((CODE) == 'T')	  ? double_memory_operand (OP, GET_MODE (OP))	\
+   : ((CODE) == 'Q')	  ? FALSE                                       \
+   : (GET_CODE (OP) != MEM) ? FALSE					\
+   : ((CODE) == 'R')	  ? simple_memory_operand (OP, GET_MODE (OP))	\
+   : ((CODE) == 'S')	  ? (HALF_PIC_P () && CONSTANT_P (OP)		\
+			     && HALF_PIC_ADDRESS_P (OP))		\
+   : ((CODE) == 's')      ? ST_REG_P(INTVAL(OP))   			\
+   : FALSE)
+
+/* Say which of the above are memory constraints.  */
+#define EXTRA_MEMORY_CONSTRAINT(C, STR) ((C) == 'R' || (C) == 'T')
+
+/* Given an rtx X being reloaded into a reg required to be
+   in class CLASS, return the class of reg to actually use.
+   In general this is just CLASS; but on some machines
+   in some cases it is preferable to use a more restrictive class.  */
+
+#define PREFERRED_RELOAD_CLASS(X,CLASS)					\
+  ((CLASS) != ALL_REGS							\
+   ? (CLASS)							\
+   : ((GET_MODE_CLASS (GET_MODE (X)) == MODE_FLOAT			\
+       || GET_MODE_CLASS (GET_MODE (X)) == MODE_COMPLEX_FLOAT)		\
+      ? (GR_REGS)			\
+      : ((GET_MODE_CLASS (GET_MODE (X)) == MODE_INT			\
+	  || GET_MODE (X) == VOIDmode)					\
+	 ? (GR_REGS) : (CLASS))))
+
+/* Certain machines have the property that some registers cannot be
+   copied to some other registers without using memory.  Define this
+   macro on those machines to be a C expression that is non-zero if
+   objects of mode MODE in registers of CLASS1 can only be copied to
+   registers of class CLASS2 by storing a register of CLASS1 into
+   memory and loading that memory location into a register of CLASS2.
+
+   Do not define this macro if its value would always be zero.  */
+
+#define SECONDARY_MEMORY_NEEDED(CLASS1, CLASS2, MODE)			\
+  ((!TARGET_DEBUG_H_MODE						\
+    && GET_MODE_CLASS (MODE) == MODE_INT				\
+    && ((CLASS1 == FP_REGS && GR_REG_CLASS_P (CLASS2))			\
+	|| (GR_REG_CLASS_P (CLASS1) && CLASS2 == FP_REGS))))
+
+/* The HI and LO registers can only be reloaded via the general
+   registers.  Condition code registers can only be loaded to the
+   general registers, and from the floating point registers.  */
+
+#define SECONDARY_INPUT_RELOAD_CLASS(CLASS, MODE, X)			\
+  microblaze_secondary_reload_class (CLASS, MODE, X, 1)
+#define SECONDARY_OUTPUT_RELOAD_CLASS(CLASS, MODE, X)			\
+  microblaze_secondary_reload_class (CLASS, MODE, X, 0)
+
+/* Not declared above, with the other functions, because enum
+   reg_class is not declared yet.  */
+/* enum reg_class	microblaze_secondary_reload_class ();*/
+
+/* Return the maximum number of consecutive registers
+   needed to represent mode MODE in a register of class CLASS.  */
+
+#define CLASS_UNITS(mode, size)						\
+  ((GET_MODE_SIZE (mode) + (size) - 1) / (size))
+
+#define CLASS_MAX_NREGS(CLASS, MODE)					\
+  ((CLASS) == FP_REGS							\
+   ? 2 * CLASS_UNITS (MODE, 8)					\
+   : CLASS_UNITS (MODE, UNITS_PER_WORD))
+
+
+/* Stack layout; function entry, exit and calling.  */
+
+/* Define this if pushing a word on the stack
+   makes the stack pointer a smaller address.  */
+#define STACK_GROWS_DOWNWARD
+
+/* Offset within stack frame to start allocating local variables at.
+   If FRAME_GROWS_DOWNWARD, this is the offset to the END of the
+   first local allocated.  Otherwise, it is the offset to the BEGINNING
+   of the first local allocated.  */
+/* Changed the starting frame offset to including the new link stuff */
+#define STARTING_FRAME_OFFSET						\
+  (current_function_outgoing_args_size					\
+   +  (FIRST_PARM_OFFSET(FNDECL)))
+
+/* The return address for the current frame is in r31 is this is a leaf
+   function.  Otherwise, it is on the stack.  It is at a variable offset
+   from sp/fp/ap, so we define a fake hard register rap which is a
+   poiner to the return address on the stack.  This always gets eliminated
+   during reload to be either the frame pointer or the stack pointer plus
+   an offset.  */
+
+/* ??? This definition fails for leaf functions.  There is currently no
+   general solution for this problem.  */
+
+/* ??? There appears to be no way to get the return address of any previous
+   frame except by disassembling instructions in the prologue/epilogue.
+   So currently we support only the current frame.  */
+
+#define RETURN_ADDR_RTX(count, frame)			\
+  ((count == 0)						\
+   ? gen_rtx (MEM, Pmode, gen_rtx (REG, Pmode, RETURN_ADDRESS_POINTER_REGNUM))\
+   : (rtx) 0)
+
+/* Structure to be filled in by compute_frame_size with register
+   save masks, and offsets for the current function.  */
+
+    struct microblaze_frame_info
+    {
+        long total_size;		/* # bytes that the entire frame takes up */
+        long var_size;                  /* # bytes that variables take up */
+        long args_size;                 /* # bytes that outgoing arguments take up */
+        int  link_debug_size;           /* # bytes for the link reg and back pointer */
+        int  gp_reg_size;		/* # bytes needed to store gp regs */
+        long gp_offset;		        /* offset from new sp to store gp registers */
+        long mask;			/* mask of saved gp registers */
+        int  initialized;		/* != 0 if frame size already calculated */
+        int  num_gp;			/* number of gp registers saved */
+        long insns_len;                 /* length of insns */
+        int  alloc_stack;               /* Flag to indicate if the current function must not create stack space. (As an optimization) */
+    };
+
+extern struct microblaze_frame_info current_frame_info;
+
+/* If defined, this macro specifies a table of register pairs used to
+   eliminate unneeded registers that point into the stack frame.  If
+   it is not defined, the only elimination attempted by the compiler
+   is to replace references to the frame pointer with references to
+   the stack pointer.
+
+   The definition of this macro is a list of structure
+   initializations, each of which specifies an original and
+   replacement register.
+
+   On some machines, the position of the argument pointer is not
+   known until the compilation is completed.  In such a case, a
+   separate hard register must be used for the argument pointer. 
+   This register can be eliminated by replacing it with either the
+   frame pointer or the argument pointer, depending on whether or not
+   the frame pointer has been eliminated.
+
+   In this case, you might specify:
+   #define ELIMINABLE_REGS  \
+   {{ARG_POINTER_REGNUM, STACK_POINTER_REGNUM}, \
+   {ARG_POINTER_REGNUM, FRAME_POINTER_REGNUM}, \
+   {FRAME_POINTER_REGNUM, STACK_POINTER_REGNUM}}
+
+   Note that the elimination of the argument pointer with the stack
+   pointer is specified first since that is the preferred elimination.
+
+*/
+
+#define ELIMINABLE_REGS							\
+{{ ARG_POINTER_REGNUM,   STACK_POINTER_REGNUM},				\
+ { ARG_POINTER_REGNUM,   GP_REG_FIRST + MB_ABI_FRAME_POINTER_REGNUM},				\
+ { RETURN_ADDRESS_POINTER_REGNUM, STACK_POINTER_REGNUM},		\
+ { RETURN_ADDRESS_POINTER_REGNUM, GP_REG_FIRST + MB_ABI_FRAME_POINTER_REGNUM},			\
+ { RETURN_ADDRESS_POINTER_REGNUM, GP_REG_FIRST + MB_ABI_SUB_RETURN_ADDR_REGNUM},			\
+ { FRAME_POINTER_REGNUM, STACK_POINTER_REGNUM},				\
+ { FRAME_POINTER_REGNUM, GP_REG_FIRST + MB_ABI_FRAME_POINTER_REGNUM}}
+
+/* A C expression that returns non-zero if the compiler is allowed to
+   try to replace register number FROM-REG with register number
+   TO-REG.  This macro need only be defined if `ELIMINABLE_REGS' is
+   defined, and will usually be the constant 1, since most of the
+   cases preventing register elimination are things that the compiler
+   already knows about.
+
+   We can always eliminate to the
+   frame pointer.  We can eliminate to the stack pointer unless
+   a frame pointer is needed.  
+*/
+
+#define CAN_ELIMINATE(FROM, TO)						\
+  (((FROM) == RETURN_ADDRESS_POINTER_REGNUM && (! leaf_function_p ()	\
+   || (TO == GP_REG_FIRST + MB_ABI_SUB_RETURN_ADDR_REGNUM && leaf_function_p)))   			\
+  || ((FROM) != RETURN_ADDRESS_POINTER_REGNUM				\
+   && ((TO) == HARD_FRAME_POINTER_REGNUM 				\
+   || ((TO) == STACK_POINTER_REGNUM && ! frame_pointer_needed))))
+
+
+/* This macro is similar to `INITIAL_FRAME_POINTER_OFFSET'.  It
+   specifies the initial difference between the specified pair of
+   registers.  This macro must be defined if `ELIMINABLE_REGS' is
+   defined.  */
+
+#define INITIAL_ELIMINATION_OFFSET(FROM, TO, OFFSET)			 \
+        (OFFSET) = microblaze_initial_elimination_offset ((FROM), (TO))
+
+/* If defined, the maximum amount of space required for outgoing
+   arguments will be computed and placed into the variable
+   `current_function_outgoing_args_size'.  No space will be pushed
+   onto the stack for each call; instead, the function prologue
+   should increase the stack frame size by this amount.
+
+   It is not proper to define both `PUSH_ROUNDING' and
+   `ACCUMULATE_OUTGOING_ARGS'.  */
+#define ACCUMULATE_OUTGOING_ARGS        1
+
+/* Offset from the argument pointer register to the first argument's
+   address.  On some machines it may depend on the data type of the
+   function.
+
+   If `ARGS_GROW_DOWNWARD', this is the offset to the location above
+   the first argument's address.
+
+   On the MICROBLAZE, we must skip the first argument position if we are
+   returning a structure or a union, to account for its address being
+   passed in $4.  However, at the current time, this produces a compiler
+   that can't bootstrap, so comment it out for now.  */
+
+/* The first Parameter is dependent wether the
+   current_function_is_leaf and the wether in debug mode or not */
+
+/*  GCC 3.4.1 
+ *  Ideal definition is commented out below as "Old Code". Somehow, current_function_is_leaf seems to be incorrect when used in these contexts.
+ *  Hence there is a hack in compute_frame_size to identify when not to allocate stack for leaf functions.
+ */ 
+#define FIRST_PARM_OFFSET(FNDECL)       (UNITS_PER_WORD)  
+
+/* Old Code */		     
+/*#define FIRST_PARM_OFFSET(FNDECL) (((current_function_is_leaf == 0 ? 1 : 0)  * (UNITS_PER_WORD)))   */
+
+
+/* When a parameter is passed in a register, stack space is still
+   allocated for it.  For the MICROBLAZE, stack space must be allocated 
+
+   BEWARE that some space is also allocated for non existing arguments
+   in register. In case an argument list is of form GF used registers
+   are a0 (a2,a3), but we should push over a1...  */
+
+#define REG_PARM_STACK_SPACE(FNDECL)        (MAX_ARGS_IN_REGISTERS * UNITS_PER_WORD) 
+
+/* Define this if it is the responsibility of the caller to
+   allocate the area reserved for arguments passed in registers. 
+   If `ACCUMULATE_OUTGOING_ARGS' is also defined, the only effect
+   of this macro is to determine whether the space is included in 
+   `current_function_outgoing_args_size'.  */
+#define OUTGOING_REG_PARM_STACK_SPACE       1
+
+/* Align stack frames on 32 bits */
+#define STACK_BOUNDARY                      32
+
+/* Make sure 4 words are always allocated on the stack.  */
+/* XLNX : Need to see if this should be changed to 6 since there are 6
+   registers for argument now */
+
+/* Added NUM_OF_ARGS to have value 6, so that atleast 6 *
+   UNITS_PER_WORD is stored for arguments */
+    
+#define NUM_OF_ARGS                         6
+
+/* A C expression that should indicate the number of bytes of its
+   own arguments that a function pops on returning, or 0
+   if the function pops no arguments and the caller must therefore
+   pop them all after the function returns.
+
+   FUNDECL is the declaration node of the function (as a tree).
+
+   FUNTYPE is a C variable whose value is a tree node that
+   describes the function in question.  Normally it is a node of
+   type `FUNCTION_TYPE' that describes the data type of the function.
+   From this it is possible to obtain the data types of the value
+   and arguments (if known).
+
+   When a call to a library function is being considered, FUNTYPE
+   will contain an identifier node for the library function.  Thus,
+   if you need to distinguish among various library functions, you
+   can do so by their names.  Note that "library function" in this
+   context means a function used to perform arithmetic, whose name
+   is known specially in the compiler and was not mentioned in the
+   C code being compiled.
+
+   STACK-SIZE is the number of bytes of arguments passed on the
+   stack.  If a variable number of bytes is passed, it is zero, and
+   argument popping will always be the responsibility of the
+   calling function.  */
+
+#define RETURN_POPS_ARGS(FUNDECL,FUNTYPE,SIZE) 0
+
+
+/* Symbolic macros for the registers used to return integer and floating
+   point values.  */
+
+#define GP_RETURN (GP_REG_FIRST + MB_ABI_INT_RETURN_VAL_REGNUM)
+#define FP_RETURN (GP_RETURN)
+
+/* Symbolic macros for the first/last argument registers.  */
+
+#define GP_ARG_FIRST (GP_REG_FIRST + MB_ABI_FIRST_ARG_REGNUM)
+#define GP_ARG_LAST  (GP_REG_FIRST + MB_ABI_LAST_ARG_REGNUM)
+#define FP_ARG_FIRST (FP_REG_FIRST + 12)
+#define FP_ARG_LAST  (FP_REG_FIRST + 15)
+
+#define MAX_ARGS_IN_REGISTERS	MB_ABI_MAX_ARG_REGS
+
+/* Define how to find the value returned by a library function
+   assuming the value has mode MODE.  Because we define
+   PROMOTE_FUNCTION_RETURN, we must promote the mode just as
+   PROMOTE_MODE does.  */
+
+#define LIBCALL_VALUE(MODE)						\
+  gen_rtx (REG,								\
+	   ((GET_MODE_CLASS (MODE) != MODE_INT				\
+	     || GET_MODE_SIZE (MODE) >= 4)				\
+	    ? (MODE)							\
+	    : SImode),							\
+	   ((GET_MODE_CLASS (MODE) == MODE_FLOAT			\
+	     && (! TARGET_SINGLE_FLOAT					\
+		 || GET_MODE_SIZE (MODE) <= 4))				\
+	    ? FP_RETURN							\
+	    : GP_RETURN))
+
+/* Define how to find the value returned by a function.
+   VALTYPE is the data type of the value (as a tree).
+   If the precise function being called is known, FUNC is its FUNCTION_DECL;
+   otherwise, FUNC is 0.  */
+
+#define FUNCTION_VALUE(VALTYPE, FUNC) LIBCALL_VALUE (TYPE_MODE (VALTYPE))
+
+
+/* 1 if N is a possible register number for a function value.
+   On the MICROBLAZE, R2 R3 and F0 F2 are the only register thus used.
+   Currently, R2 and F0 are only implemented  here (C has no complex type)  */
+
+#define FUNCTION_VALUE_REGNO_P(N) ((N) == GP_RETURN || (N) == FP_RETURN)
+
+/* 1 if N is a possible register number for function argument passing.
+   We have no FP argument registers when soft-float.  When FP registers
+   are 32 bits, we can't directly reference the odd numbered ones.  */
+
+#define FUNCTION_ARG_REGNO_P(N)					\
+  (((N) >= GP_ARG_FIRST && (N) <= GP_ARG_LAST))
+
+/* A C expression which can inhibit the returning of certain function
+   values in registers, based on the type of value.  A nonzero value says
+   to return the function value in memory, just as large structures are
+   always returned.  Here TYPE will be a C expression of type
+   `tree', representing the data type of the value.
+
+   Note that values of mode `BLKmode' must be explicitly
+   handled by this macro.  Also, the option `-fpcc-struct-return'
+   takes effect regardless of this macro.  On most systems, it is
+   possible to leave the macro undefined; this causes a default
+   definition to be used, whose value is the constant 1 for BLKmode
+   values, and 0 otherwise.
+
+   GCC normally converts 1 byte structures into chars, 2 byte
+   structs into shorts, and 4 byte structs into ints, and returns
+   them this way.  Defining the following macro overrides this,
+   to give us MICROBLAZE cc compatibility.  */
+
+#define RETURN_IN_MEMORY(TYPE)	\
+  (TYPE_MODE (TYPE) == BLKmode)
+
+/* A code distinguishing the floating point format of the target
+   machine.  There are three defined values: IEEE_FLOAT_FORMAT,
+   VAX_FLOAT_FORMAT, and UNKNOWN_FLOAT_FORMAT.  */
+
+#define TARGET_FLOAT_FORMAT IEEE_FLOAT_FORMAT
+
+
+/* Define a data type for recording info about an argument list
+   during the scan of that argument list.  This data type should
+   hold all necessary information about the function itself
+   and about the args processed so far, enough to enable macros
+   such as FUNCTION_ARG to determine where the next arg should go.
+
+*/
+
+typedef struct microblaze_args {
+    int gp_reg_found;		/* whether a gp register was found yet */
+    int arg_number;		/* argument number */
+    int arg_words;		/* # total words the arguments take */
+    int fp_arg_words;		/* # words for FP args (MICROBLAZE_EABI only) */
+    int last_arg_fp;		/* nonzero if last arg was FP (EABI only) */
+    int fp_code;		/* Mode of FP arguments */
+    int num_adjusts;		/* number of adjustments made */
+				/* Adjustments made to args pass in regs.  */
+				/* ??? The size is doubled to work around a 
+				   bug in the code that sets the adjustments
+				   in function_arg.  */
+    struct rtx_def *adjust[MAX_ARGS_IN_REGISTERS*2];
+} CUMULATIVE_ARGS;
+
+/* Initialize a variable CUM of type CUMULATIVE_ARGS
+   for a call to a function whose data type is FNTYPE.
+   For a library call, FNTYPE is 0.
+
+*/
+
+#define INIT_CUMULATIVE_ARGS(CUM,FNTYPE,LIBNAME,FNDECL,N_NAMED_ARGS)		\
+  init_cumulative_args (&CUM, FNTYPE, LIBNAME)                                  
+
+/* Update the data in CUM to advance over an argument
+   of mode MODE and data type TYPE.
+   (TYPE is null for libcalls where that information may not be available.)  */
+
+#define FUNCTION_ARG_ADVANCE(CUM, MODE, TYPE, NAMED)			\
+  function_arg_advance (&CUM, MODE, TYPE, NAMED)
+
+/* Determine where to put an argument to a function.
+   Value is zero to push the argument on the stack,
+   or a hard register in which to store the argument.
+
+   MODE is the argument's machine mode.
+   TYPE is the data type of the argument (as a tree).
+   This is null for libcalls where that information may
+   not be available.
+   CUM is a variable of type CUMULATIVE_ARGS which gives info about
+   the preceding args and about the function being called.
+   NAMED is nonzero if this argument is a named parameter
+   (otherwise it is an extra parameter matching an ellipsis).  */
+
+#define FUNCTION_ARG(CUM, MODE, TYPE, NAMED) \
+  function_arg( &CUM, MODE, TYPE, NAMED)
+
+/* For an arg passed partly in registers and partly in memory,
+   this is the number of registers used.
+   For args passed entirely in registers or entirely in memory, zero. */
+
+#define FUNCTION_ARG_PARTIAL_NREGS(CUM, MODE, TYPE, NAMED) \
+  function_arg_partial_nregs (&CUM, MODE, TYPE, NAMED)
+
+/* Tell prologue and epilogue if register REGNO should be saved / restored.  */
+
+/* Added the link register for functions which are not leaf */
+/* Also added the interrupt_handler temp reg save, which is being used for saving the MSR */
+
+/* [02/13/02] : Added interrupt and exception registers to the list of
+   registers being saved on interrupts */
+
+/* [05/28/02] : Modified to make interrupt_handler save all the registers */
+/* Save all volatiles if you have the interrupt handler attribute */
+
+#define MUST_SAVE_REGISTER(regno) \
+ (((regs_ever_live[regno] && !call_used_regs[regno])			\
+  || (regno == HARD_FRAME_POINTER_REGNUM && frame_pointer_needed)       \
+  || (((regs_ever_live[regno] || (regno == MB_ABI_MSR_SAVE_REG)) && interrupt_handler))   \
+  || (regs_ever_live[regno] && save_volatiles)                 \
+  || (regno == MB_ABI_ASM_TEMP_REGNUM && ( save_volatiles || interrupt_handler)) \
+  || (regno == MB_ABI_EXCEPTION_RETURN_ADDR_REGNUM && ( save_volatiles || interrupt_handler)) \
+  || ((regno == MB_ABI_SUB_RETURN_ADDR_REGNUM) && (!current_function_is_leaf))) \
+  || ((regno >= 3 && regno <= 12) && (interrupt_handler || save_volatiles) && (!current_function_is_leaf))\
+    && regno != 0)
+
+/* Define this macro if the mcount subroutine on your system does not need 
+   a counter variable allocated for each function. This is true for almost 
+   all modern implementations. If you define this macro, you must not use 
+   the labelno argument to FUNCTION_PROFILER. */
+
+#define NO_PROFILE_COUNTERS         1
+
+/* Output assembler code to FILE to call profiling routine '_mcount'
+   for profiling a function entry.  */
+
+#define FUNCTION_PROFILER(FILE, LABELNO) { \
+  {                                        \
+    char *fnname = XSTR (XEXP (DECL_RTL (current_function_decl), 0), 0); \
+    fprintf (FILE, "\tbrki\tr16,_mcount\n");           \
+  }                                                    \
+ }
+
+/* Define this macro if the code for function profiling should come
+   before the function prologue.  Normally, the profiling code comes
+   after.  
+   
+   #define PROFILE_BEFORE_PROLOGUE 
+*/
+
+/* EXIT_IGNORE_STACK should be nonzero if, when returning from a function,
+   the stack pointer does not matter.  The value is tested only in
+   functions that have frame pointers.
+   No definition is equivalent to always zero.  */
+
+#define EXIT_IGNORE_STACK 1
+
+
+/* A C statement to output, on the stream FILE, assembler code for a
+   block of data that contains the constant parts of a trampoline. 
+   This code should not include a label--the label is taken care of
+   automatically.  */
+
+#define TRAMPOLINE_TEMPLATE(STREAM)					 \
+{									 \
+  fprintf (STREAM, "\t.word\t0x03e00821\t\t# move   $1,$31\n");		\
+  fprintf (STREAM, "\t.word\t0x04110001\t\t# bgezal $0,.+8\n");		\
+  fprintf (STREAM, "\t.word\t0x00000000\t\t# nop\n");			\
+  fprintf (STREAM, "\t.word\t0x8fe30014\t\t# lw     $3,20($31)\n");	\
+  fprintf (STREAM, "\t.word\t0x8fe20018\t\t# lw     $2,24($31)\n");	\
+  fprintf (STREAM, "\t.word\t0x0060c821\t\t# move   $25,$3 (abicalls)\n"); \
+  fprintf (STREAM, "\t.word\t0x00600008\t\t# jr     $3\n");		\
+  fprintf (STREAM, "\t.word\t0x0020f821\t\t# move   $31,$1\n");		\
+  fprintf (STREAM, "\t.word\t0x00000000\t\t# <function address>\n"); \
+  fprintf (STREAM, "\t.word\t0x00000000\t\t# <static chain value>\n"); \
+}
+
+/* A C expression for the size in bytes of the trampoline, as an
+   integer.  */
+
+#define TRAMPOLINE_SIZE (32 + (8))
+
+/* Alignment required for trampolines, in bits.  */
+
+#define TRAMPOLINE_ALIGNMENT    32
+
+/* INITIALIZE_TRAMPOLINE calls this library function to flush
+   program and data caches.  */
+
+#ifndef CACHE_FLUSH_FUNC
+#define CACHE_FLUSH_FUNC "_flush_cache"
+#endif
+
+/* A C statement to initialize the variable parts of a trampoline. 
+   ADDR is an RTX for the address of the trampoline; FNADDR is an
+   RTX for the address of the nested function; STATIC_CHAIN is an
+   RTX for the static chain value that should be passed to the
+   function when it is called.  */
+
+#define INITIALIZE_TRAMPOLINE(ADDR, FUNC, CHAIN)			    \
+{									    \
+  rtx addr = ADDR;							    \
+  emit_move_insn (gen_rtx (MEM, SImode, plus_constant (addr, 32)), FUNC);   \
+  emit_move_insn (gen_rtx (MEM, SImode, plus_constant (addr, 36)), CHAIN);  \
+									    \
+  /* Flush both caches.  We need to flush the data cache in case	    \
+     the system has a write-back cache.  */				    \
+  /* ??? Should check the return value for errors.  */			    \
+  emit_library_call (gen_rtx (SYMBOL_REF, Pmode, CACHE_FLUSH_FUNC),	    \
+		     0, VOIDmode, 3, addr, Pmode,			    \
+		     GEN_INT (TRAMPOLINE_SIZE), TYPE_MODE (integer_type_node),\
+		     GEN_INT (3), TYPE_MODE (integer_type_node));	    \
+}
+
+/* Addressing modes, and classification of registers for them.  */
+
+#define REGNO_OK_FOR_BASE_P(regno)   \
+  microblaze_regno_ok_for_base_p ((regno), 1)
+
+
+#define REGNO_OK_FOR_INDEX_P(regno)  \
+  microblaze_regno_ok_for_base_p ((regno), 1)
+
+/* The macros REG_OK_FOR..._P assume that the arg is a REG rtx
+   and check its validity for a certain class.
+   We have two alternate definitions for each of them.
+   The usual definition accepts all pseudo regs; the other rejects them all.
+   The symbol REG_OK_STRICT causes the latter definition to be used.
+
+   Most source files want to accept pseudo regs in the hope that
+   they will get allocated to the class that the insn wants them to be in.
+   Some source files that are used after register allocation
+   need to be strict.  */
+
+#ifndef REG_OK_STRICT
+#define REG_STRICT_FLAG         0
+#else
+#define REG_STRICT_FLAG         1
+#endif
+
+/* Nonzero if X is a hard reg that can be used as an index
+   or if it is a pseudo reg in the non-strict case.  */
+#define REG_OK_FOR_BASE_P(X)    \
+  microblaze_regno_ok_for_base_p (REGNO (X), REG_STRICT_FLAG)
+
+/* Nonzero if X is a hard reg that can be used as an index
+   or if it is a pseudo reg in the non-strict case.  */
+#define REG_OK_FOR_INDEX_P(X)   \
+  microblaze_regno_ok_for_base_p (REGNO (X), REG_STRICT_FLAG)
+
+
+/* Maximum number of registers that can appear in a valid memory address.  */
+
+#define MAX_REGS_PER_ADDRESS 2
+
+/* A C compound statement with a conditional `goto LABEL;' executed
+   if X (an RTX) is a legitimate memory address on the target
+   machine for a memory operand of mode MODE.
+
+   It usually pays to define several simpler macros to serve as
+   subroutines for this one.  Otherwise it may be too complicated
+   to understand.
+
+   This macro must exist in two variants: a strict variant and a
+   non-strict one.  The strict variant is used in the reload pass. 
+   It must be defined so that any pseudo-register that has not been
+   allocated a hard register is considered a memory reference.  In
+   contexts where some kind of register is required, a
+   pseudo-register with no hard register must be rejected.
+
+   The non-strict variant is used in other passes.  It must be
+   defined to accept all pseudo-registers in every context where
+   some kind of register is required.
+
+   Typically among the subroutines used to define
+   `GO_IF_LEGITIMATE_ADDRESS' are subroutines to check for
+   acceptable registers for various purposes (one for base
+   registers, one for index registers, and so on).  Then only these
+   subroutine macros need have two variants; the higher levels of
+   macros may be the same whether strict or not.
+
+   Normally, constant addresses which are the sum of a `symbol_ref'
+   and an integer are stored inside a `const' RTX to mark them as
+   constant.  Therefore, there is no need to recognize such sums
+   specifically as legitimate addresses.  Normally you would simply
+   recognize any `const' as legitimate.
+
+   Usually `PRINT_OPERAND_ADDRESS' is not prepared to handle
+   constant sums that are not marked with  `const'.  It assumes
+   that a naked `plus' indicates indexing.  If so, then you *must*
+   reject such naked constant sums as illegitimate addresses, so
+   that none of them will be given to `PRINT_OPERAND_ADDRESS'.
+
+   On some machines, whether a symbolic address is legitimate
+   depends on the section that the address refers to.  On these
+   machines, define the macro `ENCODE_SECTION_INFO' to store the
+   information into the `symbol_ref', and then check for it here. 
+   When you see a `const', you will have to look inside it to find
+   the `symbol_ref' in order to determine the section.  */
+
+#define GO_IF_LEGITIMATE_ADDRESS(MODE, X, ADDR)                     \
+{                                                                   \
+  if (microblaze_legitimate_address_p (MODE, X, REG_STRICT_FLAG))   \
+    goto ADDR;                                                      \
+}
+
+
+/* A C statement or compound statement with a conditional `goto
+   LABEL;' executed if memory address X (an RTX) can have different
+   meanings depending on the machine mode of the memory reference it
+   is used for.
+
+   Autoincrement and autodecrement addresses typically have
+   mode-dependent effects because the amount of the increment or
+   decrement is the size of the operand being addressed.  Some
+   machines have other mode-dependent addresses.  Many RISC machines
+   have no mode-dependent addresses.
+
+   You may assume that ADDR is a valid address for the machine.  */
+
+#define GO_IF_MODE_DEPENDENT_ADDRESS(ADDR,LABEL) {}
+
+#if 1
+#define GO_PRINTF(x)	trace(x)
+#define GO_PRINTF2(x,y)	trace(x,y)
+#define GO_DEBUG_RTX(x) debug_rtx(x)
+#else
+#define GO_PRINTF(x)
+#define GO_PRINTF2(x,y)
+#define GO_DEBUG_RTX(x)
+#endif
+
+/* A C expression that is 1 if the RTX X is a constant which is a
+   valid address.  This is defined to be the same as `CONSTANT_P (X)',
+   but rejecting CONST_DOUBLE.  */
+/* When pic, we must reject addresses of the form symbol+large int.
+   This is because an instruction `sw $4,s+70000' needs to be converted
+   by the assembler to `lw $at,s($gp);sw $4,70000($at)'.  Normally the
+   assembler would use $at as a temp to load in the large offset.  In this
+   case $at is already in use.  We convert such problem addresses to
+   `la $5,s;sw $4,70000($5)' via LEGITIMIZE_ADDRESS.  */
+#define CONSTANT_ADDRESS_P(X)						\
+  ((GET_CODE (X) == LABEL_REF || GET_CODE (X) == SYMBOL_REF		\
+    || GET_CODE (X) == CONST_INT 		                        \
+    || (GET_CODE (X) == CONST						\
+	&& ! (flag_pic && pic_address_needs_scratch (X))))		\
+   && (!HALF_PIC_P () || !HALF_PIC_ADDRESS_P (X)))
+
+/* Define this, so that when PIC, reload won't try to reload invalid
+   addresses which require two reload registers.  */
+
+#define LEGITIMATE_PIC_OPERAND_P(X)  (! pic_address_needs_scratch (X))
+
+/* Nonzero if the constant value X is a legitimate general operand.
+   It is given that X satisfies CONSTANT_P or is a CONST_DOUBLE.
+
+   At present, GAS doesn't understand li.[sd], so don't allow it
+   to be generated at present.  Also, the MICROBLAZE assembler does not
+   grok li.d Infinity.  */
+
+#define LEGITIMATE_CONSTANT_P(X)				\
+  (GET_CODE (X) != CONST_DOUBLE					\
+    || microblaze_const_double_ok (X, GET_MODE (X)))
+
+/* Try a machine-dependent way of reloading an illegitimate address
+   operand.  If we find one, push the reload and jump to WIN.  This
+   macro is used in only one place: `find_reloads_address' in reload.c.
+
+   Implemented on microblaze by microblaze_legitimize_reload_address.  
+   Note that (X) is evaluated twice; this is safe in current usage.  */ 
+#define LEGITIMIZE_ADDRESS(X,OLDX,MODE,WIN)			\
+{  rtx result = microblaze_legitimize_address (X, OLDX, MODE);	\
+   if (result != NULL_RTX) {					\
+       (X) = result;						\
+       goto WIN;						\
+     }								\
+}
+
+/* Define this macro if references to a symbol must be treated
+   differently depending on something about the variable or
+   function named by the symbol (such as what section it is in).
+
+   The macro definition, if any, is executed immediately after the
+   rtl for DECL has been created and stored in `DECL_RTL (DECL)'. 
+   The value of the rtl will be a `mem' whose address is a
+   `symbol_ref'.
+
+   The usual thing for this macro to do is to a flag in the
+   `symbol_ref' (such as `SYMBOL_REF_FLAG') or to store a modified
+   name string in the `symbol_ref' (if one bit is not enough
+   information).
+
+   The best way to modify the name string is by adding text to the
+   beginning, with suitable punctuation to prevent any ambiguity. 
+   Allocate the new name in `saveable_obstack'.  You will have to
+   modify `ASM_OUTPUT_LABELREF' to remove and decode the added text
+   and output the name accordingly.
+
+   You can also check the information stored in the `symbol_ref' in
+   the definition of `GO_IF_LEGITIMATE_ADDRESS' or
+   `PRINT_OPERAND_ADDRESS'.
+
+   When optimizing for the $gp pointer, SYMBOL_REF_FLAG is set for all
+   small objects.
+
+   When generating embedded PIC code, SYMBOL_REF_FLAG is set for
+   symbols which are not in the .text section.
+
+   When not embedded PIC, if a symbol is in a
+   gp addresable section, SYMBOL_REF_FLAG is set prevent gcc from
+   splitting the reference so that gas can generate a gp relative
+   reference.
+
+   When TARGET_EMBEDDED_DATA is set, we assume that all const
+   variables will be stored in ROM, which is too far from %gp to use
+   %gprel addressing.  Note that (1) we include "extern const"
+   variables in this, which microblaze_select_section doesn't, and (2) we
+   can't always tell if they're really const (they might be const C++
+   objects with non-const constructors), so we err on the side of
+   caution and won't use %gprel anyway (otherwise we'd have to defer
+   this decision to the linker/loader).  The handling of extern consts
+   is why the DECL_INITIAL macros differ from microblaze_select_section.
+
+   If you are changing this macro, you should look at
+   microblaze_select_section and see if it needs a similar change.  */
+
+#ifndef UNIQUE_SECTION_P
+#define UNIQUE_SECTION_P(DECL) (0)
+#endif
+
+#define TREE_STRING_RTL(NODE) (STRING_CST_CHECK (NODE)->string.rtl)
+
+#define CONSTANT_POOL_BEFORE_FUNCTION TRUE
+
+
+/* Specify the machine mode that this machine uses
+   for the index in the tablejump instruction.
+*/
+#define CASE_VECTOR_MODE    (SImode)
+
+/* Define as C expression which evaluates to nonzero if the tablejump
+   instruction expects the table to contain offsets from the address of the
+   table.
+   Do not define this if the table should contain absolute addresses. */
+
+/* GCC 3.4.1 
+ * Removed
+ */
+/* Specify the tree operation to be used to convert reals to integers.  */
+/* #define IMPLICIT_FIX_EXPR FIX_ROUND_EXPR*/
+
+/* GCC 3.4.1 
+ * Removed
+ */
+/* This is the kind of divide that is easiest to do in the general case.  */
+/* #define EASY_DIV_EXPR TRUNC_DIV_EXPR*/
+
+/* Define this as 1 if `char' should by default be signed; else as 0.  */
+#ifndef DEFAULT_SIGNED_CHAR
+#define DEFAULT_SIGNED_CHAR 1
+#endif
+
+/* Max number of bytes we can move from memory to memory
+   in one reasonably fast instruction.  */
+#define MOVE_MAX 4
+#define MAX_MOVE_MAX 8
+
+/* Define this macro as a C expression which is nonzero if
+   accessing less than a word of memory (i.e. a `char' or a
+   `short') is no faster than accessing a word of memory, i.e., if
+   such access require more than one instruction or if there is no
+   difference in cost between byte and (aligned) word loads.
+
+   On RISC machines, it tends to generate better code to define
+   this as 1, since it avoids making a QI or HI mode register.  */
+#define SLOW_BYTE_ACCESS 1
+
+/* We assume that the store-condition-codes instructions store 0 for false
+   and some other value for true.  This is the value stored for true.  */
+
+#define STORE_FLAG_VALUE 1
+
+/* Define this if zero-extension is slow (more than one real instruction).  */
+/* removed for GCC 3.3 */
+/*#define SLOW_ZERO_EXTEND*/
+
+/* Define this to be nonzero if shift instructions ignore all but the low-order
+   few bits. */
+#define SHIFT_COUNT_TRUNCATED 1
+
+/* Value is 1 if truncating an integer of INPREC bits to OUTPREC bits
+   is done just by pretending it is already truncated.  */
+/* In 64 bit mode, 32 bit instructions require that register values be properly
+   sign-extended to 64 bits.  As a result, a truncate is not a no-op if it
+   converts a value >32 bits to a value <32 bits.  */
+/* ??? This results in inefficient code for 64 bit to 32 conversions.
+   Something needs to be done about this.  Perhaps not use any 32 bit
+   instructions?  Perhaps use PROMOTE_MODE?  */
+#define TRULY_NOOP_TRUNCATION(OUTPREC, INPREC)  1
+
+/* Specify the machine mode that pointers have.
+   After generation of rtl, the compiler makes no further distinction
+   between pointers and any other objects of this machine mode. */
+
+#ifndef Pmode
+#define Pmode SImode
+#endif
+
+/* A function address in a call instruction
+   is a word address (for indexing purposes)
+   so give the MEM rtx a words's mode.  */
+
+#define FUNCTION_MODE   (SImode)
+
+/* Define TARGET_MEM_FUNCTIONS if we want to use calls to memcpy and
+   memset, instead of the BSD functions bcopy and bzero.  */
+#define TARGET_MEM_FUNCTIONS
+
+/* GCC 3.4.1
+ * Removed CONST_COSTS. Find a way to get behavior back ???
+ */
+
+/* /\* A part of a C `switch' statement that describes the relative */
+/*    costs of constant RTL expressions.  It must contain `case' */
+/*    labels for expression codes `const_int', `const', `symbol_ref', */
+/*    `label_ref' and `const_double'.  Each case must ultimately reach */
+/*    a `return' statement to return the relative cost of the use of */
+/*    that kind of constant value in an expression.  The cost may */
+/*    depend on the precise value of the constant, which is available */
+/*    for examination in X. */
+
+/*    CODE is the expression code--redundant, since it can be obtained */
+/*    with `GET_CODE (X)'.  *\/ */
+
+/* #define CONST_COSTS(X,CODE,OUTER_CODE)					\ */
+/*   case CONST_INT:							\ */
+/* 	/\* Always return 0, since we don't have different sized		\ */
+/* 	   instructions, hence different costs according to Richard	\ */
+/* 	   Kenner *\/							\ */
+/* 	return 0;							\ */
+/*   case LABEL_REF:							\ */
+/*     return COSTS_N_INSNS (2);						\ */
+/* 									\ */
+/*   case CONST:								\ */
+/*     {									\ */
+/*       rtx offset = const0_rtx;						\ */
+/*       rtx symref = eliminate_constant_term (XEXP (X, 0), &offset);	\ */
+/* 									\ */
+/*       if (GET_CODE (symref) == LABEL_REF)				\ */
+/* 	return COSTS_N_INSNS (2);					\ */
+/* 									\ */
+/*       if (GET_CODE (symref) != SYMBOL_REF)				\ */
+/* 	return COSTS_N_INSNS (4);					\ */
+/* 									\ */
+/*       /\* let's be paranoid.... *\/					\ */
+/*       if (INTVAL (offset) < -32768 || INTVAL (offset) > 32767)		\ */
+/* 	return COSTS_N_INSNS (2);					\ */
+/* 									\ */
+/*       return COSTS_N_INSNS (SYMBOL_REF_FLAG (symref) ? 1 : 2);		\ */
+/*     }									\ */
+/* 									\ */
+/*   case SYMBOL_REF:							\ */
+/*     return COSTS_N_INSNS (SYMBOL_REF_FLAG (X) ? 1 : 2);			\ */
+/* 									\ */
+/*   case CONST_DOUBLE:							\ */
+/*     {									\ */
+/*       rtx high, low;							\ */
+/*       split_double (X, &high, &low);					\ */
+/*       return COSTS_N_INSNS ((high == CONST0_RTX (GET_MODE (high))	\ */
+/* 			     || low == CONST0_RTX (GET_MODE (low)))	\ */
+/* 			    ? 2 : 4);					\ */
+/*     } */
+
+/* A C expression for the cost of moving data from a register in
+   class FROM to one in class TO.  The classes are expressed using
+   the enumeration values such as `GENERAL_REGS'.  A value of 2 is
+   the default; other values are interpreted relative to that.
+
+   It is not required that the cost always equal 2 when FROM is the
+   same as TO; on some machines it is expensive to move between
+   registers if they are not general registers.
+
+   If reload sees an insn consisting of a single `set' between two
+   hard registers, and if `REGISTER_MOVE_COST' applied to their
+   classes returns a value of 2, reload does not check to ensure
+   that the constraints of the insn are met.  Setting a cost of
+   other than 2 will allow reload to verify that the constraints are
+   met.  You should do this if the `movM' pattern's constraints do
+   not allow such copying. */
+
+/* SID : GCC move Initial definition was : REGISTER_MOVE_COST(FROM, TO) */
+/* Mode should alwasy be SImode */
+#define REGISTER_MOVE_COST(MODE, FROM, TO)	\
+  ( GR_REG_CLASS_P (FROM) && GR_REG_CLASS_P (TO) ? 2 \
+   : (FROM) == FP_REGS && (TO) == FP_REGS ? 2				\
+   : GR_REG_CLASS_P (FROM) && (TO) == FP_REGS ? 4			\
+   : (FROM) == FP_REGS && GR_REG_CLASS_P (TO) ? 4			\
+   : (((FROM) == HI_REG || (FROM) == LO_REG				\
+       || (FROM) == MD_REGS || (FROM) == HILO_REG)			\
+      && GR_REG_CLASS_P (TO)) ? 6		\
+   : (((TO) == HI_REG || (TO) == LO_REG					\
+       || (TO) == MD_REGS || (TO) == HILO_REG)				\
+      && GR_REG_CLASS_P (FROM)) ? 6		\
+   : (FROM) == ST_REGS && GR_REG_CLASS_P (TO) ? 4			\
+   : (FROM) == FP_REGS && (TO) == ST_REGS ? 8				\
+   : 12)
+
+#define MEMORY_MOVE_COST(MODE,CLASS,TO_P) \
+  (4 + memory_move_secondary_cost ((MODE), (CLASS), (TO_P)))
+
+/* Define if copies to/from condition code registers should be avoided.
+
+This is needed for the MICROBLAZE because reload_outcc is not complete;
+it needs to handle cases where the source is a general or another
+condition code register.  */
+#define AVOID_CCMODE_COPIES
+
+/* A C expression for the cost of a branch instruction.  A value of
+   1 is the default; other values are interpreted relative to that.  */
+
+#define BRANCH_COST   2
+
+/* A C statement (sans semicolon) to update the integer variable COST
+   based on the relationship between INSN that is dependent on
+   DEP_INSN through the dependence LINK.  The default is to make no
+   adjustment to COST.  On the MICROBLAZE, ignore the cost of anti- and
+   output-dependencies.  */
+
+#define ADJUST_COST(INSN,LINK,DEP_INSN,COST)				\
+  if (REG_NOTE_KIND (LINK) != 0)					\
+    (COST) = 0; /* Anti or output dependence.  */
+
+/* Optionally define this if you have added predicates to
+   `MACHINE.c'.  This macro is called within an initializer of an
+   array of structures.  The first field in the structure is the
+   name of a predicate and the second field is an array of rtl
+   codes.  For each predicate, list all rtl codes that can be in
+   expressions matched by the predicate.  The list should have a
+   trailing comma.  Here is an example of two entries in the list
+   for a typical RISC machine:
+
+   #define PREDICATE_CODES \
+   {"gen_reg_rtx_operand", {SUBREG, REG}},  \
+   {"reg_or_short_cint_operand", {SUBREG, REG, CONST_INT}},
+
+   Defining this macro does not affect the generated code (however,
+   incorrect definitions that omit an rtl code that may be matched
+   by the predicate can cause the compiler to malfunction). 
+   Instead, it allows the table built by `genrecog' to be more
+   compact and efficient, thus speeding up the compiler.  The most
+   important predicates to include in the list specified by this
+   macro are thoses used in the most insn patterns.  */
+
+#define PREDICATE_CODES							\
+  {"arith_operand",		{ REG, CONST_INT, SUBREG }},		\
+  {"reg_or_0_operand",		{ REG, CONST_INT, CONST_DOUBLE, SUBREG }}, \
+  {"small_int",			{ CONST_INT }},				\
+  {"immediate32_operand",	{ CONST_INT }},				\
+  {"microblaze_const_double_ok",	{ CONST_DOUBLE }},		\
+  {"const_float_1_operand",	{ CONST_DOUBLE }},			\
+  {"simple_memory_operand",	{ MEM, SUBREG }},			\
+  {"imm_required_operand",	{ MEM, SUBREG }},			\
+  {"equality_op",		{ EQ, NE }},				\
+  {"lessthan_op",		{ LT, LTU }},				\
+  {"cmp_op",			{ EQ, NE, GT, GE, GTU, GEU, LT, LE,	\
+				  LTU, LEU }},				\
+  {"signed_cmp_op",             { EQ, NE, GT, GE, LT, LE }},            \
+  {"unsigned_cmp_op",           { GTU, GEU, LTU, LEU }},                \
+  {"pc_or_label_operand",	{ PC, LABEL_REF }},			\
+  {"call_insn_operand",		{ CONST_INT, CONST, SYMBOL_REF, REG}},	\
+  {"move_operand", 		{ CONST_INT, CONST_DOUBLE, CONST,	\
+				  SYMBOL_REF, LABEL_REF, SUBREG,	\
+				  REG, MEM}},				\
+  {"movdi_operand",		{ CONST_INT, CONST_DOUBLE, CONST,	\
+				  SYMBOL_REF, LABEL_REF, SUBREG, REG,	\
+				  MEM, SIGN_EXTEND }},			\
+  {"extend_operator",           { SIGN_EXTEND, ZERO_EXTEND }},          \
+  {"highpart_shift_operator",   { ASHIFTRT, LSHIFTRT, ROTATERT, ROTATE }}, 
+
+
+
+/* If defined, a C statement to be executed just prior to the
+   output of assembler code for INSN, to modify the extracted
+   operands so they will be output differently.
+
+   Here the argument OPVEC is the vector containing the operands
+   extracted from INSN, and NOPERANDS is the number of elements of
+   the vector which contain meaningful data for this insn.  The
+   contents of this vector are what will be used to convert the
+   insn template into assembler code, so you can change the
+   assembler output by changing the contents of the vector.
+
+   We use it to check if the current insn needs a nop in front of it
+   because of load delays, and also to update the delay slot
+   statistics.  */
+
+#define FINAL_PRESCAN_INSN(INSN, OPVEC, NOPERANDS)			\
+  final_prescan_insn (INSN, OPVEC, NOPERANDS)
+
+
+/* Control the assembler format that we output.  */
+
+/* Output to assembler file text saying following lines
+   may contain character constants, extra white space, comments, etc.  */
+
+#define ASM_APP_ON " #APP\n"
+
+/* Output to assembler file text saying following lines
+   no longer contain unusual constructs.  */
+
+#define ASM_APP_OFF " #NO_APP\n"
+
+/* How to refer to registers in assembler output.
+   This sequence is indexed by compiler's hard-register-number (see above).
+
+   In order to support the two different conventions for register names,
+   we use the name of a table set up in microblaze.c, which is overwritten
+   if -mrnames is used.  */
+
+#define REGISTER_NAMES							\
+{									\
+  &microblaze_reg_names[ 0][0],						\
+  &microblaze_reg_names[ 1][0],						\
+  &microblaze_reg_names[ 2][0],						\
+  &microblaze_reg_names[ 3][0],						\
+  &microblaze_reg_names[ 4][0],						\
+  &microblaze_reg_names[ 5][0],						\
+  &microblaze_reg_names[ 6][0],						\
+  &microblaze_reg_names[ 7][0],						\
+  &microblaze_reg_names[ 8][0],						\
+  &microblaze_reg_names[ 9][0],						\
+  &microblaze_reg_names[10][0],						\
+  &microblaze_reg_names[11][0],						\
+  &microblaze_reg_names[12][0],						\
+  &microblaze_reg_names[13][0],						\
+  &microblaze_reg_names[14][0],						\
+  &microblaze_reg_names[15][0],						\
+  &microblaze_reg_names[16][0],						\
+  &microblaze_reg_names[17][0],						\
+  &microblaze_reg_names[18][0],						\
+  &microblaze_reg_names[19][0],						\
+  &microblaze_reg_names[20][0],						\
+  &microblaze_reg_names[21][0],						\
+  &microblaze_reg_names[22][0],						\
+  &microblaze_reg_names[23][0],						\
+  &microblaze_reg_names[24][0],						\
+  &microblaze_reg_names[25][0],						\
+  &microblaze_reg_names[26][0],						\
+  &microblaze_reg_names[27][0],						\
+  &microblaze_reg_names[28][0],						\
+  &microblaze_reg_names[29][0],						\
+  &microblaze_reg_names[30][0],						\
+  &microblaze_reg_names[31][0],						\
+  &microblaze_reg_names[32][0],						\
+  &microblaze_reg_names[33][0],						\
+  &microblaze_reg_names[34][0],						\
+  &microblaze_reg_names[35][0],						\
+  &microblaze_reg_names[36][0],						\
+  &microblaze_reg_names[37][0],						\
+  &microblaze_reg_names[38][0],						\
+  &microblaze_reg_names[39][0],						\
+  &microblaze_reg_names[40][0],						\
+  &microblaze_reg_names[41][0],						\
+  &microblaze_reg_names[42][0],						\
+  &microblaze_reg_names[43][0],						\
+  &microblaze_reg_names[44][0],						\
+  &microblaze_reg_names[45][0],						\
+  &microblaze_reg_names[46][0],						\
+  &microblaze_reg_names[47][0],						\
+  &microblaze_reg_names[48][0],						\
+  &microblaze_reg_names[49][0],						\
+  &microblaze_reg_names[50][0],						\
+  &microblaze_reg_names[51][0],						\
+  &microblaze_reg_names[52][0],						\
+  &microblaze_reg_names[53][0],						\
+  &microblaze_reg_names[54][0],						\
+  &microblaze_reg_names[55][0],						\
+  &microblaze_reg_names[56][0],						\
+  &microblaze_reg_names[57][0],						\
+  &microblaze_reg_names[58][0],						\
+  &microblaze_reg_names[59][0],						\
+  &microblaze_reg_names[60][0],						\
+  &microblaze_reg_names[61][0],						\
+  &microblaze_reg_names[62][0],						\
+  &microblaze_reg_names[63][0],						\
+  &microblaze_reg_names[64][0],						\
+  &microblaze_reg_names[65][0],						\
+  &microblaze_reg_names[66][0],						\
+  &microblaze_reg_names[67][0],						\
+  &microblaze_reg_names[68][0],						\
+  &microblaze_reg_names[69][0],						\
+  &microblaze_reg_names[70][0],						\
+  &microblaze_reg_names[71][0],						\
+  &microblaze_reg_names[72][0],						\
+  &microblaze_reg_names[73][0],						\
+  &microblaze_reg_names[74][0],						\
+  &microblaze_reg_names[75][0],						\
+  &microblaze_reg_names[76][0],						\
+  &microblaze_reg_names[77][0],						\
+}
+
+/* print-rtl.c can't use REGISTER_NAMES, since it depends on microblaze.c.
+   So define this for it.  */
+#define DEBUG_REGISTER_NAMES						\
+{									\
+  "$0",   "sp", "rogp",   "v0",   "v1",   "a0",   "a1",   "a2",		\
+  "a3",   "a4",   "a5",   "t0",   "t1",   "rwgp", "k0",   "k1",		\
+  "k2",   "k3",   "at",   "s0",   "s1",   "s2",   "s3",   "s4",		\
+  "s5",   "s6",   "s7",   "s8",   "s9",   "s10",  "s11",  "s12",	\
+  "$f0",  "$f1",  "$f2",  "$f3",  "$f4",  "$f5",  "$f6",  "$f7",	\
+  "$f8",  "$f9",  "$f10", "$f11", "$f12", "$f13", "$f14", "$f15",	\
+  "$f16", "$f17", "$f18", "$f19", "$f20", "$f21", "$f22", "$f23",	\
+  "$f24", "$f25", "$f26", "$f27", "$f28", "$f29", "$f30", "$f31",	\
+  "hi",   "lo",   "accum","rmsr","$fcc1","$fcc2","$fcc3","$fcc4",	\
+  "$fcc5","$fcc6","$fcc7","$rap"					\
+}
+
+/* If defined, a C initializer for an array of structures
+   containing a name and a register number.  This macro defines
+   additional names for hard registers, thus allowing the `asm'
+   option in declarations to refer to registers using alternate
+   names.
+
+   We define both names for the integer registers here.  */
+
+#define ADDITIONAL_REGISTER_NAMES					\
+{									\
+  { "r0",	 0 + GP_REG_FIRST },					\
+  { "r1",	 1 + GP_REG_FIRST },					\
+  { "r2",	 2 + GP_REG_FIRST },					\
+  { "r3",	 3 + GP_REG_FIRST },					\
+  { "r4",	 4 + GP_REG_FIRST },					\
+  { "r5",	 5 + GP_REG_FIRST },					\
+  { "r6",	 6 + GP_REG_FIRST },					\
+  { "r7",	 7 + GP_REG_FIRST },					\
+  { "r8",	 8 + GP_REG_FIRST },					\
+  { "r9",	 9 + GP_REG_FIRST },					\
+  { "r10",	10 + GP_REG_FIRST },					\
+  { "r11",	11 + GP_REG_FIRST },					\
+  { "r12",	12 + GP_REG_FIRST },					\
+  { "r13",	13 + GP_REG_FIRST },					\
+  { "r14",	14 + GP_REG_FIRST },					\
+  { "r15",	15 + GP_REG_FIRST },					\
+  { "r16",	16 + GP_REG_FIRST },					\
+  { "r17",	17 + GP_REG_FIRST },					\
+  { "r18",	18 + GP_REG_FIRST },					\
+  { "r19",	19 + GP_REG_FIRST },					\
+  { "r20",	20 + GP_REG_FIRST },					\
+  { "r21",	21 + GP_REG_FIRST },					\
+  { "r22",	22 + GP_REG_FIRST },					\
+  { "r23",	23 + GP_REG_FIRST },					\
+  { "r24",	24 + GP_REG_FIRST },					\
+  { "r25",	25 + GP_REG_FIRST },					\
+  { "r26",	26 + GP_REG_FIRST },					\
+  { "r27",	27 + GP_REG_FIRST },					\
+  { "r28",	28 + GP_REG_FIRST },					\
+  { "r29",	29 + GP_REG_FIRST },					\
+  { "r30",	30 + GP_REG_FIRST },					\
+  { "r31",	31 + GP_REG_FIRST },					\
+  { "rsp",	MB_ABI_STACK_POINTER_REGNUM + GP_REG_FIRST },					\
+  { "rfp",	MB_ABI_FRAME_POINTER_REGNUM + GP_REG_FIRST },					\
+  { "at",	 MB_ABI_ASM_TEMP_REGNUM + GP_REG_FIRST },					\
+  { "rmsr",     ST_REG_FIRST}, \
+  { "rogp",	 2 + GP_REG_FIRST },					\
+  { "v0",	 3 + GP_REG_FIRST },					\
+  { "v1",	 4 + GP_REG_FIRST },					\
+  { "a0",	 5 + GP_REG_FIRST },					\
+  { "a1",	 6 + GP_REG_FIRST },					\
+  { "a2",	 7 + GP_REG_FIRST },					\
+  { "t3",	 8 + GP_REG_FIRST },					\
+  { "t4",	 9 + GP_REG_FIRST },					\
+  { "t5",	10 + GP_REG_FIRST },					\
+  { "t0",	11 + GP_REG_FIRST },					\
+  { "t1",	12 + GP_REG_FIRST },					\
+  { "rwgp",	13 + GP_REG_FIRST },					\
+  { "k0",	14 + GP_REG_FIRST },					\
+  { "k1",	15 + GP_REG_FIRST },					\
+  { "k2",	16 + GP_REG_FIRST },					\
+  { "k3",	17 + GP_REG_FIRST },					\
+  { "at",	18 + GP_REG_FIRST },					\
+  { "$fp",	19 + GP_REG_FIRST },					\
+  { "s0",	20 + GP_REG_FIRST },					\
+  { "s1",	21 + GP_REG_FIRST },					\
+  { "s2",	22 + GP_REG_FIRST },					\
+  { "s3",	23 + GP_REG_FIRST },					\
+  { "s4",	24 + GP_REG_FIRST },					\
+  { "s5",	25 + GP_REG_FIRST },					\
+  { "s6",	26 + GP_REG_FIRST },					\
+  { "s7",	27 + GP_REG_FIRST },					\
+  { "s8",	28 + GP_REG_FIRST },					\
+  { "s9",	29 + GP_REG_FIRST },					\
+  { "s10",	30 + GP_REG_FIRST },					\
+  { "s11",	31 + GP_REG_FIRST },					\
+  { "s12",	29 + GP_REG_FIRST },					\
+  { "s13",	30 + GP_REG_FIRST }					\
+}
+
+/* Define results of standard character escape sequences.  */
+/* Removed since defined in Default */
+/*
+  #define TARGET_BELL	007
+  #define TARGET_BS	010
+  #define TARGET_TAB	011
+  #define TARGET_NEWLINE	012
+  #define TARGET_VT	013
+  #define TARGET_FF	014
+  #define TARGET_CR	015
+*/
+
+/* A C compound statement to output to stdio stream STREAM the
+   assembler syntax for an instruction operand X.  X is an RTL
+   expression.
+
+   CODE is a value that can be used to specify one of several ways
+   of printing the operand.  It is used when identical operands
+   must be printed differently depending on the context.  CODE
+   comes from the `%' specification that was used to request
+   printing of the operand.  If the specification was just `%DIGIT'
+   then CODE is 0; if the specification was `%LTR DIGIT' then CODE
+   is the ASCII code for LTR.
+
+   If X is a register, this macro should print the register's name.
+   The names can be found in an array `reg_names' whose type is
+   `char *[]'.  `reg_names' is initialized from `REGISTER_NAMES'.
+
+   When the machine description has a specification `%PUNCT' (a `%'
+   followed by a punctuation character), this macro is called with
+   a null pointer for X and the punctuation character for CODE.
+
+   See microblaze.c for the MICROBLAZE specific codes.  */
+
+#define PRINT_OPERAND(FILE, X, CODE) print_operand (FILE, X, CODE)
+
+/* A C expression which evaluates to true if CODE is a valid
+   punctuation character for use in the `PRINT_OPERAND' macro.  If
+   `PRINT_OPERAND_PUNCT_VALID_P' is not defined, it means that no
+   punctuation characters (except for the standard one, `%') are
+   used in this way.  */
+
+#define PRINT_OPERAND_PUNCT_VALID_P(CODE) microblaze_print_operand_punct[CODE]
+
+/* A C compound statement to output to stdio stream STREAM the
+   assembler syntax for an instruction operand that is a memory
+   reference whose address is ADDR.  ADDR is an RTL expression.
+
+   On some machines, the syntax for a symbolic address depends on
+   the section that the address refers to.  On these machines,
+   define the macro `ENCODE_SECTION_INFO' to store the information
+   into the `symbol_ref', and then check for it here.  */
+
+#define PRINT_OPERAND_ADDRESS(FILE, ADDR) print_operand_address (FILE, ADDR)
+
+
+/* A C statement, to be executed after all slot-filler instructions
+   have been output.  If necessary, call `dbr_sequence_length' to
+   determine the number of slots filled in a sequence (zero if not
+   currently outputting a sequence), to decide how many no-ops to
+   output, or whatever.
+
+   Don't define this macro if it has nothing to do, but it is
+   helpful in reading assembly output if the extent of the delay
+   sequence is made explicit (e.g. with white space).
+
+   Note that output routines for instructions with delay slots must
+   be prepared to deal with not being output as part of a sequence
+   (i.e.  when the scheduling pass is not run, or when no slot
+   fillers could be found.)  The variable `final_sequence' is null
+   when not processing a sequence, otherwise it contains the
+   `sequence' rtx being output.  */
+
+#define DBR_OUTPUT_SEQEND(STREAM)					\
+do									\
+  {									\
+    if (set_nomacro > 0 && --set_nomacro == 0)				\
+      {}								\
+									\
+    if (set_noreorder > 0 && --set_noreorder == 0)			\
+      {}								\
+									\
+    dslots_jump_filled++;						\
+    fputs ("\n", STREAM);						\
+  }									\
+while (0)
+
+
+/* How to tell the debugger about changes of source files.  Note, the
+   microblaze ECOFF format cannot deal with changes of files inside of
+   functions, which means the output of parser generators like bison
+   is generally not debuggable without using the -l switch.  Lose,
+   lose, lose.  Silicon graphics seems to want all .file's hardwired
+   to 1.  */
+
+#ifndef SET_FILE_NUMBER
+#define SET_FILE_NUMBER() ++num_source_filenames
+#endif
+
+#define ASM_OUTPUT_SOURCE_FILENAME(STREAM, NAME)			\
+  microblaze_output_filename (STREAM, NAME)
+
+#define ASM_OUTPUT_FILENAME(STREAM, NUM_SOURCE_FILENAMES, NAME) \
+do								\
+  {								\
+    fprintf (STREAM, "\t.file\t%d ", NUM_SOURCE_FILENAMES);	\
+    output_quoted_string (STREAM, NAME);			\
+    fputs ("\n", STREAM);					\
+  }								\
+while (0)
+
+/* This is how to output a note the debugger telling it the line number
+   to which the following sequence of instructions corresponds.
+   Silicon graphics puts a label after each .loc.  */
+
+#ifndef LABEL_AFTER_LOC
+#define LABEL_AFTER_LOC(STREAM)
+#endif
+
+#define ASM_OUTPUT_SOURCE_LINE(STREAM, LINE, COUNTER)		\
+  microblaze_output_lineno (STREAM, LINE)
+
+/* The MICROBLAZE implementation uses some labels for its own purpose.  The
+   following lists what labels are created, and are all formed by the
+   pattern $L[a-z].*.  The machine independent portion of GCC creates
+   labels matching:  $L[A-Z][0-9]+ and $L[0-9]+.
+
+   LM[0-9]+	Silicon Graphics/ECOFF stabs label before each stmt.
+   $Lb[0-9]+	Begin blocks for MICROBLAZE debug support
+   $Lc[0-9]+	Label for use in s<xx> operation.
+   $Le[0-9]+	End blocks for MICROBLAZE debug support
+   $Lp\..+		Half-pic labels. */
+
+/* This is how to output the definition of a user-level label named NAME,
+   such as the label on a static function or variable NAME.
+
+   If we are optimizing the gp, remember that this label has been put
+   out, so we know not to emit an .extern for it in microblaze_asm_file_end.
+   We use one of the common bits in the IDENTIFIER tree node for this,
+   since those bits seem to be unused, and we don't have any method
+   of getting the decl nodes from the name.  */
+
+#define ASM_OUTPUT_LABEL(STREAM,NAME)					\
+do {									\
+  assemble_name (STREAM, NAME);						\
+  fputs (":\n", STREAM);						\
+} while (0)
+
+
+/* This says how to define an aligned common symbol */
+#define ASM_OUTPUT_ALIGNED_COMMON(STREAM, NAME, SIZE, ALIGNMENT)   \
+    (microblaze_declare_comm_object (STREAM, NAME, "\n\t.comm\t", ",%u,%u\n", (SIZE), (ALIGNMENT)))
+
+/* This says how to define an aligned static common symbol */
+#define ASM_OUTPUT_ALIGNED_LOCAL(STREAM, NAME, SIZE, ALIGNMENT)     \
+    (microblaze_declare_comm_object (STREAM, NAME, "\n\t.lcomm\t", ",%u,%u\n", (SIZE), (ALIGNMENT)))
+
+/* This says how to output an aligned BSS symbol */
+#define ASM_OUTPUT_ALIGNED_BSS(STREAM, DECL, NAME, SIZE, ALIGNMENT) \
+    asm_output_aligned_bss (STREAM, DECL, NAME, SIZE, ALIGNMENT)
+
+/* This says how to output an external.  It would be possible not to
+   output anything and let undefined symbol become external. However
+   the assembler uses length information on externals to allocate in
+   data/sdata bss/sbss, thereby saving exec time.  */
+
+#define ASM_OUTPUT_EXTERNAL(STREAM,DECL,NAME) \
+  microblaze_output_external(STREAM,DECL,NAME)
+
+/* This is how to declare a function name.  The actual work of
+   emitting the label is moved to function_prologue, so that we can
+   get the line number correctly emitted before the .ent directive,
+   and after any .file directives.
+
+   Also, switch files if we are optimizing the global pointer.  */
+
+/* Older FUNCTION_NAME macro. has been replaced by a new one for the
+   .size issue */
+#if 0 
+#define ASM_DECLARE_FUNCTION_NAME(STREAM,NAME,DECL)			\
+{									\
+  extern FILE *asm_out_text_file;					\
+  if (TARGET_GP_OPT )					\
+    {									\
+      STREAM = asm_out_text_file;					\
+      /* ??? text_section gets called too soon.  If the previous	\
+	 function is in a special section and we're not, we have	\
+	 to switch back to the text section.  We can't call		\
+	 text_section again as gcc thinks we're already there.  */	\
+      /* ??? See varasm.c.  There are other things that get output	\
+	 too early, like alignment (before we've switched STREAM).  */	\
+      if (DECL_SECTION_NAME (DECL) == NULL_TREE)			\
+	fprintf (STREAM, "%s\n", TEXT_SECTION_ASM_OP);			\
+    }									\
+									\
+  HALF_PIC_DECLARE (NAME);						\
+}
+#endif
+
+#define ASM_DECLARE_FUNCTION_NAME(STREAM,NAME,DECL)                         \
+{                                                                           \
+}
+
+
+/* This is how to store into the string LABEL
+   the symbol_ref name of an internal numbered label where
+   PREFIX is the class of label and NUM is the number within the class.
+   This is suitable for output with `assemble_name'.  */
+
+#define ASM_GENERATE_INTERNAL_LABEL(LABEL,PREFIX,NUM)			\
+  sprintf ((LABEL), "*%s%s%ld", (LOCAL_LABEL_PREFIX), (PREFIX), (long)(NUM))
+
+/* This is how to output an assembler line defining a `double' constant.  */
+
+#define ASM_OUTPUT_DOUBLE(STREAM,VALUE)					\
+  microblaze_output_double (STREAM, VALUE)
+
+
+/* This is how to output an assembler line defining a `float' constant.  */
+
+#define ASM_OUTPUT_FLOAT(STREAM,VALUE)					\
+  microblaze_output_float (STREAM, VALUE)
+
+
+/* This is how to output an assembler line defining an `int' constant.  */
+
+#define ASM_OUTPUT_INT(STREAM,VALUE)					\
+do {									\
+  fprintf (STREAM, "\t.data32\t");					\
+  output_addr_const (STREAM, (VALUE));					\
+  fprintf (STREAM, "\n");						\
+} while (0)
+
+/* Likewise for 64 bit, `char' and `short' constants.  */
+
+#define ASM_OUTPUT_DOUBLE_INT(STREAM,VALUE)				\
+do {									\
+      assemble_integer (operand_subword ((VALUE), 0, 0, DImode),	\
+			UNITS_PER_WORD, 1);				\
+      assemble_integer (operand_subword ((VALUE), 1, 0, DImode),	\
+			UNITS_PER_WORD, 1);				\
+} while (0)
+
+#define ASM_OUTPUT_SHORT(STREAM,VALUE)					\
+{									\
+  fprintf (STREAM, "\t.data16\t");					\
+  output_addr_const (STREAM, (VALUE));					\
+  fprintf (STREAM, "\n");						\
+}
+
+#define ASM_OUTPUT_CHAR(STREAM,VALUE)					\
+{									\
+  fprintf (STREAM, "\t.data8\t");					\
+  output_addr_const (STREAM, (VALUE));					\
+  fprintf (STREAM, "\n");						\
+}
+
+/* This is how to output an assembler line for a numeric constant byte.  */
+
+#define ASM_OUTPUT_BYTE(STREAM,VALUE)					\
+  fprintf (STREAM, "\t.data8\t0x%x\n", (VALUE))
+
+/* This is how to output an element of a case-vector that is absolute.  */
+
+/*   Changed .word to .gpword*/
+#define ASM_OUTPUT_ADDR_VEC_ELT(STREAM, VALUE)				\
+  fprintf (STREAM, "\t%s\t%sL%d\n",					\
+	   ".gpword",                                                   \
+	   LOCAL_LABEL_PREFIX, VALUE)
+
+/*   Changed .word to .gpword*/
+
+/* This is how to output an element of a case-vector that is relative.
+   This is used for pc-relative code (e.g. when TARGET_ABICALLS or
+   TARGET_EMBEDDED_PIC).  */
+
+#define ASM_OUTPUT_ADDR_DIFF_ELT(STREAM, BODY, VALUE, REL)		\
+do {									\
+  if (TARGET_EMBEDDED_PIC)						\
+    fprintf (STREAM, "\t%s\t%sL%d-%sLS%d\n",				\
+	     ".gpword",                                                 \
+	     LOCAL_LABEL_PREFIX, VALUE, LOCAL_LABEL_PREFIX, REL);	\
+  else                                                                  \
+    fprintf (STREAM, "\t%s\t%sL%d\n",					\
+	     ".gpword",                                                 \
+	     LOCAL_LABEL_PREFIX, VALUE);				\
+} while (0)
+
+/* When generating embedded PIC code we want to put the jump
+   table in the .text section.  In all other cases, we want to put the
+   jump table in the .rdata section.  Unfortunately, we can't use
+   JUMP_TABLES_IN_TEXT_SECTION, because it is not conditional.
+   Instead, we use ASM_OUTPUT_CASE_LABEL to switch back to the .text
+   section if appropriate.  */
+#define ASM_OUTPUT_CASE_LABEL(FILE, PREFIX, NUM, INSN)			\
+do {									\
+  if (TARGET_EMBEDDED_PIC )                                             \
+    function_section (current_function_decl);				\
+  (*targetm.asm_out.internal_label) (FILE, PREFIX, NUM);		\
+} while (0)
+
+/* This is how to output an assembler line
+   that says to advance the location counter
+   to a multiple of 2**LOG bytes.  */
+
+#define ASM_OUTPUT_ALIGN(STREAM,LOG)					\
+  fprintf (STREAM, "\t.align\t%d\n", (LOG))
+
+/* This is how to output an assembler line to advance the location
+   counter by SIZE bytes.  */
+
+#define ASM_OUTPUT_SKIP(STREAM,SIZE)					\
+  fprintf (STREAM, "\t.space\t%u\n", (SIZE))
+
+/* This is how to output a string.  */
+/*   Changed the way the string is to be printed in the assembly file. It
+     will output .data8 words instead of .ascii and the string
+*/
+#define ASM_OUTPUT_ASCII(STREAM, STRING, LEN)				\
+do {									\
+if(TARGET_MICROBLAZE_ASM){                                                    \
+  register int i, c, len = (LEN) ;  				        \
+  register unsigned char *string = (unsigned char *)(STRING);		\
+  for (i = 0; i < len; i++)						\
+       fprintf((STREAM),".data8 %d\n",string[i]);			\
+  }                                                                     \
+ else /* If not MICROBLAZE_ASM print ascii */                                 \
+  {/* Original code for ASM_OUTPUT_ASCII */                             \
+  register int i, c, len = (LEN), cur_pos = 17;				\
+  register unsigned char *string = (unsigned char *)(STRING);		\
+  fprintf ((STREAM), "\t.ascii\t\"");					\
+  for (i = 0; i < len; i++)						\
+    {									\
+      register int c = string[i];					\
+									\
+      switch (c)							\
+	{								\
+	case '\"':							\
+	case '\\':							\
+	  putc ('\\', (STREAM));					\
+	  putc (c, (STREAM));						\
+	  cur_pos += 2;							\
+	  break;							\
+									\
+	case TARGET_NEWLINE:						\
+	  fputs ("\\n", (STREAM));					\
+	  if (i+1 < len							\
+	      && (((c = string[i+1]) >= '\040' && c <= '~')		\
+		  || c == TARGET_TAB))					\
+	    cur_pos = 32767;		/* break right here */		\
+	  else								\
+	    cur_pos += 2;						\
+	  break;							\
+									\
+	case TARGET_TAB:						\
+	  fputs ("\\t", (STREAM));					\
+	  cur_pos += 2;							\
+	  break;							\
+									\
+	case TARGET_FF:							\
+	  fputs ("\\f", (STREAM));					\
+	  cur_pos += 2;							\
+	  break;							\
+									\
+	case TARGET_BS:							\
+	  fputs ("\\b", (STREAM));					\
+	  cur_pos += 2;							\
+	  break;							\
+									\
+	case TARGET_CR:							\
+	  fputs ("\\r", (STREAM));					\
+	  cur_pos += 2;							\
+	  break;							\
+									\
+	default:							\
+	  if (c >= ' ' && c < 0177)					\
+	    {								\
+	      putc (c, (STREAM));					\
+	      cur_pos++;						\
+	    }								\
+	  else								\
+	    {								\
+	      fprintf ((STREAM), "\\%03o", c);				\
+	      cur_pos += 4;						\
+	    }								\
+	}								\
+									\
+      if (cur_pos > 72 && i+1 < len)					\
+	{								\
+	  cur_pos = 17;							\
+	  fprintf ((STREAM), "\"\n\t.ascii\t\"");			\
+	}								\
+    }									\
+   fprintf ((STREAM), "\"\n");						\
+  } 								        \
+} while (0)
+
+/* Handle certain cpp directives used in header files on sysV.  */
+/* GCC 3.4.1 - Poisoned */
+/* #define SCCS_DIRECTIVE */
+
+/* Output #ident as a in the read-only data section.  */
+#define ASM_OUTPUT_IDENT(FILE, STRING)					\
+{									\
+  char *p = STRING;							\
+  int size = strlen (p) + 1;						\
+  if(size <= microblaze_section_threshold)				\
+     sdata2_section ();							\
+  else 									\
+     rodata_section ();							\
+  assemble_string (p, size);						\
+}
+
+/* Default to -G 8 */
+
+
+#ifndef MICROBLAZE_DEFAULT_GVALUE
+#define MICROBLAZE_DEFAULT_GVALUE 8
+#endif
+
+/* Given a decl node or constant node, choose the section to output it in
+   and select that section.  */
+
+/* Store in OUTPUT a string (made with alloca) containing
+   an assembler-name for a local static variable named NAME.
+   LABELNO is an integer which is different for each call.  */
+
+#define ASM_FORMAT_PRIVATE_NAME(OUTPUT, NAME, LABELNO)			\
+( (OUTPUT) = (char *) alloca (strlen ((NAME)) + 10),			\
+  sprintf ((OUTPUT), "%s.%d", (NAME), (LABELNO)))
+
+/* How to start an assembler comment.
+   The leading space is important (the microblaze native assembler requires it).  */
+
+#ifndef ASM_COMMENT_START
+#define ASM_COMMENT_START " #"
+#endif
+
+
+
+/* Macros for microblaze-tfile.c to encapsulate stabs in ECOFF, and for
+   and microblaze-tdump.c to print them out.
+
+   These must match the corresponding definitions in gdb/microblazeread.c.
+   Unfortunately, gcc and gdb do not currently share any directories. */
+
+#define CODE_MASK 0x8F300
+#define MICROBLAZE_IS_STAB(sym) (((sym)->index & 0xFFF00) == CODE_MASK)
+#define MICROBLAZE_MARK_STAB(code) ((code)+CODE_MASK)
+#define MICROBLAZE_UNMARK_STAB(code) ((code)-CODE_MASK)
+
+#undef TARGET_FP_CALL_32
+
+#define BSS_VAR         1
+#define SBSS_VAR        2
+#define DATA_VAR        4
+#define SDATA_VAR       5
+#define RODATA_VAR      6
+#define SDATA2_VAR      7
+
+/* These definitions are used in combination with the shift_type flag in the rtl */
+
+#define SHIFT_CONST     1
+#define SHIFT_REG       2
+#define USE_ADDK        3
+
+/* handle interrupt attribute */
+
+/*extern int microblaze_valid_machine_decl_attribute ();*/
+/*
+  #define VALID_MACHINE_DECL_ATTRIBUTE(DECL, ATTRIBUTES, IDENTIFIER, ARGS) \
+  microblaze_valid_machine_decl_attribute (DECL, ATTRIBUTES, IDENTIFIER, ARGS)
+*/
+extern int interrupt_handler;
+extern int save_volatiles;
+
+/*int microblaze_is_interrupt_handler();*/
+
+/*extern int dbl_register_operand();*/
+/*extern int shiftdone;*/
+
+#define INTERRUPT_HANDLER_NAME "_interrupt_handler"
+
+/* these #define added for C++ */
+#ifdef OBJECT_FORMAT_ELF
+#define UNALIGNED_SHORT_ASM_OP          ".data16"
+#define UNALIGNED_INT_ASM_OP            ".data32"
+#define UNALIGNED_DOUBLE_INT_ASM_OP     ".data8"
+#endif /* OBJECT_FORMAT_ELF */
+
+#define ASM_BYTE_OP                     ".data8"
+
+/* the following #defines are used in the headers files. Always retain these */
+
+/* Added for declaring size at the end of the function*/
+#undef ASM_DECLARE_FUNCTION_SIZE
+#define ASM_DECLARE_FUNCTION_SIZE(FILE, FNAME, DECL)			\
+  do {									\
+    if (!flag_inhibit_size_directive)					\
+      {									\
+        char label[256];						\
+	static int labelno;						\
+	labelno++;							\
+	ASM_GENERATE_INTERNAL_LABEL (label, "Lfe", labelno);		\
+        (*targetm.asm_out.internal_label) (FILE, "Lfe", labelno);	\
+	fprintf (FILE, "%s", SIZE_ASM_OP);				\
+	assemble_name (FILE, (FNAME));					\
+        fprintf (FILE, ",");						\
+	assemble_name (FILE, label);					\
+        fprintf (FILE, "-");						\
+	assemble_name (FILE, (FNAME));					\
+	putc ('\n', FILE);						\
+      }									\
+  } while (0)
+
+/*  ASM_DECLARE_FUNCTION_NAME (asm_out_file, fnname, current_function_decl);*/
+#if 0 
+#define ASM_DECLARE_FUNCTION_NAME(FILE,NAME,DECL)		\
+do{ \
+   if (!flag_inhibit_size_directive) \
+   { \
+      fputs ("\t.ent\t", FILE); \
+      if (interrupt_handler && strcmp(INTERRUPT_HANDLER_NAME,NAME)) \
+         fputs("_interrupt_handler",FILE);\
+      else\
+         assemble_name (FILE, NAME);\
+      fputs ("\n", FILE);\
+   }\
+\
+   assemble_name (FILE, NAME);\
+   fputs (":\n", FILE);\
+   if (interrupt_handler && strcmp(INTERRUPT_HANDLER_NAME,NAME))\
+      fputs ("_interrupt_handler:\n",FILE);\
+   \
+} while(0) 
+#endif
+
+#define GLOBAL_ASM_OP "\t.globl\t"
+
+#define MAX_OFILE_ALIGNMENT (32768*8)
+
+/* A C statement to output something to the assembler file to switch to section
+   NAME for object DECL which is either a FUNCTION_DECL, a VAR_DECL or
+   NULL_TREE.  Some target formats do not support arbitrary sections.  Do not
+   define this macro in such cases.  */
+
+#define ASM_OUTPUT_SECTION_NAME(F, DECL, NAME, RELOC) \
+do {								\
+  extern FILE *asm_out_text_file;				\
+  if ((DECL) && TREE_CODE (DECL) == FUNCTION_DECL)		\
+    fprintf (asm_out_text_file, "\t.section %s,\"ax\",@progbits\n", (NAME)); \
+  else if ((DECL) && DECL_READONLY_SECTION (DECL, RELOC))	\
+    fprintf (F, "\t.section %s,\"a\",@progbits\n", (NAME));	\
+  else								\
+    fprintf (F, "\t.section %s,\"aw\",@progbits\n", (NAME));	\
+} while (0)
+
+/* The following macro defines the format used to output the second
+   operand of the .type assembler directive.  Different svr4 assemblers
+   expect various different forms for this operand.  The one given here
+   is just a default.  You may need to override it in your machine-
+   specific tm.h file (depending upon the particulars of your assembler).  */
+
+#define TYPE_OPERAND_FMT        "@%s"
+
+/* Define the strings used for the special svr4 .type and .size directives.
+   These strings generally do not vary from one system running svr4 to
+   another, but if a given system (e.g. m88k running svr) needs to use
+   different pseudo-op names for these, they may be overridden in the
+   file which includes this one.  */
+
+#undef TYPE_ASM_OP
+#undef SIZE_ASM_OP
+#define TYPE_ASM_OP	"\t.type\t"
+#define SIZE_ASM_OP	"\t.size\t"
+
+/* These macros generate the special .type and .size directives which
+   are used to set the corresponding fields of the linker symbol table
+   entries in an ELF object file under SVR4.  These macros also output
+   the starting labels for the relevant functions/objects.  */
+
+/* Write the extra assembler code needed to declare an object properly.  */
+
+#undef ASM_DECLARE_OBJECT_NAME
+#define ASM_DECLARE_OBJECT_NAME(FILE, NAME, DECL)			\
+  do {									\
+    fprintf (FILE, "%s", TYPE_ASM_OP);			         	\
+    assemble_name (FILE, NAME);						\
+    putc (',', FILE);							\
+    fprintf (FILE, TYPE_OPERAND_FMT, "object");				\
+    putc ('\n', FILE);							\
+    size_directive_output = 0;						\
+    if (!flag_inhibit_size_directive && DECL_SIZE (DECL))		\
+      {									\
+	size_directive_output = 1;					\
+	fprintf (FILE, "%s", SIZE_ASM_OP);				\
+	assemble_name (FILE, NAME);					\
+	fprintf (FILE, ",%d\n",  int_size_in_bytes (TREE_TYPE (DECL)));	\
+      }									\
+    microblaze_declare_object (FILE, NAME, "", ":\n", 0);			\
+  } while (0)
+
+/* Output the size directive for a decl in rest_of_decl_compilation
+   in the case where we did not do so before the initializer.
+   Once we find the error_mark_node, we know that the value of
+   size_directive_output was set
+   by ASM_DECLARE_OBJECT_NAME when it was run for the same decl.  */
+
+#undef ASM_FINISH_DECLARE_OBJECT
+#define ASM_FINISH_DECLARE_OBJECT(FILE, DECL, TOP_LEVEL, AT_END)	 \
+do {									 \
+     char *name = XSTR (XEXP (DECL_RTL (DECL), 0), 0);			 \
+     if (!flag_inhibit_size_directive && DECL_SIZE (DECL)		 \
+         && ! AT_END && TOP_LEVEL					 \
+	 && DECL_INITIAL (DECL) == error_mark_node			 \
+	 && !size_directive_output)					 \
+       {								 \
+	 size_directive_output = 1;					 \
+	 fprintf (FILE, "%s", SIZE_ASM_OP);			         \
+	 assemble_name (FILE, name);					 \
+	 fprintf (FILE, ",%d\n", int_size_in_bytes (TREE_TYPE (DECL)));  \
+       }								 \
+   } while (0)
+
+#define ASM_OUTPUT_DEF(FILE,LABEL1,LABEL2)                            \
+ do { fputc ( '\t', FILE);                                            \
+      assemble_name (FILE, LABEL1);                                   \
+      fputs ( " = ", FILE);                                           \
+      assemble_name (FILE, LABEL2);                                   \
+      fputc ( '\n', FILE);                                            \
+ } while (0)
+
+
+/* No support for weak in MicroBlaze. Hence commenting out this part */
+
+#define ASM_WEAKEN_LABEL(FILE,NAME) ASM_OUTPUT_WEAK_ALIAS(FILE,NAME,0)
+#define ASM_OUTPUT_WEAK_ALIAS(FILE,NAME,VALUE)	\
+  do {						\
+  if (TARGET_GAS)                               \
+  fputs ("\t.weak\t", FILE);		\
+  else                                          \
+  fputs ("\t.weakext\t", FILE);		\
+  assemble_name (FILE, NAME);			\
+  if (VALUE)					\
+  {						\
+  fputc (' ', FILE);			\
+  assemble_name (FILE, VALUE);		\
+  }						\
+  fputc ('\n', FILE);				\
+  } while (0)
+
+
+#define MAKE_DECL_ONE_ONLY(DECL) (DECL_WEAK (DECL) = 1)
+#undef UNIQUE_SECTION_P
+#define UNIQUE_SECTION_P(DECL) (DECL_ONE_ONLY (DECL))
+
+#undef TARGET_ASM_NAMED_SECTION
+#define TARGET_ASM_NAMED_SECTION        default_elf_asm_named_section
+
+#undef TARGET_UNIQUE_SECTION
+#define TARGET_UNIQUE_SECTION microblaze_unique_section
+
+/* Define the strings to put out for each section in the object file.  
+   
+   Note: For ctors/dtors, we want to give these sections the SHF_WRITE attribute 
+   to allow shared libraries to patch/resolve addresses into these locations.
+   On Microblaze, there is no concept of shared libraries yet, so this is for
+   future use. */
+#define TEXT_SECTION_ASM_OP	"\t.text"	
+#define DATA_SECTION_ASM_OP	"\t.data"	
+#define READONLY_DATA_SECTION_ASM_OP    \
+                                "\t.rodata"
+#define BSS_SECTION_ASM_OP      "\t.bss"        
+#define CTORS_SECTION_ASM_OP    "\t.section\t.ctors,\"aw\""
+#define DTORS_SECTION_ASM_OP    "\t.section\t.dtors,\"aw\""
+#define INIT_SECTION_ASM_OP     "\t.section\t.init,\"ax\""
+#define FINI_SECTION_ASM_OP     "\t.section\t.fini,\"ax\""
+
+#define SDATA_SECTION_ASM_OP	"\t.sdata"	/* Small RW initialized data   */
+#define SDATA2_SECTION_ASM_OP	"\t.sdata2"	/* Small RO initialized data   */ 
+#define SBSS_SECTION_ASM_OP     "\t.sbss"       /* Small RW uninitialized data */
+#define SBSS2_SECTION_ASM_OP    "\t.sbss2"      /* Small RO uninitialized data */
+
+#define HOT_TEXT_SECTION_NAME   ".text.hot"
+#define UNLIKELY_EXECUTED_TEXT_SECTION_NAME \
+                                ".text.unlikely"
+
+#define READONLY_DATA_SECTION   rodata_section
+#define SDATA_SECTION           sdata_section
+#define READONLY_SDATA_SECTION  sdata2_section
+#define SBSS_SECTION            sbss_section
+#define READONLY_SBSS_SECTION   sbss2_section
+#define BSS_SECTION             bss_section
+
+/* A list of other sections which the compiler might be "in" at any
+   given time.  */
+#undef EXTRA_SECTIONS
+#define EXTRA_SECTIONS in_rodata, in_sdata, in_sdata2, in_sbss, in_sbss2, in_init, in_fini
+
+#undef EXTRA_SECTION_FUNCTIONS
+#define EXTRA_SECTION_FUNCTIONS                                                     \
+  SECTION_FUNCTION_TEMPLATE(rodata_section, in_rodata, READONLY_DATA_SECTION_ASM_OP)\
+  SECTION_FUNCTION_TEMPLATE(sdata_section,  in_sdata,  SDATA_SECTION_ASM_OP)        \
+  SECTION_FUNCTION_TEMPLATE(sdata2_section, in_sdata2, SDATA2_SECTION_ASM_OP)       \
+  SECTION_FUNCTION_TEMPLATE(sbss_section,   in_sbss,   SBSS_SECTION_ASM_OP)         \
+  SECTION_FUNCTION_TEMPLATE(sbss2_section,  in_sbss2,  SBSS2_SECTION_ASM_OP)        \
+  SECTION_FUNCTION_TEMPLATE(init_section,  in_init,  INIT_SECTION_ASM_OP)           \
+  SECTION_FUNCTION_TEMPLATE(fini_section,  in_fini,  FINI_SECTION_ASM_OP)        
+
+#define SECTION_FUNCTION_TEMPLATE(FN, ENUM, OP)                                 \
+void FN ()                                                                      \
+{                                                                               \
+  if (in_section != ENUM)                                                       \
+    {                                                                           \
+      fprintf (asm_out_file, "%s\n", OP);                                       \
+      in_section = ENUM;                                                        \
+    }                                                                           \
+}
+
+/* We do this to save a few 10s of code space that would be taken up
+   by the call_FUNC () wrappers, used by the generic CRT_CALL_STATIC_FUNCTION
+   definition in crtstuff.c */
+#define CRT_CALL_STATIC_FUNCTION(SECTION_OP, FUNC)	\
+    asm ( SECTION_OP "\n"                               \
+          "\tbralid   r15, " #FUNC "\n\t nop\n"         \
+          TEXT_SECTION_ASM_OP);
+
+/* #define CTOR_LIST_BEGIN                                 \ */
+/* asm (CTORS_SECTION_ASM_OP);                             \ */
+/* func_ptr __CTOR_LIST__[1] = { (func_ptr) (-1) } */
+ 
+/* #define CTOR_LIST_END                                   \ */
+/* asm (CTORS_SECTION_ASM_OP);                             \ */
+/* func_ptr __CTOR_END__[1] = { (func_ptr) 0 }; */
+ 
+/* #define DTOR_LIST_BEGIN                                 \ */
+/* asm (DTORS_SECTION_ASM_OP);                             \ */
+/* func_ptr __DTOR_LIST__[1] = { (func_ptr) (-1) } */
+
+/* #define DTOR_LIST_END                                   \ */
+/* asm (DTORS_SECTION_ASM_OP);                             \ */
+/* func_ptr __DTOR_END__[1] = { (func_ptr) 0 }; */
+
+/* Don't set the target flags, this is done by the linker script */
+#undef LIB_SPEC
+#define LIB_SPEC "%{!pg:%{!nostdlib:%{!Zxl-no-libxil:-( -lxil -lc -lm -) }}} %{pg:%{!nostdlib:-( -lxilprofile -lxil -lc -lm -) }} %{Zxl-no-libxil: %{!nostdlib: -( -lc -lm -) }}"        /* Xilinx: We need to group -lm as well, since some Newlib math functions reference __errno! */
+
+#undef  ENDFILE_SPEC
+#define ENDFILE_SPEC "crtend.o%s crtn.o%s"
+
+
+#define STARTFILE_EXECUTABLE_SPEC   "crt0.o%s crti.o%s crtbegin.o%s"
+#define STARTFILE_XMDSTUB_SPEC      "crt1.o%s crti.o%s crtbegin.o%s"
+#define STARTFILE_BOOTSTRAP_SPEC    "crt2.o%s crti.o%s crtbegin.o%s"
+#define STARTFILE_NOVECTORS_SPEC    "crt3.o%s crti.o%s crtbegin.o%s"
+#define STARTFILE_XILKERNEL_SPEC    "crt4.o%s crti.o%s crtbegin.o%s"
+#define STARTFILE_CRTINIT_SPEC      "%{!pg: %{!mno-clearbss: crtinit.o%s} %{mno-clearbss: sim-crtinit.o%s}} \
+                                     %{pg: %{!mno-clearbss: pgcrtinit.o%s} %{mno-clearbss: sim-pgcrtinit.o%s}}"
+
+
+#define STARTFILE_DEFAULT_SPEC      STARTFILE_EXECUTABLE_SPEC
+
+
+#undef SUBTARGET_EXTRA_SPECS
+#define	SUBTARGET_EXTRA_SPECS						\
+  { "startfile_executable",	STARTFILE_EXECUTABLE_SPEC },		\
+  { "startfile_xmdstub",	STARTFILE_XMDSTUB_SPEC },		\
+  { "startfile_bootstrap",	STARTFILE_BOOTSTRAP_SPEC },		\
+  { "startfile_novectors",	STARTFILE_NOVECTORS_SPEC },		\
+  { "startfile_xilkernel",	STARTFILE_XILKERNEL_SPEC },		\
+  { "startfile_crtinit",        STARTFILE_CRTINIT_SPEC },               \
+  { "startfile_default",	STARTFILE_DEFAULT_SPEC },		
+
+#undef  STARTFILE_SPEC
+#define STARTFILE_SPEC  "\
+%{Zxl-mode-executable   : %(startfile_executable)  ; \
+  Zxl-mode-xmdstub      : %(startfile_xmdstub)     ; \
+  Zxl-mode-bootstrap    : %(startfile_bootstrap)   ; \
+  Zxl-mode-novectors    : %(startfile_novectors)   ; \
+  Zxl-mode-xilkernel    : %(startfile_xilkernel)   ; \
+                        : %(startfile_default)       \
+} \
+%(startfile_crtinit)"
+
--- /dev/null
+++ b/gcc/config/microblaze/microblaze.md
@@ -0,0 +1,3767 @@
+;; microblaze.md
+;; Copyright (C) 1989, 90-98, 1999 Free Software Foundation, Inc.
+
+;; This file is part of GNU CC.
+
+;; GNU CC is free software; you can redistribute it and/or modify
+;; it under the terms of the GNU General Public License as published by
+;; the Free Software Foundation; either version 2, or (at your option)
+;; any later version.
+
+;; GNU CC is distributed in the hope that it will be useful,
+;; but WITHOUT ANY WARRANTY; without even the implied warranty of
+;; MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+;; GNU General Public License for more details.
+
+;; You should have received a copy of the GNU General Public License
+;; along with GNU CC; see the file COPYING.  If not, write to
+;; the Free Software Foundation, 59 Temple Place - Suite 330,
+;; Boston, MA 02111-1307, USA.
+
+
+;;----------------------------------------------------------------------------------------------------------------------------------------------------
+;; 
+;; Copyright (c) 2001 - 2005 Xilinx, Inc.  All rights reserved. 
+;; 
+;; microblaze.md
+;; 
+;; MicroBlaze specific file. Contains functions for generating MicroBlaze code. 
+;; Certain lines of code are from Free Software Foundation
+;; 
+;; $Header: /devl/xcs/repo/env/Jobs/MDT/sw/ThirdParty/gnu/src/gcc/src-3.4/gcc/config/microblaze/microblaze.md,v 1.17.2.12 2006/05/01 17:32:48 vasanth Exp $
+;; 
+;;----------------------------------------------------------------------------------------------------------------------------------------------------
+
+
+;;----------------------------------------------------
+;; Constants
+;;----------------------------------------------------
+(define_constants [
+  (R_SP        1)       ;; Stack pointer reg
+  (R_SR       15)       ;; Sub-routine return addr reg
+  (R_IR       14)       ;; Interrupt return addr reg
+  (R_DR       16)       ;; Debug trap return addr reg
+  (R_ER       17)       ;; Exception return addr reg
+  (R_TMP      18)       ;; Assembler temporary reg
+  (MB_PIPE_3   0)       ;; Microblaze 3-stage pipeline 
+  (MB_PIPE_5   1)       ;; Microblaze 5-stage pipeline 
+])
+
+
+;;----------------------------------------------------
+;; Instruction Attributes
+;;----------------------------------------------------
+
+;; Classification of each insn.
+;; branch	conditional branch
+;; jump		unconditional jump
+;; call		unconditional call
+;; load		load instruction(s)
+;; store	store instruction(s)
+;; move		data movement within same register set
+;; arith	integer arithmetic instruction
+;; darith	double precision integer arithmetic instructions
+;; imul		integer multiply
+;; idiv		integer divide
+;; icmp		integer compare
+;; Xfadd		floating point add/subtract
+;; Xfmul		floating point multiply
+;; Xfmadd	floating point multiply-add
+;; Xfdiv		floating point divide
+;; Xfabs		floating point absolute value
+;; Xfneg		floating point negation
+;; Xfcmp		floating point compare
+;; Xfcvt		floating point convert
+;; Xfsqrt	floating point square root
+;; multi	multiword sequence (or user asm statements)
+;; nop		no operation
+;; bshift 	Shift operations
+
+(define_attr "type"
+  "unknown,branch,jump,call,load,store,move,arith,darith,imul,idiv,icmp,multi,nop,no_delay_arith,no_delay_load,no_delay_store,no_delay_imul,no_delay_move,bshift,fadd,frsub,fmul,fdiv,fcmp,fsl"
+  (const_string "unknown"))
+
+;; Main data type used by the insn
+(define_attr "mode" "unknown,none,QI,HI,SI,DI,SF,DF" (const_string "unknown"))
+
+;; # instructions (4 bytes each)
+(define_attr "length" "" (const_int 4))
+
+;;----------------------------------------------------
+;; Attribute describing the processor.  
+;;----------------------------------------------------
+
+;; Describe a user's asm statement.
+(define_asm_attributes
+  [(set_attr "type" "multi")])
+
+;; whether or not generating calls to position independent functions
+(define_attr "abicalls" "no,yes"
+  (const (symbol_ref "microblaze_abicalls_attr")))
+
+
+;;----------------------------------------------------------------
+;; Microblaze DFA Pipeline description
+;;----------------------------------------------------------------
+                  
+;;-----------------------------------------------------------------
+/*
+   This is description of pipeline hazards based on DFA.  The
+   following constructions can be used for this:
+
+   o define_cpu_unit string [string]) describes a cpu functional unit
+     (separated by comma).
+
+     1st operand: Names of cpu function units.
+     2nd operand: Name of automaton (see comments for
+     DEFINE_AUTOMATON).
+
+     All define_reservations and define_cpu_units should have unique
+     names which can not be "nothing".
+
+   o (exclusion_set string string) means that each CPU function unit
+     in the first string can not be reserved simultaneously with each
+     unit whose name is in the second string and vise versa.  CPU
+     units in the string are separated by commas. For example, it is
+     useful for description CPU with fully pipelined floating point
+     functional unit which can execute simultaneously only single
+     floating point insns or only double floating point insns.
+
+   o (presence_set string string) means that each CPU function unit in
+     the first string can not be reserved unless at least one of units
+     whose names are in the second string is reserved.  This is an
+     asymmetric relation.  CPU units in the string are separated by
+     commas.  For example, it is useful for description that slot1 is
+     reserved after slot0 reservation for a VLIW processor.
+
+   o (absence_set string string) means that each CPU function unit in
+     the first string can not be reserved only if each unit whose name
+     is in the second string is not reserved.  This is an asymmetric
+     relation (actually exclusion set is analogous to this one but it
+     is symmetric).  CPU units in the string are separated by commas.
+     For example, it is useful for description that slot0 can not be
+     reserved after slot1 or slot2 reservation for a VLIW processor.
+
+   o (define_bypass number out_insn_names in_insn_names) names bypass with
+     given latency (the first number) from insns given by the first
+     string (see define_insn_reservation) into insns given by the
+     second string.  Insn names in the strings are separated by
+     commas.
+
+   o (define_automaton string) describes names of an automaton
+     generated and used for pipeline hazards recognition.  The names
+     are separated by comma.  Actually it is possibly to generate the
+     single automaton but unfortunately it can be very large.  If we
+     use more one automata, the summary size of the automata usually
+     is less than the single one.  The automaton name is used in
+     define_cpu_unit.  All automata should have unique names.
+
+   o (define_reservation string string) names reservation (the first
+     string) of cpu functional units (the 2nd string).  Sometimes unit
+     reservations for different insns contain common parts.  In such
+     case, you describe common part and use one its name (the 1st
+     parameter) in regular expression in define_insn_reservation.  All
+     define_reservations, define results and define_cpu_units should
+     have unique names which can not be "nothing".
+
+   o (define_insn_reservation name default_latency condition regexpr)
+     describes reservation of cpu functional units (the 3nd operand)
+     for instruction which is selected by the condition (the 2nd
+     parameter).  The first parameter is used for output of debugging
+     information.  The reservations are described by a regular
+     expression according the following syntax:
+
+       regexp = regexp "," oneof
+              | oneof
+
+       oneof = oneof "|" allof
+             | allof
+
+       allof = allof "+" repeat
+             | repeat
+
+       repeat = element "*" number
+              | element
+
+       element = cpu_function_name
+               | reservation_name
+               | result_name
+               | "nothing"
+               | "(" regexp ")"
+
+       1. "," is used for describing start of the next cycle in
+          reservation.
+
+       2. "|" is used for describing the reservation described by the
+          first regular expression *or* the reservation described by
+          the second regular expression *or* etc.
+
+       3. "+" is used for describing the reservation described by the
+          first regular expression *and* the reservation described by
+          the second regular expression *and* etc.
+
+       4. "*" is used for convenience and simply means sequence in
+          which the regular expression are repeated NUMBER times with
+          cycle advancing (see ",").
+
+       5. cpu function unit name which means reservation.
+
+       6. reservation name -- see define_reservation.
+
+       7. string "nothing" means no units reservation.
+
+*/
+;;-----------------------------------------------------------------
+
+
+;;----------------------------------------------------------------
+;; Microblaze 5-stage pipeline description (v5.00.a and later)
+;;----------------------------------------------------------------                 
+                    
+(define_automaton   "mbpipe_5")
+(define_cpu_unit    "mb_issue,mb_iu,mb_wb,mb_fpu,mb_fpu_2,mb_mul,mb_mul_2,mb_div,mb_div_2,mb_bs,mb_bs_2" "mbpipe_5")
+
+(define_insn_reservation "mb-integer" 1 
+  (and (eq_attr "type" "branch,jump,call,arith,darith,icmp,nop,no_delay_arith")
+       (eq (symbol_ref  "microblaze_pipe") (const_int MB_PIPE_5)))
+  "mb_issue,mb_iu,mb_wb")
+
+(define_insn_reservation "mb-special-move" 2
+  (and (eq_attr "type" "move")
+       (eq (symbol_ref  "microblaze_pipe") (const_int MB_PIPE_5)))
+  "mb_issue,mb_iu*2,mb_wb")
+
+(define_insn_reservation "mb-mem-load" 3
+  (and (eq_attr "type" "load,no_delay_load")
+       (eq (symbol_ref  "microblaze_pipe") (const_int MB_PIPE_5)))
+  "mb_issue,mb_iu,mb_wb")
+
+(define_insn_reservation "mb-mem-store" 1
+  (and (eq_attr "type" "store,no_delay_store")
+       (eq (symbol_ref  "microblaze_pipe") (const_int MB_PIPE_5)))
+  "mb_issue,mb_iu,mb_wb")
+
+(define_insn_reservation "mb-mul" 3
+  (and (eq_attr "type" "imul,no_delay_imul")
+       (eq (symbol_ref  "microblaze_pipe") (const_int MB_PIPE_5)))
+  "mb_issue,mb_mul,mb_mul_2*2,mb_wb")
+
+(define_insn_reservation "mb-div" 34            
+  (and (eq_attr "type" "idiv")
+       (eq (symbol_ref  "microblaze_pipe") (const_int MB_PIPE_5)))
+    "mb_issue,mb_div,mb_div_2*33,mb_wb")
+
+(define_insn_reservation "mb-bs" 2 
+  (and (eq_attr "type" "bshift")
+       (eq (symbol_ref  "microblaze_pipe") (const_int MB_PIPE_5)))
+   "mb_issue,mb_bs,mb_bs_2,mb_wb")
+
+;; We are not producing FSL instructions anyways for this to have any practical use.
+;; (define_insn_reservation "mb-fsl" 2 
+;;  (and (eq_attr "type" "fsl")
+;;       (eq (symbol_ref  "microblaze_pipe") (const_int MB_PIPE_5)))
+;;    "mb_fsl,mb_fsl_2")
+
+(define_insn_reservation "mb-fpu-add-sub-mul" 6
+  (and (eq_attr "type" "fadd,frsub,fmul")
+       (eq (symbol_ref  "microblaze_pipe") (const_int MB_PIPE_5)))
+  "mb_issue,mb_fpu,mb_fpu_2*5,mb_wb")
+
+(define_insn_reservation "mb-fpu-fcmp" 3
+  (and (eq_attr "type" "fcmp")
+       (eq (symbol_ref  "microblaze_pipe") (const_int MB_PIPE_5)))
+  "mb_issue,mb_fpu,mb_fpu*2,mb_wb")
+
+(define_insn_reservation "mb-fpu-div" 30
+  (and (eq_attr "type" "fdiv")
+       (eq (symbol_ref  "microblaze_pipe") (const_int MB_PIPE_5)))
+  "mb_issue,mb_fpu,mb_fpu_2*29,mb_wb")
+
+
+
+;;----------------------------------------------------------------
+;; Microblaze 3-stage pipeline description (for v4.00.a and earlier)
+;;----------------------------------------------------------------
+
+(define_automaton   "mbpipe_3")
+(define_cpu_unit    "mb3_iu" "mbpipe_3")
+
+(define_insn_reservation "mb3-integer" 1 
+  (and (eq_attr "type" "branch,jump,call,arith,darith,icmp,nop,no_delay_arith")
+       (eq (symbol_ref  "microblaze_pipe") (const_int MB_PIPE_3)))
+  "mb3_iu")
+
+(define_insn_reservation "mb3-special-move" 2
+  (and (eq_attr "type" "move")
+       (eq (symbol_ref  "microblaze_pipe") (const_int MB_PIPE_3)))
+  "mb3_iu*2")
+
+(define_insn_reservation "mb3-mem-load" 2
+  (and (eq_attr "type" "load,no_delay_load")
+       (eq (symbol_ref  "microblaze_pipe") (const_int MB_PIPE_3)))
+  "mb3_iu")
+
+(define_insn_reservation "mb3-mem-store" 1
+  (and (eq_attr "type" "store,no_delay_store")
+       (eq (symbol_ref  "microblaze_pipe") (const_int MB_PIPE_3)))
+  "mb3_iu")
+
+(define_insn_reservation "mb3-mul" 3
+  (and (eq_attr "type" "imul,no_delay_imul")
+       (eq (symbol_ref  "microblaze_pipe") (const_int MB_PIPE_3)))
+  "mb3_iu")
+
+(define_insn_reservation "mb3-div" 34            
+  (and (eq_attr "type" "idiv")
+       (eq (symbol_ref  "microblaze_pipe") (const_int MB_PIPE_3)))
+    "mb3_iu")
+
+(define_insn_reservation "mb3-bs" 2 
+  (and (eq_attr "type" "bshift")
+       (eq (symbol_ref  "microblaze_pipe") (const_int MB_PIPE_3)))
+   "mb3_iu")
+
+;; We are not producing FSL instructions anyways for this to have any practical use.
+;; (define_insn_reservation "mb-fsl" 2 
+;;  (and (eq_attr "type" "fsl")
+;;       (eq (symbol_ref  "microblaze_pipe") (const_int MB_PIPE_3)))
+;;    "mb3_iu*2")
+
+(define_insn_reservation "mb3-fpu-add-sub-mul" 6
+  (and (eq_attr "type" "fadd,frsub,fmul")
+       (eq (symbol_ref  "microblaze_pipe") (const_int MB_PIPE_3)))
+  "mb3_iu")
+
+(define_insn_reservation "mb3-fpu-fcmp" 3
+  (and (eq_attr "type" "fcmp")
+       (eq (symbol_ref  "microblaze_pipe") (const_int MB_PIPE_3)))
+  "mb3_iu")
+
+(define_insn_reservation "mb3-fpu-div" 30
+  (and (eq_attr "type" "fdiv")
+       (eq (symbol_ref  "microblaze_pipe") (const_int MB_PIPE_3)))
+  "mb3_iu")
+
+(automata_option "v")
+(automata_option "time")
+(automata_option "progress")
+
+;;----------------------------------------------------------------
+;; Microblaze delay slot description
+;;----------------------------------------------------------------
+(define_delay (eq_attr "type" "branch,call,jump")
+  [(and (eq_attr "type" "!branch,call,jump,icmp,multi,no_delay_arith,no_delay_load,no_delay_store,no_delay_imul,no_delay_move,darith") 
+        (ior (eq (symbol_ref "microblaze_no_unsafe_delay") (const_int 0))
+              (eq_attr "type" "!fadd,frsub,fmul,fdiv,fcmp,store,load")
+              ))
+  (nil) (nil)])
+
+
+;;----------------------------------------------------------------
+;; Microblaze FPU
+;;----------------------------------------------------------------
+
+(define_insn "addsf3"
+  [(set (match_operand:SF 0 "register_operand" "=d")
+        (plus:SF (match_operand:SF 1 "register_operand" "d")
+                 (match_operand:SF 2 "register_operand" "d")))]
+  "TARGET_HARD_FLOAT"
+  "fadd\t%0,%1,%2"
+  [(set_attr "type"     "fadd")
+  (set_attr "mode"      "SF")
+  (set_attr "length"    "4")])
+
+(define_insn "subsf3"
+  [(set (match_operand:SF 0 "register_operand" "=d")
+        (minus:SF (match_operand:SF 1 "register_operand" "d")
+                  (match_operand:SF 2 "register_operand" "d")))]
+  "TARGET_HARD_FLOAT"
+  "frsub\t%0,%2,%1"
+  [(set_attr "type"     "frsub")
+  (set_attr "mode"      "SF")
+  (set_attr "length"    "4")])
+
+(define_insn "mulsf3"
+  [(set (match_operand:SF 0 "register_operand" "=d")
+        (mult:SF (match_operand:SF 1 "register_operand" "d")
+                 (match_operand:SF 2 "register_operand" "d")))]
+  "TARGET_HARD_FLOAT"
+  "fmul\t%0,%1,%2"
+  [(set_attr "type"     "fmul")
+  (set_attr "mode"      "SF")
+  (set_attr "length"    "4")])
+
+
+(define_insn "divsf3"
+  [(set (match_operand:SF 0 "register_operand" "=d")
+        (div:SF (match_operand:SF 1 "register_operand" "d")
+                (match_operand:SF 2 "register_operand" "d")))]
+  "TARGET_HARD_FLOAT"
+  "fdiv\t%0,%2,%1"
+  [(set_attr "type"     "fdiv")
+  (set_attr "mode"      "SF")
+  (set_attr "length"    "4")])
+
+
+;;----------------------------------------------------------------
+;; Add
+;;----------------------------------------------------------------
+
+;; Add 2 SImode integers [ src1 = reg ; src2 = arith ; dest = reg ]
+
+(define_expand "addsi3"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+	(plus:SI (match_operand:SI 1 "reg_or_0_operand" "dJ")
+		 (match_operand:SI 2 "arith_operand" "d")))]
+  ""
+  ""
+  )
+
+
+;; Leave carry as is
+(define_insn "addsi3_internal_reg"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+	(plus:SI (match_operand:SI 1 "reg_or_0_operand" "dJ")
+		 (match_operand:SI 2 "register_operand" "d")))]
+  "GET_CODE (operands[2]) != CONST_INT"
+  "addk\t%0,%z1,%2"
+  [(set_attr "type"	"arith")
+  (set_attr "mode"	"SI")
+  (set_attr "length"	"4")])
+
+;; Leave carry as is
+(define_insn "addsi3_internal_imm"
+  [(set (match_operand:SI 0 "register_operand" "=d,d")
+	(plus:SI (match_operand:SI 1 "reg_or_0_operand" "dJ,dJ")
+		 (match_operand:SI 2 "immediate_operand" "K,i")))]
+  "(GET_CODE (operands[2]) == CONST_INT)"
+  "addik\t%0,%z1,%2"
+  [(set_attr "type"	"arith,no_delay_arith")
+  (set_attr "mode"	"SI,SI")
+  (set_attr "length"	"4,8")])
+
+;;----------------------------------------------------------------
+;; Double Precision Additions
+;;----------------------------------------------------------------
+
+;; reg_DI_dest = reg_DI_src1 + DI_src2
+
+(define_expand "adddi3"
+  [(set (match_operand:DI 0 "register_operand" "")
+	(plus:DI (match_operand: DI 1 "register_operand" "")
+		 (match_operand: DI 2 "arith_operand" "")))]
+  ""
+  ""
+)
+
+;; Adding 2 DI operands in register
+
+(define_insn "adddi3_internal1"
+  [(set (match_operand:DI 0 "register_operand" "=d")
+	(plus:DI (match_operand:DI 1 "register_operand" "d")
+		 (match_operand:DI 2 "register_operand" "d")))]
+  ""
+  { 
+      return "add\t%L0,%L1,%L2\;addc\t%M0,%M1,%M2";
+  }
+  [(set_attr "type"	"darith")
+  (set_attr "mode"	"DI")
+  (set_attr "length"	"8")])
+
+;; Adding 2 DI operands, one in reg, other imm.
+(define_insn "adddi3_internal2"
+  [(set (match_operand:DI 0 "register_operand" "=d")
+	(plus:DI (match_operand:DI 1 "register_operand" "d")
+		 (match_operand:SI 2 "immediate32_operand" "i")))]
+  ""
+  { 
+        if (INTVAL (operands[2]) > 0)
+                return "addi\t%L0,%L1,%2\;addc\t%M0,%M1,r0";
+        else
+                return "addi\t%L0,%L1,%2\;addc\t%M0,%M1,r0\;addi\t%M0,%M0,-1";
+  }
+  [(set_attr "type"	"darith")
+  (set_attr "mode"	"DI")
+  (set_attr "length"	"8")])
+
+
+;;----------------------------------------------------------------
+;; Add with carry 
+;; 
+;; This will never be generated by the compiler, 
+;; via matching but by internal generation
+;; OBSOLETE: Not used any more.
+;;----------------------------------------------------------------
+
+(define_insn "addc"
+  [(set (match_operand:SI 0 "register_operand" "+d")
+	(plus:SI (match_operand:SI 1 "register_operand" "+d")
+		 (match_operand:SI 2 "register_operand" "+d")))
+  (use (match_operand:SI 3 "arith_operand" "i"))]
+  "(GET_CODE (operands[3]) == CONST_INT && (INTVAL(operands[3]) == 0xff))" 
+  "addc\t%0,%1,%1"
+  [(set_attr "type"	"arith")
+  (set_attr "mode"	"SI")
+  (set_attr "length"	"4")])
+
+
+
+;;----------------------------------------------------------------
+;; Subtraction
+;;----------------------------------------------------------------
+
+
+(define_expand "subsi3"
+  [(set (match_operand:SI 0 "register_operand" "=d,d,d")
+	(minus:SI (match_operand:SI 1 "arith_operand" "dJ,dJ,i")
+                  (match_operand:SI 2 "arith_operand" "d,i,dJ")))]
+  ""
+  ""
+  )
+
+(define_insn "subsi3_internal"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+	(minus:SI (match_operand:SI 1 "reg_or_0_operand" "dJ")
+		  (match_operand:SI 2 "register_operand" "d")))]
+  "(GET_CODE (operands[2]) != CONST_INT)"
+  "rsubk\t%0,%2,%z1"
+  [(set_attr "type"	"arith")
+  (set_attr "mode"	"SI")
+  (set_attr "length"	"4")])
+
+
+(define_insn "subsi3_internal2"
+  [(set (match_operand:SI 0 "register_operand" "=d,d")
+	(minus:SI (match_operand:SI 1 "reg_or_0_operand" "dJ,dJ")
+		  (match_operand:SI 2 "immediate_operand" "K,i")))]
+  "(GET_CODE (operands[2]) == CONST_INT)"
+  {
+      return "addik\t%0,%z1,-%2";
+  }
+  [(set_attr "type"	"arith,no_delay_arith")
+  (set_attr "mode"	"SI")
+  (set_attr "length"	"8")])
+
+;;
+;;(define_insn "subsi3_internal2"
+;;  [(set (match_operand:SI 0 "register_operand" "=d,d")
+;;	(minus:SI (match_operand:SI 1 "reg_or_0_operand" "dJ,dJ")
+;;		  (match_operand:SI 2 "immediate_operand" "K,i")))]
+;;  "(GET_CODE (operands[2]) == CONST_INT)"
+;;  "addi\t%0,%z1,-%2"
+;;  [(set_attr "type"	"arith,no_delay_arith")
+;;   (set_attr "mode"	"SI")
+;;   (set_attr "length"	"1")])
+
+(define_insn "subsi3_internal3"
+  [(set (match_operand:SI 0 "register_operand" "=d,d")
+	(minus:SI (match_operand:SI 1 "immediate_operand" "K,i")
+		  (match_operand:SI 2 "register_operand" "dJ,dJ")))]
+  "(GET_CODE (operands[1]) == CONST_INT)"
+  "rsubik\t%0,%2,%z1"
+  [(set_attr "type"	"arith,no_delay_arith")
+  (set_attr "mode"	"SI")
+  (set_attr "length"	"8")])
+
+
+;;----------------------------------------------------------------
+;; Double Precision Subtraction
+;;----------------------------------------------------------------
+(define_expand "subdi3"
+  [(set (match_operand:DI 0 "register_operand" "")
+	(minus:DI (match_operand:DI 1 "register_operand" "")
+		  (match_operand:DI 2 "arith_operand" "")))]
+  ""
+  ""
+)
+
+(define_insn "subdi3_internal"
+  [(set (match_operand:DI 0 "register_operand" "=d")
+	(minus:DI (match_operand:DI 1 "register_operand" "d")
+		  (match_operand:DI 2 "register_operand" "d")))]
+  "GET_CODE (operands[2]) != CONST_INT"
+  "rsub\t%L0,%L2,%L1\;rsubc\t%M0,%M2,%M1"
+  [(set_attr "type"	"darith")
+  (set_attr "mode"	"DI")
+  (set_attr "length"	"8")])
+
+
+(define_insn "subdi3_internal_2"
+  [(set (match_operand:DI 0 "register_operand" "=d,d,d")
+	(minus:DI (match_operand:DI 1 "register_operand" "d,d,d")
+		  (match_operand:DI 2 "small_int" "P,J,N")))
+  (clobber (match_operand:SI 3 "register_operand" "=d,d,d"))]
+  ""
+  "@
+   rsub\t%L0,%L2,%L1\;rsubc\t%M0,%M2,%M1
+   add\t%L0,%L1\;add\t%M0,%M1
+   rsub\t%L0,%L2,%L1\;rsubc\t%M0,%M2,%M1"
+  [(set_attr "type"	"darith")
+  (set_attr "mode"	"DI")
+  (set_attr "length"	"8,8,8")])
+
+
+;;----------------------------------------------------------------
+;; Multiplication
+;;----------------------------------------------------------------
+
+(define_insn "mulsi3_imm0"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+	(mult:SI  (match_operand:SI 1 "register_operand" "d")
+		  (match_operand:SI 2 "immediate_operand" "di")))]
+  "INTVAL(operands[2]) == 0"
+  "addk\t%0,r0,r0"
+  [(set_attr "type"	"imul")
+  (set_attr "mode"	"SI")
+  (set_attr "length"	"4")])
+
+(define_insn "mulsi3_imm2"
+  [(set (match_operand:SI 0 "register_operand" "=d,d")
+	(mult:SI  (match_operand:SI 1 "register_operand" "d,d")
+		  (match_operand:SI 2 "immediate_operand" "K,i")))]
+  "INTVAL(operands[2]) == 2"
+  "addk\t%0,%1,%1"
+  [(set_attr "type"	"imul,no_delay_imul")
+  (set_attr "mode"	"SI")
+  (set_attr "length"	"4,4")])
+
+(define_insn "mulsi3_imm4"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+	(mult:SI  (match_operand:SI 1 "register_operand" "d")
+		  (match_operand:SI 2 "immediate_operand" "di")))]
+  "(INTVAL(operands[2]) == 4 && (TARGET_SOFT_MUL))"
+  "addk\t%0,%1,%1\;addk\t%0,%0,%0"
+  [(set_attr "type"	"no_delay_imul")
+  (set_attr "mode"	"SI")
+  (set_attr "length"	"8")])
+
+(define_insn "mulsi3_imm8"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+	(mult:SI  (match_operand:SI 1 "register_operand" "d")
+		  (match_operand:SI 2 "immediate_operand" "di")))]
+  "(INTVAL(operands[2]) == 8 && (TARGET_SOFT_MUL))"
+  "addk\t%0,%1,%1\;addk\t%0,%0,%0\;addk\t%0,%0,%0"
+  [(set_attr "type"	"no_delay_imul")
+  (set_attr "mode"	"SI")
+  (set_attr "length"	"12")])
+
+(define_insn "mulsi3_imm"
+  [(set (match_operand:SI 0 "register_operand" "=d,d")
+	(mult:SI  (match_operand:SI 1 "register_operand" "d,d")
+		  (match_operand:SI 2 "immediate_operand" "K,i")))]
+  "!TARGET_SOFT_MUL"
+  "muli\t%0,%1,%2"
+  [(set_attr "type"	"imul,no_delay_imul")
+  (set_attr "mode"	"SI")
+  (set_attr "length"	"4,8")])
+
+(define_expand "mulsi3"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+	(mult:SI (match_operand:SI 1 "register_operand" "d")
+		 (match_operand:SI 2 "register_operand" "d")))
+  (clobber (match_scratch:SI 3 "=d"))
+  (clobber (match_scratch:SI 4 "=d"))]
+  "!TARGET_SOFT_MUL"
+  {
+    emit_insn (gen_mulsi3_mult3 (operands[0], operands[1], operands[2]));
+    DONE;
+  }
+)
+
+(define_insn "mulsi3_mult3"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+	(mult:SI (match_operand:SI 1 "register_operand" "d")
+		 (match_operand:SI 2 "register_operand" "d")))]
+  "!TARGET_SOFT_MUL"
+  "mul\t%0,%1,%2"
+  [(set_attr "type"	"imul")
+  (set_attr "mode"	"SI")
+  (set_attr "length"	"4")])
+
+
+;;----------------------------------------------------------------
+;; Division and remainder
+;;----------------------------------------------------------------
+(define_expand "divsi3"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+	(div:SI (match_operand:SI 1 "register_operand" "d")
+                (match_operand:SI 2 "register_operand" "d")))
+  ]
+  "(!TARGET_SOFT_DIV) || (TARGET_BARREL_SHIFT && TARGET_SMALL_DIVIDES)"
+  {
+    if (TARGET_SOFT_DIV && TARGET_BARREL_SHIFT && TARGET_SMALL_DIVIDES) { 
+        
+        /* Table lookup software divides. Works for all (nr/dr) where (0 <= nr,dr <= 15) */
+
+        rtx regt1 = gen_reg_rtx (SImode); 
+        rtx reg18 = gen_rtx_REG (SImode, R_TMP);
+        rtx regqi = gen_reg_rtx (QImode);
+        rtx div_label = gen_label_rtx ();
+        rtx div_label_ref = gen_rtx_LABEL_REF (VOIDmode, div_label);
+        rtx div_end_label = gen_label_rtx ();
+        rtx div_table_rtx = gen_rtx_SYMBOL_REF (QImode,"_divsi3_table");
+        rtx mem_rtx;
+        rtx op0, ret;
+        rtx jump, cjump, insn;
+
+        insn = emit_insn (gen_iorsi3 (regt1, operands[1], operands[2]));
+        cjump = emit_jump_insn_after (gen_branch_compare_imm_uns (gen_rtx_GTU (SImode, regt1, GEN_INT (15)), regt1, GEN_INT (15), div_label_ref, pc_rtx), insn);
+        LABEL_NUSES (div_label) = 1; 
+        JUMP_LABEL (cjump) = div_label;
+        emit_insn (gen_rtx_CLOBBER (SImode, reg18));
+
+        emit_insn (gen_ashlsi3_bshift (regt1, operands[1], GEN_INT(4)));
+        emit_insn (gen_addsi3 (regt1, regt1, operands[2]));
+        mem_rtx = gen_rtx (MEM, QImode,
+                            gen_rtx (PLUS, Pmode, regt1, div_table_rtx));
+
+        insn = emit_insn (gen_movqi (regqi, mem_rtx)); 
+        insn = emit_insn (gen_movsi (operands[0], gen_rtx_SUBREG (SImode, regqi, 0)));
+        jump = emit_jump_insn_after (gen_jump (div_end_label), insn); 
+        JUMP_LABEL (jump) = div_end_label;
+        LABEL_NUSES (div_end_label) = 1; 
+        emit_barrier ();
+
+        emit_label (div_label);
+        ret = emit_library_call_value (gen_rtx_SYMBOL_REF (Pmode, "__divsi3"), operands[0], LCT_NORMAL, GET_MODE (operands[0]), 
+                                                           2, operands[1], GET_MODE (operands[1]), operands[2], GET_MODE (operands[2]));
+        if (ret != operands[0])
+                emit_move_insn (operands[0], ret);    
+
+        emit_label (div_end_label);
+        emit_insn (gen_blockage ());
+        DONE;
+    } else if (!TARGET_SOFT_DIV) {
+        emit_insn (gen_divsi3_internal (operands[0], operands[1], operands[2]));
+        DONE;
+    }
+  }     
+)
+
+
+(define_insn "divsi3_internal"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+	(div:SI (match_operand:SI 1 "register_operand" "d")
+		(match_operand:SI 2 "register_operand" "d")))
+  ]
+  "!TARGET_SOFT_DIV"
+  "idiv\t%0,%2,%1"
+  [(set_attr "type"	"idiv")
+  (set_attr "mode"	"SI")
+  (set_attr "length"	"4")]
+)
+
+(define_expand "udivsi3"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+	(udiv:SI (match_operand:SI 1 "register_operand" "d")
+                 (match_operand:SI 2 "register_operand" "d")))
+  ]
+  "!TARGET_SOFT_DIV"
+  {
+    emit_insn (gen_udivsi3_internal (operands[0], operands[1], operands[2]));
+    DONE;
+  }
+)
+
+(define_insn "udivsi3_internal"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+	(udiv:SI (match_operand:SI 1 "register_operand" "d")
+                 (match_operand:SI 2 "register_operand" "d")))
+  ]
+  "!TARGET_SOFT_DIV"
+  "idivu\t%0,%2,%1"
+  [(set_attr "type"	"idiv")
+  (set_attr "mode"	"SI")
+  (set_attr "length"	"4")])
+
+
+;;----------------------------------------------------------------
+;; Negation and one's complement
+;;----------------------------------------------------------------
+
+(define_insn "negsi2"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+	(neg:SI (match_operand:SI 1 "register_operand" "d")))]
+  ""
+  {
+        operands[2] = const0_rtx;
+        return "rsubk\t%0,%1,%z2";
+  }
+  [(set_attr "type"	"arith")
+  (set_attr "mode"	"SI")
+  (set_attr "length"	"4")])
+
+(define_expand "negdi2"
+  [(parallel [(set (match_operand:DI 0 "register_operand" "=d")
+		   (neg:DI (match_operand:DI 1 "register_operand" "d")))
+             (clobber (match_dup 2))])]
+  ""
+  {
+    operands[2] = gen_reg_rtx (SImode);
+  }
+)
+
+(define_insn "negdi2_internal"
+  [(set (match_operand:DI 0 "register_operand" "=d")
+	(neg:DI (match_operand:DI 1 "register_operand" "d")))
+  (clobber (match_operand:SI 2 "register_operand" "=d"))]
+  ""
+  {
+        operands[3] = const0_rtx;
+        return "rsub\t%L0,%L1,%z3\;rsubc\t%M0,%M1,%z3";
+  }
+  [(set_attr "type"	"darith")
+  (set_attr "mode"	"DI")
+  (set_attr "length"	"8")])
+
+
+(define_insn "one_cmplsi2"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+	(not:SI (match_operand:SI 1 "register_operand" "d")))]
+  ""
+  {
+        return "xori\t%0,%1,-1";
+  }
+  [(set_attr "type"	"arith")
+  (set_attr "mode"	"SI")
+  (set_attr "length"	"4")])
+
+(define_insn "one_cmpldi2"
+  [(set (match_operand:DI 0 "register_operand" "=d")
+	(not:DI (match_operand:DI 1 "register_operand" "d")))]
+  ""
+  {
+        operands[2] = const0_rtx;
+        return "nor\t%M0,%z2,%M1\;nor\t%L0,%z2,%L1";
+  }
+  [(set_attr "type"	"darith")
+  (set_attr "mode"	"DI")
+  (set_attr "length"    "8")]
+)
+
+(define_split
+  [(set (match_operand:DI 0 "register_operand" "")
+	(not:DI (match_operand:DI 1 "register_operand" "")))]
+  "reload_completed && !TARGET_DEBUG_D_MODE && !TARGET_DEBUG_G_MODE
+   && GET_CODE (operands[0]) == REG && GP_REG_P (REGNO (operands[0]))
+   && GET_CODE (operands[1]) == REG && GP_REG_P (REGNO (operands[1]))"
+
+  [(set (subreg:SI (match_dup 0) 0) (not:SI (subreg:SI (match_dup 1) 0)))
+  (set (subreg:SI (match_dup 0) 4) (not:SI (subreg:SI (match_dup 1) 4)))]
+  "")
+
+
+;;----------------------------------------------------------------
+;; Logical
+;;----------------------------------------------------------------
+
+;; Many of these instructions uses trivial define_expands
+
+(define_expand "andsi3"
+  [(set (match_operand:SI 0 "register_operand" "")
+	(and:SI (match_operand:SI 1 "arith_operand" "")
+		(match_operand:SI 2 "arith_operand" "")))]
+  ""
+  ""
+)
+
+(define_insn "andsi3_internal"
+  [(set (match_operand:SI 0 "register_operand" "=d,d,d,d")
+	(and:SI (match_operand:SI 1 "arith_operand" "%d,d,d,d")
+		(match_operand:SI 2 "arith_operand" "d,K,i,M")))]
+  ""
+  "@
+   and\t%0,%1,%2  
+   andi\t%0,%1,%2 #and1
+   andi\t%0,%1,%2 #and2
+   andi\t%0,%1,%2 #and3"
+  [(set_attr "type"	"arith,arith,no_delay_arith,no_delay_arith")
+  (set_attr "mode"	"SI,SI,SI,SI")
+  (set_attr "length"	"4,8,8,8")])
+
+
+(define_expand "anddi3"
+  [(set (match_operand:DI 0 "register_operand" "=d")
+	(and:DI (match_operand:DI 1 "register_operand" "d")
+		(match_operand:DI 2 "register_operand" "d")))]
+  "!TARGET_DEBUG_G_MODE"
+  ""
+)
+
+(define_insn ""
+  [(set (match_operand:DI 0 "register_operand" "=d")
+	(and:DI (match_operand:DI 1 "register_operand" "d")
+		(match_operand:DI 2 "register_operand" "d")))]
+  "(!TARGET_DEBUG_G_MODE)"
+  {
+        return "and\t%M0,%M1,%M2\;and\t%L0,%L1,%L2";
+  }
+  [(set_attr "type"	"darith")
+  (set_attr "mode"	"DI")
+  (set_attr "length"    "8")])
+
+
+(define_split
+  [(set (match_operand:DI 0 "register_operand" "")
+	(and:DI (match_operand:DI 1 "register_operand" "")
+		(match_operand:DI 2 "register_operand" "")))]
+  "reload_completed && !TARGET_DEBUG_D_MODE && !TARGET_DEBUG_G_MODE
+   && GET_CODE (operands[0]) == REG && GP_REG_P (REGNO (operands[0]))
+   && GET_CODE (operands[1]) == REG && GP_REG_P (REGNO (operands[1]))
+   && GET_CODE (operands[2]) == REG && GP_REG_P (REGNO (operands[2]))"
+
+  [(set (subreg:SI (match_dup 0) 0) (and:SI (subreg:SI (match_dup 1) 0) (subreg:SI (match_dup 2) 0)))
+  (set (subreg:SI (match_dup 0) 4) (and:SI (subreg:SI (match_dup 1) 4) (subreg:SI (match_dup 2) 4)))]
+  "")
+
+(define_expand "iorsi3"
+  [(set (match_operand:SI 0 "register_operand" "+d,d,d,d")
+	(ior:SI (match_operand:SI 1 "arith_operand" "%d,d,d,d")
+		(match_operand:SI 2 "arith_operand" "d,K,i,M")))]
+  ""
+  ""
+)
+
+(define_insn ""
+  [(set (match_operand:SI 0 "register_operand" "=d,d,d,d")
+	(ior:SI (match_operand:SI 1 "arith_operand" "%d,d,d,d")
+		(match_operand:SI 2 "arith_operand" "d,K,M,i")))]
+  ""
+  "@
+   or\t%0,%1,%2 
+   ori\t%0,%1,%2 
+   ori\t%0,%1,%2 
+   ori\t%0,%1,%2" 
+  [(set_attr "type"	"arith,no_delay_arith,no_delay_arith,no_delay_arith")
+  (set_attr "mode"	"SI,SI,SI,SI")
+  (set_attr "length"	"4,8,8,8")])
+
+
+(define_expand "iordi3"
+  [(set (match_operand:DI 0 "register_operand" "=d")
+	(ior:DI (match_operand:DI 1 "register_operand" "d")
+		(match_operand:DI 2 "register_operand" "d")))]
+  "!TARGET_DEBUG_G_MODE"
+  ""
+)
+
+(define_insn ""
+  [(set (match_operand:DI 0 "register_operand" "=d")
+	(ior:DI (match_operand:DI 1 "register_operand" "d")
+		(match_operand:DI 2 "register_operand" "d")))]
+  "(!TARGET_DEBUG_G_MODE)"
+  {
+        return "or\t%M0,%M1,%M2\;or\t%L0,%L1,%L2";
+  }
+  [(set_attr "type"	"darith")
+  (set_attr "mode"	"DI")
+  (set_attr "length"    "8")]
+)
+
+
+(define_split
+  [(set (match_operand:DI 0 "register_operand" "")
+	(ior:DI (match_operand:DI 1 "register_operand" "")
+		(match_operand:DI 2 "register_operand" "")))]
+  "reload_completed && !TARGET_DEBUG_D_MODE && !TARGET_DEBUG_G_MODE
+   && GET_CODE (operands[0]) == REG && GP_REG_P (REGNO (operands[0]))
+   && GET_CODE (operands[1]) == REG && GP_REG_P (REGNO (operands[1]))
+   && GET_CODE (operands[2]) == REG && GP_REG_P (REGNO (operands[2]))"
+
+  [(set (subreg:SI (match_dup 0) 0) (ior:SI (subreg:SI (match_dup 1) 0) (subreg:SI (match_dup 2) 0)))
+  (set (subreg:SI (match_dup 0) 4) (ior:SI (subreg:SI (match_dup 1) 4) (subreg:SI (match_dup 2) 4)))]
+  "")
+
+(define_expand "xorsi3"
+  [(set (match_operand:SI 0 "register_operand" "=d,d,d")
+	(xor:SI (match_operand:SI 1 "arith_operand" "%d,d,d")
+		(match_operand:SI 2 "arith_operand" "d,K,i")))]
+  ""
+  ""
+)
+
+(define_insn ""
+  [(set (match_operand:SI 0 "register_operand" "=d,d,d")
+	(xor:SI (match_operand:SI 1 "arith_operand" "%d,d,d")
+		(match_operand:SI 2 "arith_operand" "d,K,i")))]
+  ""
+  "@
+   xor\t%0,%1,%2
+   xori\t%0,%1,%2
+   xori\t%0,%1,%2"
+  [(set_attr "type"	"arith,arith,no_delay_arith")
+  (set_attr "mode"	"SI,SI,SI")
+  (set_attr "length"	"4,8,8")])
+
+;; ??? If delete the 32-bit long long patterns, then could merge this with
+;; the following xordi3_internal pattern.
+(define_expand "xordi3"
+  [(set (match_operand:DI 0 "register_operand" "=d")
+	(xor:DI (match_operand:DI 1 "register_operand" "d")
+		(match_operand:DI 2 "register_operand" "d")))]
+  "!TARGET_DEBUG_G_MODE"
+  ""
+)
+
+(define_insn ""
+  [(set (match_operand:DI 0 "register_operand" "=d")
+	(xor:DI (match_operand:DI 1 "register_operand" "d")
+		(match_operand:DI 2 "register_operand" "d")))]
+  "(!TARGET_DEBUG_G_MODE) "
+  {
+        return "xor\t%M0,%M1,%M2\;xor\t%L0,%L1,%L2";
+  }
+  [(set_attr "type"	"darith")
+  (set_attr "mode"	"DI")
+  (set_attr "length"    "8")]
+)
+
+
+(define_split
+  [(set (match_operand:DI 0 "register_operand" "")
+	(xor:DI (match_operand:DI 1 "register_operand" "")
+		(match_operand:DI 2 "register_operand" "")))]
+  "reload_completed && !TARGET_DEBUG_D_MODE && !TARGET_DEBUG_G_MODE
+   && GET_CODE (operands[0]) == REG && GP_REG_P (REGNO (operands[0]))
+   && GET_CODE (operands[1]) == REG && GP_REG_P (REGNO (operands[1]))
+   && GET_CODE (operands[2]) == REG && GP_REG_P (REGNO (operands[2]))"
+
+  [(set (subreg:SI (match_dup 0) 0) (xor:SI (subreg:SI (match_dup 1) 0) (subreg:SI (match_dup 2) 0)))
+  (set (subreg:SI (match_dup 0) 4) (xor:SI (subreg:SI (match_dup 1) 4) (subreg:SI (match_dup 2) 4)))]
+  "")
+
+;;----------------------------------------------------------------
+;; Truncation
+;;----------------------------------------------------------------
+
+;;----------------------------------------------------------------
+;; Zero extension
+;;----------------------------------------------------------------
+
+;; Extension insns.
+;; Those for integer source operand are ordered widest source type first.
+
+(define_expand "zero_extendsidi2"
+  [(set (match_operand:DI 0 "register_operand" "")
+	(zero_extend:DI (match_operand:SI 1 "nonimmediate_operand" "")))]
+  ""
+  {
+    /* In GCC 3.4.1 all SUBREG have changed from 1 to 4 */
+    rtx temp = gen_rtx_SUBREG (SImode,operands[0],4);
+    gen_rtx_CLOBBER (SImode,temp);
+    /*	operands[0] = temp; */
+    emit_insn (gen_zero_extendsidi2_internal(operands[0], operands[1]));
+    DONE;
+  }
+)
+
+(define_insn "zero_extendsidi2_internal"
+  [(set (match_operand:DI 0 "register_operand" "=d,d,d")
+	(zero_extend:DI (match_operand:SI 1 "nonimmediate_operand" "d,R,m")))]
+  ""
+  {
+        if (which_alternative == 0)
+                return "addik\t%D0,%1,0\;addik\t%0,r0,0";
+        else {
+                char *temp2, *temp_final;
+                char *temp = microblaze_move_1word(operands,insn,TRUE);
+
+                operands[2] = gen_rtx_SUBREG(SImode, operands[0],4);
+                temp2 = "\n\taddk\t%D0,r0,%0\;addk\t%0,r0,r0";
+
+                temp_final = (char*) xmalloc(strlen(temp) + strlen(temp2) + 1);
+                strcpy(temp_final,temp);
+                strcat(temp_final,temp2);
+                return temp_final;
+        }
+  }
+  [(set_attr "type"	"no_delay_arith,no_delay_load,no_delay_load")
+  (set_attr "mode"	"SI,SI,SI")
+  (set_attr "length"	"8,8,8")])
+
+
+(define_expand "zero_extendhisi2"
+  [(set (match_operand:SI 0 "register_operand" "")
+	(zero_extend:SI (match_operand:HI 1 "nonimmediate_operand" "")))]
+  ""
+  ""
+)
+
+;; [MB-Delay] Cannot have this load being done in the delay slot 
+
+(define_insn "zero_extendhisi2_internal"
+  [(set (match_operand:SI 0 "register_operand" "=d,d,d")
+	(zero_extend:SI (match_operand:HI 1 "nonimmediate_operand" "d,R,m")))]
+  ""
+  {
+        if (which_alternative == 0)
+                return "andi\t%0,%1,0xffff";
+        else {
+                /*  fprintf(stderr,"move_1word 1 \n"); */
+                return microblaze_move_1word (operands, insn, TRUE);
+        }
+  }
+  [(set_attr "type"	"no_delay_arith,no_delay_load,no_delay_load")
+  (set_attr "mode"	"SI,SI,SI")
+  (set_attr "length"	"8,8,8")])
+
+
+(define_expand "zero_extendqihi2"
+  [(set (match_operand:HI 0 "register_operand" "")
+	(zero_extend:HI (match_operand:QI 1 "nonimmediate_operand" "")))]
+  ""
+  ""
+)
+
+;; [MB-Delay] Cannot have this load being done in the delay slot 
+(define_insn "zero_extendqihi2_internal"
+  [(set (match_operand:HI 0 "register_operand" "=d,d,d")
+	(zero_extend:HI (match_operand:QI 1 "nonimmediate_operand" "d,R,m")))]
+  ""
+  {
+        if (which_alternative == 0)
+                return "andi\t%0,%1,0x00ff";
+        else {
+                return microblaze_move_1word (operands, insn, TRUE);
+        }
+  }
+  [(set_attr "type"	"arith,no_delay_load,no_delay_load")
+  (set_attr "mode"	"HI")
+  (set_attr "length"	"4,8,8")])
+
+(define_expand "zero_extendqisi2"
+  [(set (match_operand:SI 0 "register_operand" "")
+	(zero_extend:SI (match_operand:QI 1 "nonimmediate_operand" "")))]
+  ""
+  ""
+)
+
+;; [MB-Delay] Cannot have this load being done in the delay slot 
+(define_insn "zero_extendqisi2_internal"
+  [(set (match_operand:SI 0 "register_operand" "=d,d,d")
+	(zero_extend:SI (match_operand:QI 1 "nonimmediate_operand" "d,R,m")))]
+  ""
+  {
+        if (which_alternative == 0)
+                return "andi\t%0,%1,0x00ff";
+        else {
+        /*    fprintf(stderr,"move_1_word3 \n");*/
+              return microblaze_move_1word (operands, insn, TRUE);
+        }
+  }
+  [(set_attr "type"	"arith,no_delay_load,no_delay_load")
+  (set_attr "mode"	"SI,SI,SI")
+  (set_attr "length"	"4,8,8")])
+
+
+
+;;----------------------------------------------------------------
+;; Sign extension
+;;----------------------------------------------------------------
+
+;; basic Sign Extend Operations
+
+(define_insn "sext16"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+	(sign_extend:SI (match_operand:HI 1 "register_operand" "d")))]
+  ""
+  "sext16\t%0,%1"
+  [(set_attr "type"	"arith")
+  (set_attr "mode"	"SI")
+  (set_attr "length"	"4")])
+
+(define_insn "sext8"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+	(sign_extend:SI (match_operand:QI 1 "register_operand" "d")))]
+  "(GET_CODE(operands[1]) != MEM)"
+  "sext8\t%0,%1"
+  [(set_attr "type"	"arith")
+  (set_attr "mode"	"SI")
+  (set_attr "length"	"4")])
+
+
+;; Extension insns.  
+;; Those for integer source operand are ordered
+;; widest source type first.
+
+(define_expand "extendsidi2"
+  [(set (match_operand:DI 0 "register_operand" " ")
+	(sign_extend:DI (match_operand:SI 1 "nonimmediate_operand" " ")))]
+  ""
+  { 
+    rtx temp = gen_rtx_SUBREG (SImode,operands[0],4);
+    gen_rtx_CLOBBER (SImode,temp);
+    emit_insn (gen_extendsidi2_internal(operands[0], operands[1]));
+    DONE;
+  }
+)
+
+(define_insn "extendsidi2_internal"
+  [(set (match_operand:DI 0 "register_operand" "=d,d,d")
+	(sign_extend:DI (use (match_operand:SI 1 "nonimmediate_operand" "d,R,m"))))]
+  ""
+  { 
+        char *temp, *temp2, *temp_final;
+        /*    fprintf(stderr,"move_1word - II\n");*/
+        temp = microblaze_move_1word(operands, insn, FALSE);
+        /*     fprintf(stderr,"Sign Extending DI \n");*/
+        /*      operands[2] = gen_rtx(REG,REGNO(operands[0]) + 1);*/
+        operands[2] = gen_rtx_SUBREG(SImode,operands[0],4);
+	temp2 = "%_\;add\t%D0,r0,%0\;add\t%0,%D0,%D0\;addc\t%0,r0,r0\;beqi\t%0,%-signextend\;addi\t%0,r0,0xffffffff\n%-signextend:";
+  	temp_final = (char*) xmalloc(strlen(temp) + strlen(temp2) + 1);
+	strcpy(temp_final,temp);
+	strcat(temp_final,temp2);
+        return temp_final;
+        /*return temp; */
+  }
+  [(set_attr "type"	"multi,multi,multi")
+  (set_attr "mode"	"DI")
+  (set_attr "length"	"20,20,20")])
+
+
+;; Sign extension from QI to DI mode
+
+(define_expand "extendqidi2"
+  [(set (match_operand:DI 0 "register_operand" " ")
+	(sign_extend:DI (use(match_operand:QI 1 "nonimmediate_operand" " "))))]
+  ""
+  { 
+    rtx temp = gen_rtx_SUBREG (SImode,operands[0],4);
+    gen_rtx_CLOBBER (SImode,temp);
+    /*     fprintf (stderr,"In the extendqidi2\n");*/
+    emit_insn (gen_extendqidi2_internal(operands[0], operands[1]));
+    DONE;
+  }
+)
+
+;; Sign extension from QI to DI mode - internal step
+
+(define_insn "extendqidi2_internal"
+  [(set (match_operand:DI 0 "register_operand" "=d,d,d")
+	(sign_extend:DI (use (match_operand:QI 1 "nonimmediate_operand" "d,R,m"))))]
+  ""
+  { 
+        char *temp, *temp2, *temp_final;
+        /*      fprintf(stderr,"move_1word - II\n");*/
+        temp = microblaze_move_1word(operands, insn, FALSE);
+        /*      fprintf(stderr,"Sign Extending DI \n");*/
+        /*      operands[2] = gen_rtx(REG,REGNO(operands[0]) + 1);*/
+        operands[2] = gen_rtx_SUBREG(SImode,operands[0],4);
+
+	temp2 = "%_\n\tsext8\t%0,%0\;add\t%D0,r0,%0\;add\t%0,%D0,%D0\;addc\t%0,r0,r0\;beqi\t%0,%-signextend\;addi\t%0,r0,0xffffffff\n%-signextend:";
+  	temp_final = (char*) xmalloc(strlen(temp) + strlen(temp2) + 1);
+	strcpy(temp_final,temp);
+	strcat(temp_final,temp2);
+        return temp_final;
+        /*	return temp;*/
+  }
+  [(set_attr "type"	"multi,multi,multi")
+  (set_attr "mode"	"DI")
+  (set_attr "length"	"20,20,20")])
+
+
+;; Sign extension from HI to DI mode
+
+(define_expand "extendhidi2"
+  [(set (match_operand:DI 0 "register_operand" " ")
+	(sign_extend:DI (use(match_operand:HI 1 "nonimmediate_operand" " "))))]
+  ""
+  { 
+    rtx temp = gen_rtx_SUBREG(SImode,operands[0],4);
+    gen_rtx_CLOBBER(SImode,temp);
+    emit_insn(gen_extendhidi2_internal(operands[0], operands[1]));
+    DONE;
+  }
+)
+
+;; Sign extension from QI to DI mode - internal step
+
+(define_insn "extendhidi2_internal"
+  [(set (match_operand:DI 0 "register_operand" "=d,d,d")
+	(sign_extend:DI (use (match_operand:HI 1 "nonimmediate_operand" "d,R,m"))))]
+  ""
+  { 
+        char *temp, *temp2, *temp_final;
+        /*      fprintf(stderr,"move_1word - II\n");*/
+        temp = microblaze_move_1word(operands, insn, FALSE);
+        /*      fprintf(stderr,"Sign Extending DI \n");*/
+        /*      operands[2] = gen_rtx(REG,REGNO(operands[0]) + 1);*/
+        operands[2] = gen_rtx_SUBREG(SImode,operands[0],4);
+
+	temp2 = "%_\n\tsext16\t%0,%0\;add\t%D0,r0,%0\;add\t%0,%D0,%D0\;addc\t%0,r0,r0\;beqi\t%0,%-signextend\;addi\t%0,r0,0xffffffff\n%-signextend:";
+  	temp_final = (char*) xmalloc(strlen(temp) + strlen(temp2) + 1);
+	strcpy(temp_final,temp);
+	strcat(temp_final,temp2);
+        return temp_final;
+        /*	return temp; */
+  }
+  [(set_attr "type"	"multi,multi,multi")
+  (set_attr "mode"	"DI")
+  (set_attr "length"	"20,20,20")])
+
+;; Should change this extend to have check on the MSB and then
+;; appropriate OR
+
+(define_expand "extendhisi2"
+  [(set (match_operand:SI 0 "register_operand" "")
+	(sign_extend:SI (match_operand:HI 1 "nonimmediate_operand" "")))]
+  ""
+  {
+    /* earlier this was being done only for optimize. Removed the optimize
+       stuff from here */
+    /*  if (optimize && GET_CODE (operands[1]) == MEM)*/
+    if (GET_CODE (operands[1]) == MEM)
+        operands[1] = force_not_mem (operands[1]);
+  }
+)
+
+
+;; Need to change this similar lines as the HI->SI conversion
+
+(define_expand "extendqihi2"
+  [(set (match_operand:HI 0 "register_operand" "")
+	(sign_extend:HI (match_operand:QI 1 "nonimmediate_operand" "")))]
+  ""
+  {
+    if (GET_CODE (operands[1]) == MEM)
+        operands[1] = force_not_mem (operands[1]);
+    {
+        rtx op0   = gen_lowpart (SImode, operands[0]);
+        rtx op1   = gen_lowpart (SImode, operands[1]);
+        rtx lowpart = GEN_INT(0x0000ffff);
+
+        emit_insn (gen_sext8(op0,operands[1]));
+        emit_insn (gen_andsi3(op1,op0,lowpart));
+        emit_insn (gen_blockage ());
+        DONE;
+    }
+}
+)
+
+
+(define_expand "extendqisi2"
+  [(set (match_operand:SI 0 "register_operand" "")
+	(sign_extend:SI (match_operand:QI 1 "nonimmediate_operand" "")))]
+  ""
+  {
+    if (GET_CODE (operands[1]) == MEM)
+        operands[1] = force_not_mem (operands[1]);
+    emit_insn (gen_sext8 (operands[0], operands[1]));	
+    DONE;
+  }
+)
+
+
+
+;;----------------------------------------------------------------
+;; Data movement
+;;----------------------------------------------------------------
+
+;; 64-bit integer moves
+
+;; Unlike most other insns, the move insns can't be split with
+;; different predicates, because register spilling and other parts of
+;; the compiler, have memoized the insn number already.
+
+(define_expand "movdi"
+  [(set (match_operand:DI 0 "nonimmediate_operand" "")
+	(match_operand:DI 1 "general_operand" ""))]
+  ""
+  {
+    /* If we are generating embedded PIC code, and we are referring to a
+       symbol in the .text section, we must use an offset from the start
+       of the function.  */
+    if (TARGET_EMBEDDED_PIC
+        && (GET_CODE (operands[1]) == LABEL_REF
+	|| (GET_CODE (operands[1]) == SYMBOL_REF
+	&& ! SYMBOL_REF_FLAG (operands[1]))))
+    {
+        rtx temp;
+        temp = embedded_pic_offset (operands[1]);
+        temp = gen_rtx (PLUS, Pmode, embedded_pic_fnaddr_rtx,
+	                force_reg (DImode, temp));
+        emit_move_insn (operands[0], force_reg (DImode, temp));
+        DONE;
+    }
+
+    /* If operands[1] is a constant address illegal for pic, then we need to
+       handle it just like LEGITIMIZE_ADDRESS does.  */
+    if (flag_pic && pic_address_needs_scratch (operands[1]))
+    {
+        rtx temp = force_reg (DImode, XEXP (XEXP (operands[1], 0), 0));
+        rtx temp2 = XEXP (XEXP (operands[1], 0), 1);
+        /* if (! SMALL_INT (temp2))
+	temp2 = force_reg (DImode, temp2);
+        */
+        emit_move_insn (operands[0], gen_rtx (PLUS, DImode, temp, temp2));
+        DONE;
+    }
+
+
+    if ((reload_in_progress | reload_completed) == 0
+        && !register_operand (operands[0], DImode)
+        && !register_operand (operands[1], DImode)
+        && (((GET_CODE (operands[1]) != CONST_INT || INTVAL (operands[1]) != 0)
+	       && operands[1] != CONST0_RTX (DImode))))
+    {
+
+      rtx temp = force_reg (DImode, operands[1]);
+      emit_move_insn (operands[0], temp);
+      DONE;
+    }
+  }
+)
+
+
+
+(define_insn "movdi_internal"
+  [(set (match_operand:DI 0 "nonimmediate_operand" "=d,d ,d,d,R,m")
+	(match_operand:DI 1 "general_operand"      " d,iF,R,m,d,d"))]
+  "(register_operand (operands[0], DImode)
+       || register_operand (operands[1], DImode)
+       || (GET_CODE (operands[1]) == CONST_INT && INTVAL (operands[1]) == 0)
+       || operands[1] == CONST0_RTX (DImode))"
+  { 
+        /*        fprintf(stderr,"move_2words-I\n");*/
+	return microblaze_move_2words (operands, insn); 
+  }
+  [(set_attr "type"	"no_delay_move,no_delay_arith,no_delay_load,no_delay_load,no_delay_store,no_delay_store")
+  (set_attr "mode"	"DI")
+  (set_attr "length"   "2,4,2,4,2,4")])
+
+;;(define_insn "movdi_internal_2"
+;;  [(set (match_operand:DI 0 "nonimmediate_operand" "")
+;;	(match_operand:DI 1 "general_operand" ""))]
+;;  ""
+;;  { 
+;;      fprintf(stderr,"move_2words-II\n");
+;;	return microblaze_move_2words (operands, insn); 
+;;  }
+;;  [(set_attr "type"	"no_delay_move")
+;;   (set_attr "mode"	"DI")
+;;   (set_attr "length"   "2")])
+
+(define_split
+  [(set (match_operand:DI 0 "register_operand" "")
+	(match_operand:DI 1 "register_operand" ""))]
+  "reload_completed && !TARGET_DEBUG_D_MODE && !TARGET_DEBUG_G_MODE
+   && GET_CODE (operands[0]) == REG && GP_REG_P (REGNO (operands[0]))
+   && GET_CODE (operands[1]) == REG && GP_REG_P (REGNO (operands[1])) 
+   && (REGNO(operands[0]) == (REGNO(operands[1]) + 1))"
+
+  [(set (subreg:SI (match_dup 0) 4) (subreg:SI (match_dup 1) 4))
+  (set (subreg:SI (match_dup 0) 0) (subreg:SI (match_dup 1) 0))]
+  "")
+
+(define_split
+  [(set (match_operand:DI 0 "register_operand" "")
+	(match_operand:DI 1 "register_operand" ""))]
+  "reload_completed && !TARGET_DEBUG_D_MODE && !TARGET_DEBUG_G_MODE
+   && GET_CODE (operands[0]) == REG && GP_REG_P (REGNO (operands[0]))
+   && GET_CODE (operands[1]) == REG && GP_REG_P (REGNO (operands[1])) 
+   && (REGNO(operands[0]) != (REGNO(operands[1]) + 1))"
+
+  [(set (subreg:SI (match_dup 0) 0) (subreg:SI (match_dup 1) 0))
+  (set (subreg:SI (match_dup 0) 4) (subreg:SI (match_dup 1) 4))]
+  "")
+
+
+;; ;; 32-bit Integer moves
+;;  This looks quite bogus.
+;; (define_split
+;;   [(set (match_operand:SI 0 "register_operand" "")
+;; 	(match_operand:SI 1 "large_int" ""))]
+;;   "!TARGET_DEBUG_D_MODE "
+;;   [(set (match_dup 0)
+;; 	(match_dup 2))
+;;   (set (match_dup 0)
+;;        (ior:SI (match_dup 0)
+;;                (match_dup 3)))]
+;;   "
+;; {
+;;   operands[2] = GEN_INT (INTVAL (operands[1]) & 0xffff0000);
+;;   operands[3] = GEN_INT (INTVAL (operands[1]) & 0x0000ffff);
+;; }")
+
+;; Unlike most other insns, the move insns can't be split with
+;; different predicates, because register spilling and other parts of
+;; the compiler, have memoized the insn number already.
+
+(define_expand "movsi"
+  [(set (match_operand:SI 0 "nonimmediate_operand" "")
+	(match_operand:SI 1 "general_operand" ""))]
+  ""
+  {
+
+    /* If we are generating embedded PIC code, and we are referring to a
+       symbol in the .text section, we must use an offset from the start
+       of the function.  */
+    if (TARGET_EMBEDDED_PIC
+        && (GET_CODE (operands[1]) == LABEL_REF
+	    || (GET_CODE (operands[1]) == SYMBOL_REF
+	        && ! SYMBOL_REF_FLAG (operands[1]))))
+    {
+        rtx temp;
+
+        temp = embedded_pic_offset (operands[1]);
+        temp = gen_rtx (PLUS, Pmode, embedded_pic_fnaddr_rtx,
+	                force_reg (SImode, temp));
+        emit_move_insn (operands[0], force_reg (SImode, temp));
+        DONE;
+    }
+
+    /* If operands[1] is a constant address invalid for pic, then we need to
+       handle it just like LEGITIMIZE_ADDRESS does.  */
+    if (flag_pic && pic_address_needs_scratch (operands[1]))
+    {
+        rtx temp = force_reg (SImode, XEXP (XEXP (operands[1], 0), 0));
+        rtx temp2 = XEXP (XEXP (operands[1], 0), 1);
+
+        /* if (! SMALL_INT (temp2))
+	   temp2 = force_reg (SImode, temp2);
+        */
+        emit_move_insn (operands[0], gen_rtx (PLUS, SImode, temp, temp2));
+        DONE;
+    }
+
+    if ((reload_in_progress | reload_completed) == 0
+        && !register_operand (operands[0], SImode)
+        && !register_operand (operands[1], SImode)
+        && (GET_CODE (operands[1]) != CONST_INT
+	    || INTVAL (operands[1]) != 0))
+    {
+        rtx temp = force_reg (SImode, operands[1]);
+        emit_move_insn (operands[0], temp);
+        DONE;
+    }
+  }
+)
+
+
+;; This is for the memory accessed using small data pointer to be moved in the delay slot.
+;;(define_insn "movsi_internal4"
+;;  [(set (match_operand:SI 0 "nonimmediate_operand" "=d,R")
+;;	(match_operand:SI 1 "move_operand" "R,d"))]
+;;  "((register_operand (operands[0], SImode) && (
+;;           (VAR_SECTION (XEXP(operands[1],0)) == SDATA_VAR) ||
+;;           (VAR_SECTION (XEXP(operands[1],0)) == SBSS_VAR) ||	
+;;           (VAR_SECTION (XEXP(operands[1],0)) == SDATA2_VAR)
+;;	)) ||
+;;     (register_operand (operands[1], SImode) && (
+;;           (VAR_SECTION (XEXP(operands[0],0)) == SDATA_VAR) ||
+;;           (VAR_SECTION (XEXP(operands[0],0)) == SBSS_VAR) ||	
+;;           (VAR_SECTION (XEXP(operands[0],0)) == SDATA2_VAR)
+;;	))
+;;    )"  
+;;  { 
+;;	 return microblaze_move_1word (operands, insn, FALSE);
+;;  }
+;;  [(set_attr "type"	"load,store")
+;;   (set_attr "mode"	"SI")
+;;   (set_attr "length"	"4")])
+
+;; Added for status resgisters 
+(define_insn "movsi_status"
+  [(set (match_operand:SI 0 "register_operand" "=d,d,z")
+        (match_operand:SI 1 "register_operand" "z,d,d"))]
+  "interrupt_handler"
+  "@
+	mfs\t%0,%1  #mfs
+	addk\t%0,%1,r0 #add movsi
+	mts\t%0,%1  #mts"	
+  [(set_attr "type" "move")
+  (set_attr "mode" "SI")
+  (set_attr "length" "12")])
+
+;; This move will be not be moved to delay slot.	
+(define_insn "movsi_internal3"
+  [(set (match_operand:SI 0 "nonimmediate_operand" "=d,d")
+	(match_operand:SI 1 "immediate_operand" "IKL,Mnis"))]
+  "(register_operand (operands[0], SImode) && 
+           (GET_CODE (operands[1]) == CONST_INT && 
+                 (INTVAL (operands[1]) <= 32767 && INTVAL(operands[1]) >= -32768)))"  
+  { 
+	 return microblaze_move_1word (operands, insn, FALSE);
+  }
+  [(set_attr "type"	"arith,no_delay_arith")
+  (set_attr "mode"	"SI")
+  (set_attr "length"	"4")])
+
+(define_insn "movsi_internal2"
+  [(set (match_operand:SI 0 "nonimmediate_operand" "=d,d,d,d, d,   d,d,T ,R ")
+	(match_operand:SI 1 "move_operand"         " d,S,K,IL,Mnis,R,m,dJ,dJ"))]
+  "!TARGET_DEBUG_H_MODE
+   && (register_operand (operands[0], SImode)
+       || register_operand (operands[1], SImode) 
+      || (GET_CODE (operands[1]) == CONST_INT && INTVAL (operands[1]) == 0))"
+  { 
+	 return microblaze_move_1word (operands, insn, FALSE);
+  }
+  [(set_attr "type"	"load,no_delay_load,load,no_delay_load,no_delay_load,no_delay_load,no_delay_load,no_delay_store,store")
+  (set_attr "mode"	"SI")
+  (set_attr "length"	"4,8,4,8,8,4,8,4,8")])
+
+
+;; 16-bit Integer moves
+
+;; Unlike most other insns, the move insns can't be split with
+;; different predicates, because register spilling and other parts of
+;; the compiler, have memoized the insn number already.
+;; Unsigned loads are used because BYTE_LOADS_ZERO_EXTEND is defined
+
+(define_expand "movhi"
+  [(set (match_operand:HI 0 "nonimmediate_operand" "")
+	(match_operand:HI 1 "general_operand" ""))]
+  ""
+  {
+    if ((reload_in_progress | reload_completed) == 0
+        && !register_operand (operands[0], HImode)
+        && !register_operand (operands[1], HImode)
+        && ((GET_CODE (operands[1]) != CONST_INT
+  	    || INTVAL (operands[1]) != 0)))
+    {
+        rtx temp = force_reg (HImode, operands[1]);
+        emit_move_insn (operands[0], temp);
+        DONE;
+    }
+  }
+)
+
+;; Defined to be the only movhi instruction, which can be placed in a
+;; delay slot. Small integers
+
+(define_insn "movhi_internal3"
+  [(set (match_operand:HI 0 "nonimmediate_operand" "=d")
+	(match_operand:HI 1 "immediate_operand"       "IK"))]
+  "(register_operand (operands[0], HImode) && 
+           (GET_CODE (operands[1]) == CONST_INT && 
+                 (INTVAL (operands[1]) <= 32767 && INTVAL(operands[1]) >= -32768)))"
+  {
+	return microblaze_move_1word (operands, insn, TRUE);
+  }
+  [(set_attr "type"	"arith")
+  (set_attr "mode"	"HI")
+  (set_attr "length"	"4")])
+
+;; The only movhi instruction, which can be placed in a
+;; delay slot is the move..rest cannot be moved to delay slot. For the time being.
+
+(define_insn "movhi_internal2"
+  [(set (match_operand:HI 0 "nonimmediate_operand" "=d,d,d,d,R,m")
+	(match_operand:HI 1 "general_operand"       "d,IK,R,m,dJ,dJ"))]
+  "(register_operand (operands[0], HImode)
+       || register_operand (operands[1], HImode)
+       || (GET_CODE (operands[1]) == CONST_INT && INTVAL (operands[1]) == 0))"
+  {
+	return microblaze_move_1word (operands, insn, TRUE);
+  }
+  [(set_attr "type"	"move,no_delay_arith,no_delay_load,no_delay_load,no_delay_store,no_delay_store")
+  (set_attr "mode"	"HI")
+  (set_attr "length"	"4,8,8,8,8,8")])
+
+;; 8-bit Integer moves
+
+;; Unlike most other insns, the move insns can't be split with
+;; different predicates, because register spilling and other parts of
+;; the compiler, have memoized the insn number already.
+;; Unsigned loads are used because BYTE_LOADS_ZERO_EXTEND is defined
+
+(define_expand "movqi"
+  [(set (match_operand:QI 0 "nonimmediate_operand" "")
+	(match_operand:QI 1 "general_operand" ""))]
+  ""
+  {
+    if ((reload_in_progress | reload_completed) == 0
+        && !register_operand (operands[0], QImode)
+        && !register_operand (operands[1], QImode)
+        && ((GET_CODE (operands[1]) != CONST_INT
+            || INTVAL (operands[1]) != 0)))
+    {
+        rtx temp = force_reg (QImode, operands[1]);
+        emit_move_insn (operands[0], temp);
+        DONE;
+    }
+  }
+)
+
+;; Defined to be the only movhi instruction, which can be placed in a
+;; delay slot. Small integers
+
+
+(define_insn "movqi_internal3"
+  [(set (match_operand:QI 0 "nonimmediate_operand" "=d")
+	(match_operand:QI 1 "immediate_operand"       "IK"))]
+  "(register_operand (operands[0], QImode) && 
+           (GET_CODE (operands[1]) == CONST_INT && 
+                 (INTVAL (operands[1]) <= 32767 && INTVAL(operands[1]) >= -32768)))"
+  {
+        /*fprintf(stderr,"move_1word - 8\n");*/ 
+        return microblaze_move_1word (operands, insn, TRUE);
+  }
+  [(set_attr "type"	"arith")
+  (set_attr "mode"	"QI")
+  (set_attr "length"	"8")])
+
+;; The only movqi instruction, which can be placed in a
+;; delay slot is the move..rest cannot be moved to delay slot. For the time being.
+
+(define_insn "movqi_internal2"
+  [(set (match_operand:QI 0 "nonimmediate_operand" "=d,d,d,d,R,m")
+	(match_operand:QI 1 "general_operand"       "d,IK,R,m,dJ,dJ"))]
+  "(register_operand (operands[0], QImode)
+       || register_operand (operands[1], QImode)
+       || (GET_CODE (operands[1]) == CONST_INT && INTVAL (operands[1]) == 0))"
+  {
+        /*fprintf(stderr,"move_1word - 8\n");*/ 
+        return microblaze_move_1word (operands, insn, TRUE);
+  }
+  [(set_attr "type"	"move,no_delay_arith,no_delay_load,no_delay_load,no_delay_store,no_delay_store")
+  (set_attr "mode"	"QI")
+  (set_attr "length"	"8,8,8,8,8,8")])
+
+
+
+;; 32-bit floating point moves
+
+(define_expand "movsf"
+  [(set (match_operand:SF 0 "nonimmediate_operand" "")
+        (match_operand:SF 1 "general_operand" ""))]
+  ""
+  {
+    if ((reload_in_progress | reload_completed) == 0
+        && !register_operand (operands[0], SFmode)
+        && !register_operand (operands[1], SFmode)
+        && ( ((GET_CODE (operands[1]) != CONST_INT || INTVAL (operands[1]) != 0)
+                 && operands[1] != CONST0_RTX (SFmode))))
+    {
+        rtx temp = force_reg (SFmode, operands[1]);
+        emit_move_insn (operands[0], temp);
+        DONE;
+    }
+  }
+)
+
+;; (define_insn "movsf_internal1"
+;;   [(set (match_operand:SF 0 "nonimmediate_operand" "=d,d,d,R,m,d")
+;;      (match_operand:SF 1 "general_operand" "d,R,m,d,d,*G"))]
+;;         "TARGET_HARD_FLOAT && (register_operand (operands[0], SFmode)
+;;        || register_operand (operands[1], SFmode)
+;;        || (GET_CODE (operands[1]) == CONST_INT && INTVAL (operands[1]) == 0)
+;;        || operands[1] == CONST0_RTX (SFmode))"
+;;   { return microblaze_move_1word (operands, insn, FALSE);}
+;;   [(set_attr "type"  "move,no_delay_load,no_delay_load,no_delay_store,no_delay_store,no_delay_move")
+;;   (set_attr "mode"   "SF")
+;;   (set_attr "length" "4,4,4,4,4,4")])
+
+;; movsf_internal
+;; Applies to both TARGET_SOFT_FLOAT and TARGET_HARD_FLOAT
+;;
+(define_insn "movsf_internal"
+  [(set (match_operand:SF 0 "nonimmediate_operand" "=d,d,d,R,m")
+        (match_operand:SF 1 "general_operand" "Gd,R,Fm,d,d"))]
+  "(register_operand (operands[0], SFmode)
+       || register_operand (operands[1], SFmode)
+       || (GET_CODE (operands[1]) == CONST_INT && INTVAL (operands[1]) == 0)
+       || operands[1] == CONST0_RTX (SFmode))"
+  { 
+        return microblaze_move_1word (operands, insn, FALSE);
+  }
+  [(set_attr "type"     "move,no_delay_load,no_delay_load,no_delay_store,no_delay_store")
+  (set_attr "mode"      "SF")
+  (set_attr "length"    "4,4,4,4,4")])
+
+;; 64-bit floating point moves
+(define_expand "movdf"
+  [(set (match_operand:DF 0 "nonimmediate_operand" "")
+        (match_operand:DF 1 "general_operand" ""))]
+  ""
+  {
+    if ((reload_in_progress | reload_completed) == 0
+        && !register_operand (operands[0], DFmode)
+        && !register_operand (operands[1], DFmode)
+        && (((GET_CODE (operands[1]) != CONST_INT || INTVAL (operands[1]) != 0)
+                 && operands[1] != CONST0_RTX (DFmode))))
+    {
+        rtx temp = force_reg (DFmode, operands[1]);
+        emit_move_insn (operands[0], temp);
+        DONE;
+    }
+  }
+)
+
+;; (define_insn "movdf_internal1"
+;;   [(set (match_operand:DF 0 "general_operand" "=d,d,m,d")
+;;      (match_operand:DF 1 "general_operand" "d,m,d,*G"))]
+;;   "TARGET_HARD_FLOAT"
+;;   { 
+;;       return microblaze_move_2words (operands, insn);
+;;   }
+;;   [(set_attr "type"  "no_delay_move,no_delay_load,no_delay_store,no_delay_move")
+;;   (set_attr "mode"   "DF")
+;;   (set_attr "length" "4,4,4,4")])
+
+;; movdf_internal
+;; Applies to both TARGET_SOFT_FLOAT and TARGET_HARD_FLOAT
+;;
+(define_insn "movdf_internal"
+  [(set (match_operand:DF 0 "nonimmediate_operand" "=d,d,d,R,To")
+        (match_operand:DF 1 "general_operand" "dG,R,ToF,d,d"))]
+  "(register_operand (operands[0], DFmode)
+       || register_operand (operands[1], DFmode)
+       || (GET_CODE (operands[1]) == CONST_INT && INTVAL (operands[1]) == 0)
+       || operands[1] == CONST0_RTX (DFmode))"
+  {
+        return microblaze_move_2words (operands, insn);
+  }
+  [(set_attr "type"     "no_delay_move,no_delay_arith,no_delay_load,no_delay_store,no_delay_store")
+  (set_attr "mode"      "DF")
+  (set_attr "length"    "4,4,8,4,8")])
+
+(define_split
+  [(set (match_operand:DF 0 "register_operand" "")
+        (match_operand:DF 1 "register_operand" ""))]
+  "reload_completed && !TARGET_DEBUG_D_MODE && !TARGET_DEBUG_G_MODE
+   && GET_CODE (operands[0]) == REG && GP_REG_P (REGNO (operands[0]))
+   && GET_CODE (operands[1]) == REG && GP_REG_P (REGNO (operands[1]))
+   && (REGNO(operands[0]) == (REGNO(operands[1]) + 1))"
+  [(set (subreg:SI (match_dup 0) 4) (subreg:SI (match_dup 1) 4))
+  (set (subreg:SI (match_dup 0) 0) (subreg:SI (match_dup 1) 0))]
+  "")
+
+(define_split
+  [(set (match_operand:DF 0 "register_operand" "")
+        (match_operand:DF 1 "register_operand" ""))]
+  "reload_completed && !TARGET_DEBUG_D_MODE && !TARGET_DEBUG_G_MODE
+   && GET_CODE (operands[0]) == REG && GP_REG_P (REGNO (operands[0]))
+   && GET_CODE (operands[1]) == REG && GP_REG_P (REGNO (operands[1]))
+   && (REGNO(operands[0]) != (REGNO(operands[1]) + 1))"
+  [(set (subreg:SI (match_dup 0) 0) (subreg:SI (match_dup 1) 0))
+  (set (subreg:SI (match_dup 0) 4) (subreg:SI (match_dup 1) 4))]
+  "")
+
+;; Block moves, see microblaze.c for more details.
+;; Argument 0 is the destination
+;; Argument 1 is the source
+;; Argument 2 is the length
+;; Argument 3 is the alignment
+
+(define_expand "movstrsi"
+  [(parallel [(set (match_operand:BLK 0 "general_operand" "")
+		   (match_operand:BLK 1 "general_operand" ""))
+             (use (match_operand:SI 2 "arith_operand" ""))
+             (use (match_operand:SI 3 "immediate_operand" ""))])]
+  ""
+  {
+    if (operands[0])		/* avoid unused code messages */
+    {
+        expand_block_move (operands);
+        DONE;
+    }
+  }
+)
+
+;; Insn generated by block moves
+
+(define_insn "movstrsi_internal"
+  [(set (match_operand:BLK 0 "memory_operand" "=o") ;; destination
+	(match_operand:BLK 1 "memory_operand" "o")) ;; source
+  (clobber (match_scratch:SI 4 "=&d")) ;; temp 1
+  (clobber (match_scratch:SI 5 "=&d")) ;; temp 2
+  (clobber (match_scratch:SI 6 "=&d")) ;; temp 3
+  (clobber (match_scratch:SI 7 "=&d")) ;; temp 4
+  (use (match_operand:SI 2 "small_int" "I")) ;; # bytes to move
+  (use (match_operand:SI 3 "small_int" "I")) ;; alignment
+  (use (const_int 0))] ;; normal block move
+  ""
+  {
+	 return output_block_move (insn, operands, 4, BLOCK_MOVE_NORMAL);
+  }
+  [(set_attr "type"	"no_delay_store")
+  (set_attr "mode"	"none")
+  (set_attr "length"	"20")])
+
+;; Split a block move into 2 parts, the first part is everything
+;; except for the last move, and the second part is just the last
+;; store, which is exactly 1 instruction (ie, not a usw), so it can
+;; fill a delay slot.  This also prevents a bug in delayed branches
+;; from showing up, which reuses one of the registers in our clobbers.
+
+(define_split
+  [(set (mem:BLK (match_operand:SI 0 "register_operand" ""))
+	(mem:BLK (match_operand:SI 1 "register_operand" "")))
+  (clobber (match_operand:SI 4 "register_operand" ""))
+  (clobber (match_operand:SI 5 "register_operand" ""))
+  (clobber (match_operand:SI 6 "register_operand" ""))
+  (clobber (match_operand:SI 7 "register_operand" ""))
+  (use (match_operand:SI 2 "small_int" ""))
+  (use (match_operand:SI 3 "small_int" ""))
+  (use (const_int 0))]
+
+  "reload_completed && !TARGET_DEBUG_D_MODE && INTVAL (operands[2]) > 0"
+
+  ;; All but the last move
+  [(parallel [(set (mem:BLK (match_dup 0))
+		   (mem:BLK (match_dup 1)))
+             (clobber (match_dup 4))
+             (clobber (match_dup 5))
+             (clobber (match_dup 6))
+             (clobber (match_dup 7))
+             (use (match_dup 2))
+             (use (match_dup 3))
+             (use (const_int 1))])
+
+  ;; The last store, so it can fill a delay slot
+  (parallel [(set (mem:BLK (match_dup 0))
+                  (mem:BLK (match_dup 1)))
+            (clobber (match_dup 4))
+            (clobber (match_dup 5))
+            (clobber (match_dup 6))
+            (clobber (match_dup 7))
+            (use (match_dup 2))
+            (use (match_dup 3))
+            (use (const_int 2))])]
+
+  "")
+
+(define_insn "movstrsi_internal2"
+  [(set (match_operand:BLK 0 "memory_operand" "=o") ;; destination
+	(match_operand:BLK 1 "memory_operand" "o")) ;; source
+  (clobber (match_scratch:SI 4 "=&d")) ;; temp 1
+  (clobber (match_scratch:SI 5 "=&d")) ;; temp 2
+  (clobber (match_scratch:SI 6 "=&d")) ;; temp 3
+  (clobber (match_scratch:SI 7 "=&d")) ;; temp 4
+  (use (match_operand:SI 2 "small_int" "I")) ;; # bytes to move
+  (use (match_operand:SI 3 "small_int" "I")) ;; alignment
+  (use (const_int 1))] ;; all but last store
+  ""
+  {
+     return output_block_move (insn, operands, 4, BLOCK_MOVE_NOT_LAST);
+  } 
+  [(set_attr "type"	"no_delay_store")
+  (set_attr "mode"	"none")
+  (set_attr "length"	"20")])
+
+
+(define_insn "movstrsi_internal3"
+  [(set (match_operand:BLK 0 "memory_operand" "=Ro") ;; destination
+	(match_operand:BLK 1 "memory_operand" "Ro")) ;; source
+  (clobber (match_scratch:SI 4 "=&d")) ;; temp 1
+  (clobber (match_scratch:SI 5 "=&d")) ;; temp 2
+  (clobber (match_scratch:SI 6 "=&d")) ;; temp 3
+  (clobber (match_scratch:SI 7 "=&d")) ;; temp 4
+  (use (match_operand:SI 2 "small_int" "I")) ;; # bytes to move
+  (use (match_operand:SI 3 "small_int" "I")) ;; alignment
+  (use (const_int 2))] ;; just last store of block move
+  ""
+  {
+      return output_block_move (insn, operands, 4, BLOCK_MOVE_LAST);
+  }
+  [(set_attr "type"	"no_delay_store")
+  (set_attr "mode"	"none")
+  (set_attr "length"	"20")])
+
+
+
+
+;;----------------------------------------------------------------
+;; Shifts
+;;----------------------------------------------------------------
+
+
+
+;;----------------------------------------------------------------
+;; 32-bit left shifts
+;;----------------------------------------------------------------
+(define_expand "ashlsi3"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+	(ashift:SI (match_operand:SI 1 "register_operand" "d")
+		   (match_operand:SI 2 "arith_operand" "")))]
+  ""
+  {
+    if (TARGET_BARREL_SHIFT) {
+        emit_insn (gen_ashlsi3_bshift (operands[0],operands[1],operands[2]));
+        DONE;
+    }
+  }
+)
+
+;; Irrespective of if we have a barrel-shifter or not, we want to match shifts by 1 with a special 
+;; pattern. When a barrel shifter is present, saves a cycle. If not, allows us to annotate the
+;; instruction for delay slot optimization
+(define_insn "ashlsi3_byone"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+	(ashift:SI (match_operand:SI 1 "register_operand" "d")
+                   (match_operand:SI 2 "arith_operand"    "I")))] 
+  "(INTVAL(operands[2]) == 1)"
+  {
+    return "addk\t%0,%1,%1\;";
+  }
+  [(set_attr "type"	"arith")
+   (set_attr "mode"	"SI")
+   (set_attr "length"	"4")]
+)
+
+;; Barrel shift left
+(define_insn "ashlsi3_bshift"
+  [(set (match_operand:SI 0 "register_operand" "=d,d")
+	(ashift:SI (match_operand:SI 1 "register_operand" "d,d")
+                   (match_operand:SI 2 "arith_operand"    "I,d")))]
+  "TARGET_BARREL_SHIFT"
+  {
+    if (which_alternative == 0) {
+        return "bslli\t%0,%1,%2";
+    } else {                                    /* Shift amount in a register. No need to mask the lower 5 bits out. MB bshift ignores rB[0-26] */
+        return "bsll\t%0,%1,%2";
+    }
+  }
+  [(set_attr "type"	"bshift,bshift")
+  (set_attr "mode"	"SI,SI")
+  (set_attr "length"	"4,4")]
+)
+
+;; The following patterns apply when there is no barrel shifter present
+
+
+;; Why should the backend do this? Shouldn't RTX costs help guide the 
+;; base compiler to the advantage of using hard multiplies by itself?
+
+(define_insn "ashlsi3_with_mul_delay"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+	(ashift:SI (match_operand:SI 1 "register_operand"  "d")
+                   (match_operand:SI 2 "immediate_operand" "I")))] 
+  "!TARGET_SOFT_MUL && ((1 << INTVAL (operands[2])) <= 32767 && (1 << INTVAL (operands[2])) >= -32768)"
+  {
+    int mul = (1 << INTVAL(operands[2]));
+    char *asminsn = (char *)xmalloc (50);
+    if (INTVAL (operands[2]) == 0) {        /* We might get this scenario at -O0 ! (Verify)*/
+        strcpy (asminsn, "addk\t%0,%1,r0\;");
+        return asminsn;
+    }
+
+    sprintf (asminsn, "muli\t%%0,%%1,%d", mul);
+    return asminsn;
+  }
+  [(set_attr "type"	"arith")                        ;; This MUL will not generate an imm. Can go into a delay slot.
+   (set_attr "mode"	"SI")
+   (set_attr "length"	"4")]
+)
+
+(define_insn "ashlsi3_with_mul_nodelay"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+	(ashift:SI (match_operand:SI 1 "register_operand"  "d")
+                   (match_operand:SI 2 "immediate_operand" "I")))] 
+  "!TARGET_SOFT_MUL"
+  {
+    int mul = (1 << INTVAL(operands[2]));
+    char *asminsn = (char *)xmalloc (50);
+    sprintf (asminsn, "muli\t%%0,%%1,%d", mul);
+    return asminsn;
+  }
+  [(set_attr "type"	"no_delay_arith")               ;; This MUL will generate an IMM. Cannot go into a delay slot
+   (set_attr "mode"	"SI")
+   (set_attr "length"	"8")]
+)
+
+(define_insn "ashlsi3_with_rotate"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+	(ashift:SI (match_operand:SI 1 "register_operand"  "d")
+                   (match_operand:SI 2 "immediate_operand" "I")))] 
+  "(INTVAL(operands[2]) > 17 && !optimize_size)"
+  {
+    unsigned int mask1;
+    char *asminsn = (char *)xmalloc (400);
+    int i, nshift;
+
+    nshift = INTVAL (operands[2]);
+    mask1 = (0xFFFFFFFF << nshift);
+    
+    /* We do one extra shift so that the first bit (carry) coming into the MSB will be masked out
+     */
+    strcpy (asminsn, "src\t%0,%1\;");           
+    for (i = 0; i < (32 - nshift); i++) 
+       strcat (asminsn, "src\t%0,%0\;");
+
+    sprintf ((char*)(asminsn + strlen(asminsn)), "andi\t%%0,%%0,0x%x\;", mask1);
+    return asminsn;
+  }
+  [(set_attr "type"	"multi")
+  (set_attr "mode"	"SI")
+  (set_attr "length"	"80")]
+)
+
+(define_insn "ashlsi3_with_size_opt"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+	(ashift:SI (match_operand:SI 1 "register_operand"  "d")
+                   (match_operand:SI 2 "immediate_operand" "I")))] 
+  "(INTVAL(operands[2]) > 5 && optimize_size)"
+  {
+    char *asminsn = (char *)xmalloc (200);
+    strcpy (asminsn, "%_ori\tr18,r0,%2\;");
+    if (REGNO (operands[0]) != REGNO (operands[1]))
+        strcat (asminsn, "addk\t%0,%1,r0\n");
+    else 
+        strcat (asminsn, "\n");
+    strcat (asminsn, "%-ashl_loop:\n\taddik\tr18,r18,-1\;bneid\tr18,%-ashl_loop\;addk\t%0,%0,%0\;");  
+    return asminsn;
+  }
+  [(set_attr "type"	"multi")
+   (set_attr "mode"	"SI")
+   (set_attr "length"	"40")]
+)
+
+(define_insn "ashlsi3_serial"
+  [(set (match_operand:SI 0 "register_operand" "=d,d")
+	(ashift:SI (match_operand:SI 1 "register_operand" "d,d")
+                   (match_operand:SI 2 "arith_operand"    "I,d")))] 
+  ""
+  {
+    if (which_alternative == 0) { 
+        int i;
+        char *asminsn = xmalloc (25 * 32);
+        if (INTVAL (operands[2]) == 0) {        /* We are getting this scenario at -O0 ! */
+            if (REGNO (operands[0]) != REGNO (operands[1]))
+                strcpy (asminsn, "addk\t%0,%1,r0\;");
+            else
+                strcpy (asminsn, "\n");
+            return asminsn;
+        }
+        strcpy (asminsn, "addk\t%0,%1,%1\;");
+        for (i = 0; i < INTVAL (operands[2]) - 1; i++) 
+            strcat (asminsn, "addk\t%0,%0,%0\;");
+        return asminsn;       
+    } else {                                            /* Shift amount in a register */
+        return "%_andi\tr18,%2,31\;beqid\tr18,%-ashl_out\;addk\t%0,%1,r0\;%-ashl_loop:\n\taddik\tr18,r18,-1\;bneid\tr18,%-ashl_loop\;addk\t%0,%0,%0\n%-ashl_out:\n";  
+    }
+  }
+  [(set_attr "type"	"multi,multi")
+   (set_attr "mode"	"SI,SI")
+   (set_attr "length"	"124,40")]
+)
+
+;;----------------------------------------------------------------
+;; 32-bit right shifts
+;;----------------------------------------------------------------
+(define_expand "ashrsi3"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+	(ashiftrt:SI (match_operand:SI 1 "register_operand" "d")
+                     (match_operand:SI 2 "arith_operand" "")))]
+  ""
+  {
+    if (TARGET_BARREL_SHIFT) {
+        emit_insn (gen_ashrsi3_bshift (operands[0],operands[1],operands[2]));
+        DONE;
+    }
+  }
+)
+
+;; Irrespective of if we have a barrel-shifter or not, we want to match shifts by 1 with a special 
+;; pattern. When a barrel shifter is present, saves a cycle. If not, allows us to annotate the
+;; instruction for delay slot optimization
+(define_insn "ashrsi3_byone"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+	(ashiftrt:SI (match_operand:SI 1 "register_operand" "d")
+                     (match_operand:SI 2 "arith_operand"    "I")))] 
+  "(INTVAL(operands[2]) == 1)"
+  {
+    return "sra\t%0,%1\;";
+  }
+  [(set_attr "type"	"arith")
+   (set_attr "mode"	"SI")
+   (set_attr "length"	"4")]
+)
+
+
+;; Barrel shift right logical
+(define_insn "ashrsi3_bshift"
+  [(set (match_operand:SI 0 "register_operand" "=d,d")
+	(ashiftrt:SI (match_operand:SI 1 "register_operand" "d,d")
+                     (match_operand:SI 2 "arith_operand"    "I,d")))]
+  "TARGET_BARREL_SHIFT"
+  {
+    if (which_alternative == 0) { 
+       return "bsrai\t%0,%1,%2";
+    } else {                                    /* Shift amount in a register. No need to mask the lower 5 bits out. MB bshift ignores rB[0-26] */
+       return "bsra\t%0,%1,%2";
+    }
+  }
+  [(set_attr "type"	"bshift,bshift")
+  (set_attr "mode"	"SI,SI")
+  (set_attr "length"	"4,4")]
+)
+
+;; The following patterns apply when there is no barrel shifter present
+(define_insn "ashrsi3_with_size_opt"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+	(ashiftrt:SI (match_operand:SI 1 "register_operand"  "d")
+                     (match_operand:SI 2 "immediate_operand" "I")))] 
+  "(INTVAL(operands[2]) > 5 && optimize_size)"
+  {
+    char *asminsn = (char *)xmalloc (200);
+    strcpy (asminsn, "%_ori\tr18,r0,%2\;");
+
+    if (REGNO (operands[0]) != REGNO (operands[1]))
+        strcat (asminsn, "addk\t%0,%1,r0\n");   
+    else
+        strcat (asminsn, "\n");
+
+    strcat (asminsn, "%-ashr_loop:\n\taddik\tr18,r18,-1\;bneid\tr18,%-ashr_loop\;sra\t%0,%0\;");  
+    return asminsn;
+  }
+  [(set_attr "type"	"multi")
+   (set_attr "mode"	"SI")
+   (set_attr "length"	"40")]
+)
+
+(define_insn "ashrsi3_serial"
+  [(set (match_operand:SI 0 "register_operand" "=d,d")
+	(ashiftrt:SI (match_operand:SI 1 "register_operand" "d,d")
+                     (match_operand:SI 2 "arith_operand"    "I,d")))] 
+  ""
+  {
+    if (which_alternative == 0) { 
+        int i;
+        char *asminsn = (char *)xmalloc (25 * 32);
+        if (INTVAL (operands[2]) == 0) {        /* We might get this scenario at -O0 ! (Verify) */
+            if (REGNO (operands[0]) != REGNO (operands[1]))
+                strcpy (asminsn, "addk\t%0,%1,r0\;");
+            else
+                strcpy (asminsn, "\n");
+            return asminsn;
+        }
+
+        strcpy (asminsn, "sra\t%0,%1\;");
+        for (i = 0; i < INTVAL (operands[2]) - 1; i++) 
+            strcat (asminsn, "sra\t%0,%0\;");
+        return asminsn;       
+    } else {                                            /* Shift amount in a register */
+        return "%_andi\tr18,%2,31\;beqid\tr18,%-ashr_out\;addk\t%0,%1,r0\;%-ashr_loop:\n\taddik\tr18,r18,-1\;bneid\tr18,%-ashr_loop\;sra\t%0,%0\n%-ashr_out:\n";  
+    }
+  }
+  [(set_attr "type"	"multi,multi")
+   (set_attr "mode"	"SI,SI")
+   (set_attr "length"	"124,40")]
+)
+
+;;----------------------------------------------------------------
+;; 32-bit right shifts (logical)
+;;----------------------------------------------------------------
+
+(define_expand "lshrsi3"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+	(lshiftrt:SI (match_operand:SI 1 "register_operand" "d")
+                     (match_operand:SI 2 "arith_operand" "")))]
+  ""
+  {
+    if (TARGET_BARREL_SHIFT) {
+        emit_insn (gen_lshrsi3_bshift (operands[0],operands[1],operands[2]));
+        DONE;
+    }
+  }
+)
+
+;; Irrespective of if we have a barrel-shifter or not, we want to match shifts by 1 with a special 
+;; pattern. When a barrel shifter is present, saves a cycle. If not, allows us to annotate the
+;; instruction for delay slot optimization
+(define_insn "lshrsi3_byone"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+	(lshiftrt:SI (match_operand:SI 1 "register_operand" "d")
+                     (match_operand:SI 2 "arith_operand"    "I")))] 
+  "(INTVAL(operands[2]) == 1)"
+  {
+    return "srl\t%0,%1\;";
+  }
+  [(set_attr "type"	"arith")
+   (set_attr "mode"	"SI")
+   (set_attr "length"	"4")]
+)
+
+
+;; Barrel shift right logical
+(define_insn "lshrsi3_bshift"
+  [(set (match_operand:SI 0 "register_operand" "=d,d")
+	(lshiftrt:SI (match_operand:SI 1 "register_operand" "d,d")
+                     (match_operand:SI 2 "arith_operand"    "I,d")))]
+  "TARGET_BARREL_SHIFT"
+  {
+    if (which_alternative == 0) { 
+       return "bsrli\t%0,%1,%2";
+    } else {                                    /* Shift amount in a register. No need to mask the lower 5 bits out. MB bshift ignores rB[0-26] */
+       return "bsrl\t%0,%1,%2";
+    }
+  }
+  [(set_attr "type"	"bshift,bshift")
+  (set_attr "mode"	"SI,SI")
+  (set_attr "length"	"4,4")]
+)
+
+;; The following patterns apply when there is no barrel shifter present
+(define_insn "lshrsi3_with_size_opt"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+	(lshiftrt:SI (match_operand:SI 1 "register_operand"  "d")
+                     (match_operand:SI 2 "immediate_operand" "I")))] 
+  "(INTVAL(operands[2]) > 5 && optimize_size)"
+  {
+    char *asminsn = (char *)xmalloc (200);
+    strcpy (asminsn, "%_ori\tr18,r0,%2\;");
+
+    if (REGNO (operands[0]) != REGNO (operands[1]))
+        strcat (asminsn, "addk\t%0,%1,r0\n");   
+    else
+        strcat (asminsn, "\n");
+
+    strcat (asminsn, "%-lshr_loop:\n\taddik\tr18,r18,-1\;bneid\tr18,%-lshr_loop\;srl\t%0,%0\;");  
+    return asminsn;
+  }
+  [(set_attr "type"	"multi")
+   (set_attr "mode"	"SI")
+   (set_attr "length"	"40")]
+)
+
+(define_insn "lshrsi3_serial"
+  [(set (match_operand:SI 0 "register_operand" "=d,d")
+	(lshiftrt:SI (match_operand:SI 1 "register_operand" "d,d")
+                     (match_operand:SI 2 "arith_operand"    "I,d")))] 
+  ""
+  {
+    if (which_alternative == 0) { 
+        int i;
+        char *asminsn = (char *)xmalloc (25 * 32);
+        if (INTVAL (operands[2]) == 0) {        /* We might get this scenario at -O0 ! (Verify)*/
+        if (REGNO (operands[0]) != REGNO (operands[1]))
+            strcpy (asminsn, "addk\t%0,%1,r0\;");
+        else
+            strcpy (asminsn, "\n");
+        return asminsn;
+    }
+
+        strcpy (asminsn, "srl\t%0,%1\;");
+        for (i = 0; i < INTVAL (operands[2]) - 1; i++) 
+            strcat (asminsn, "srl\t%0,%0\;");
+        return asminsn;       
+    } else {                                            /* Shift amount in a register */
+        return "%_andi\tr18,%2,31\;beqid\tr18,%-lshr_out\;addk\t%0,%1,r0\;%-lshr_loop:\n\taddik\tr18,r18,-1\;bneid\tr18,%-lshr_loop\;srl\t%0,%0\n%-lshr_out:\n";  
+    }
+  }
+  [(set_attr "type"	"multi,multi")
+   (set_attr "mode"	"SI,SI")
+   (set_attr "length"	"124,40")]
+)
+
+
+;;----------------------------------------------------------------
+;; Comparisons
+;;----------------------------------------------------------------
+;; Flow here is rather complex:
+;;
+;;  1)	The cmp{si,di,sf,df} routine is called.  It deposits the
+;;	arguments into the branch_cmp array, and the type into
+;;	branch_type.  No RTL is generated.
+;;
+;;  2)	The appropriate branch define_expand is called, which then
+;;	creates the appropriate RTL for the comparison and branch.
+;;	Different CC modes are used, based on what type of branch is
+;;	done, so that we can constrain things appropriately.  There
+;;	are assumptions in the rest of GCC that break if we fold the
+;;	operands into the branchs for integer operations, and use cc0
+;;	for floating point, so we use the fp status register instead.
+;;	If needed, an appropriate temporary is created to hold the
+;;	of the integer compare.
+
+
+(define_expand "cmpsi"
+  [(set (cc0)
+	(compare:CC (match_operand:SI 0 "register_operand" "")
+		    (match_operand:SI 1 "arith_operand" "")))]
+  ""
+  {
+    if (operands[0])		/* avoid unused code message */
+    {
+        branch_cmp[0] = operands[0];
+        branch_cmp[1] = operands[1];
+        branch_type = CMP_SI;
+        DONE;
+    }
+  }
+)
+
+
+(define_expand "tstsi"
+  [(set (cc0)
+	(match_operand:SI 0 "register_operand" ""))]
+  ""
+  {
+    if (operands[0])		/* avoid unused code message */
+    {
+        /*	fprintf(stderr,"In tstsi \n");*/
+        branch_cmp[0] = operands[0];
+        branch_cmp[1] = const0_rtx;
+        branch_type = CMP_SI;
+        DONE;
+    }
+  }
+)
+
+
+(define_expand "cmpsf"
+  [(set (cc0)
+        (compare:CC (match_operand:SF 0 "register_operand" "")
+                    (match_operand:SF 1 "register_operand" "")))]
+  "TARGET_HARD_FLOAT"
+  {
+    if (operands[0])              /* avoid unused code message */
+    {
+      branch_cmp[0] = operands[0];
+      branch_cmp[1] = operands[1];
+      branch_type = CMP_SF;
+      DONE;
+    }
+  }
+)
+
+;;----------------------------------------------------------------
+;; Setting a register from an integer point comparison. 
+;;----------------------------------------------------------------
+(define_expand "seq"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+	(eq:SI (match_dup 1)
+	       (match_dup 2)))]
+  ""
+  {
+    /* Setup operands */
+    operands[1] = branch_cmp[0];
+    operands[2] = branch_cmp[1];
+
+    /* Find some other way for soft floating point */
+    if ((GET_MODE (operands[1]) == SFmode || GET_MODE (operands[2]) == SFmode) && !TARGET_HARD_FLOAT)
+        FAIL;
+
+    if (!TARGET_PATTERN_COMPARE)
+        FAIL;
+
+    if (GET_CODE (operands[2]) == CONST_INT) {
+        if (INTVAL (operands[2]) == 0)
+            operands[2] = gen_rtx_REG (SImode, 0);
+        else
+            operands[2] = force_reg (SImode, operands[2]);
+    } 
+
+    /* Fall through */
+  }
+)
+
+(define_expand "sne"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+	(ne:SI (match_dup 1)
+	       (match_dup 2)))]
+  ""
+  {
+    /* Setup operands */
+    operands[1] = branch_cmp[0];
+    operands[2] = branch_cmp[1];
+
+    /* Find some other way for soft floating point */
+    if ((GET_MODE (operands[1]) == SFmode || GET_MODE (operands[2]) == SFmode) && !TARGET_HARD_FLOAT)
+        FAIL;
+
+    if (!TARGET_PATTERN_COMPARE)
+        FAIL;
+
+    if (GET_CODE (operands[2]) == CONST_INT) {
+        if (INTVAL (operands[2]) == 0)
+            operands[2] = gen_rtx_REG (SImode, 0);
+        else
+            operands[2] = force_reg (SImode, operands[2]);
+    }
+
+    /* Fall through */
+  }
+)
+
+(define_insn "seq_internal" 
+  [(set (match_operand:SI 0 "register_operand" "=d")
+        (eq:SI (match_operand:SI 1 "register_operand" "d")
+               (match_operand:SI 2 "register_operand" "d")))
+  ]
+  "TARGET_PATTERN_COMPARE"
+  {
+    return "pcmpeq\t%0,%1,%2";
+  }      
+  [(set_attr "type"	"arith")
+   (set_attr "mode"	"SI")
+   (set_attr "length"	"4")]
+)              
+
+(define_insn "sne_internal" 
+  [(set (match_operand:SI 0 "register_operand" "=d")
+        (ne:SI (match_operand:SI 1 "register_operand" "d")
+               (match_operand:SI 2 "register_operand" "d")))
+  ]
+  "TARGET_PATTERN_COMPARE"
+  {
+    return "pcmpne\t%0,%1,%2";
+  }          
+  [(set_attr "type"	"arith")
+  (set_attr "mode"	"SI")
+  (set_attr "length"	"4")]
+)              
+
+(define_insn ""
+  [(set (match_operand:SI 0 "register_operand" "=d,d")
+        (eq:SI (match_operand:SI 1 "register_operand" "%d,d")
+               (match_operand:SI 2 "arith_operand" "d,i")))
+  (clobber (match_scratch:SI 3 "=d,d"))]
+  "!(TARGET_PATTERN_COMPARE)"
+  "@
+    xor\t%0,%1,%2\;rsubi\t%3,%0,0\;addc\t%0,%3,%0
+    xori\t%0,%1,%2\;rsubi\t%3,%0,0\;addc\t%0,%3,%0"
+  [(set_attr "length"   "12,12")
+   (set_attr "type"     "multi, multi")
+   (set_attr "mode"     "SI,SI")]
+)
+
+
+;;----------------------------------------------------------------
+;; Setting a register from a floating point comparison. 
+;; These s<cond> patterns are for hardware floating point 
+;; compares only
+;;----------------------------------------------------------------
+(define_expand "slt"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+        (lt:SI (match_dup 1)
+               (match_dup 2)))]
+  "TARGET_HARD_FLOAT"
+  {               
+    /* Setup operands */
+    operands[1] = branch_cmp[0];
+    operands[2] = branch_cmp[1];
+
+    /* Only for floating point compares */
+    if ((GET_MODE (operands[1]) != SFmode) || (GET_MODE (operands[2]) != SFmode))
+        FAIL;
+
+    if (GET_CODE (operands[2]) == CONST_DOUBLE)
+        force_reg (SFmode, operands[2]);
+
+    /* Fall through */
+  }
+)
+
+(define_expand "sle"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+        (le:SI (match_dup 1)
+               (match_dup 2)))]
+  "TARGET_HARD_FLOAT"
+  {               
+    /* Setup operands */
+    operands[1] = branch_cmp[0];
+    operands[2] = branch_cmp[1];
+
+    /* Only for floating point compares */
+    if ((GET_MODE (operands[1]) != SFmode) || (GET_MODE (operands[2]) != SFmode))
+        FAIL;
+
+    if (GET_CODE (operands[2]) == CONST_DOUBLE)
+        force_reg (SFmode, operands[2]);
+
+    /* Fall through */
+  }
+)
+
+(define_expand "sgt"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+        (gt:SI (match_dup 1)
+               (match_dup 2)))]
+  "TARGET_HARD_FLOAT"
+  {               
+    /* Setup operands */
+    operands[1] = branch_cmp[0];
+    operands[2] = branch_cmp[1];
+
+    /* Only for floating point compares */
+    if ((GET_MODE (operands[1]) != SFmode) || (GET_MODE (operands[2]) != SFmode))
+        FAIL;
+
+    if (GET_CODE (operands[2]) == CONST_DOUBLE)
+        force_reg (SFmode, operands[2]);
+
+    /* Fall through */
+  }
+)
+
+(define_expand "sge"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+        (ge:SI (match_dup 1)
+               (match_dup 2)))]
+  "TARGET_HARD_FLOAT"
+  {               
+    /* Setup operands */
+    operands[1] = branch_cmp[0];
+    operands[2] = branch_cmp[1];
+
+    /* Only for floating point compares */
+    if ((GET_MODE (operands[1]) != SFmode) || (GET_MODE (operands[2]) != SFmode))
+        FAIL;
+
+    if (GET_CODE (operands[2]) == CONST_DOUBLE)
+        force_reg (SFmode, operands[2]);
+
+    /* Fall through */
+  }
+)
+
+(define_insn "seq_sf"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+        (eq:SI (match_operand:SF 1 "register_operand" "d")
+               (match_operand:SF 2 "register_operand" "d")))]
+  "TARGET_HARD_FLOAT"
+  {
+        return "fcmp.eq\t%0,%2,%1";
+  }
+  [(set_attr "type"     "fcmp")
+   (set_attr "mode"      "SF")
+   (set_attr "length"    "4")]
+)
+
+(define_insn "sne_sf"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+        (ne:SI (match_operand:SF 1 "register_operand" "d")
+               (match_operand:SF 2 "register_operand" "d")))]
+  "TARGET_HARD_FLOAT"
+  {
+        return "fcmp.ne\t%0,%2,%1";
+  }
+  [(set_attr "type"     "fcmp")
+   (set_attr "mode"      "SF")
+   (set_attr "length"    "4")]
+)
+
+(define_insn "slt_sf"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+        (lt:SI (match_operand:SF 1 "register_operand" "d")
+               (match_operand:SF 2 "register_operand" "d")))]
+  "TARGET_HARD_FLOAT"
+  {
+        return "fcmp.lt\t%0,%2,%1";
+  }
+  [(set_attr "type"     "fcmp")
+   (set_attr "mode"      "SF")
+   (set_attr "length"    "4")]
+)
+
+(define_insn "sle_sf"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+        (le:SI (match_operand:SF 1 "register_operand" "d")
+               (match_operand:SF 2 "register_operand" "d")))]
+  "TARGET_HARD_FLOAT"
+  {
+        return "fcmp.le\t%0,%2,%1";
+  }
+  [(set_attr "type"     "fcmp")
+   (set_attr "mode"      "SF")
+   (set_attr "length"    "4")]
+)
+
+(define_insn "sgt_sf"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+        (gt:SI (match_operand:SF 1 "register_operand" "d")
+               (match_operand:SF 2 "register_operand" "d")))]
+  "TARGET_HARD_FLOAT"
+  {
+        return "fcmp.gt\t%0,%2,%1";
+  }
+  [(set_attr "type"     "fcmp")
+  (set_attr "mode"      "SF")
+  (set_attr "length"    "4")])
+
+(define_insn "sge_sf"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+        (ge:SI (match_operand:SF 1 "register_operand" "d")
+               (match_operand:SF 2 "register_operand" "d")))]
+  "TARGET_HARD_FLOAT"
+  {
+        return "fcmp.ge\t%0,%2,%1";
+  }
+  [(set_attr "type"     "fcmp")
+   (set_attr "mode"      "SF")
+   (set_attr "length"    "4")]
+)
+
+;;----------------------------------------------------------------
+;; Conditional branches
+;;----------------------------------------------------------------
+
+(define_expand "beq"
+  [(set (pc)
+	(if_then_else (eq:CC (cc0)
+			     (const_int 0))
+		      (label_ref (match_operand 0 "" ""))
+		      (pc)))]
+  ""
+  {
+    gen_conditional_branch (operands, EQ);
+    DONE;
+  }
+)
+
+(define_expand "bne"
+  [(set (pc)
+	(if_then_else (ne:CC (cc0)
+			     (const_int 0))
+		      (label_ref (match_operand 0 "" ""))
+		      (pc)))]
+  ""
+  {
+    gen_conditional_branch (operands, NE);
+    DONE;
+  }
+)
+
+(define_expand "bgt"
+  [(set (pc)
+	(if_then_else (gt:CC (cc0)
+			     (const_int 0))
+		      (label_ref (match_operand 0 "" ""))
+		      (pc)))]
+  ""
+  {
+    gen_conditional_branch (operands, GT);
+    DONE;
+  }
+)
+
+(define_expand "bge"
+  [(set (pc)
+	(if_then_else (ge:CC (cc0)
+			     (const_int 0))
+		      (label_ref (match_operand 0 "" ""))
+		      (pc)))]
+  ""
+  {
+    gen_conditional_branch (operands, GE);
+    DONE;
+  }
+)
+
+(define_expand "blt"
+  [(set (pc)
+	(if_then_else (lt:CC (cc0)
+			     (const_int 0))
+		      (label_ref (match_operand 0 "" ""))
+		      (pc)))]
+  ""
+  {
+    gen_conditional_branch (operands, LT);
+    DONE;
+  }
+)
+
+(define_expand "ble"
+  [(set (pc)
+	(if_then_else (le:CC (cc0)
+			     (const_int 0))
+		      (label_ref (match_operand 0 "" ""))
+		      (pc)))]
+  ""
+  {
+    gen_conditional_branch (operands, LE);
+    DONE;
+  }
+)
+
+(define_expand "bgtu"
+  [(set (pc)
+	(if_then_else (gtu:CC (cc0)
+			      (const_int 0))
+		      (label_ref (match_operand 0 "" ""))
+		      (pc)))]
+  ""
+  {
+    gen_conditional_branch (operands, GTU);
+    DONE;
+  }
+)
+
+(define_expand "bgeu"
+  [(set (pc)
+	(if_then_else (geu:CC (cc0)
+			      (const_int 0))
+		      (label_ref (match_operand 0 "" ""))
+		      (pc)))]
+  ""
+  {
+    gen_conditional_branch (operands, GEU);
+    DONE;
+  }
+)
+
+
+(define_expand "bltu"
+  [(set (pc)
+	(if_then_else (ltu:CC (cc0)
+			      (const_int 0))
+		      (label_ref (match_operand 0 "" ""))
+		      (pc)))]
+  ""
+  {
+    gen_conditional_branch (operands, LTU);
+    DONE;
+  }
+)
+
+(define_expand "bleu"
+  [(set (pc)
+	(if_then_else (leu:CC (cc0)
+			      (const_int 0))
+		      (label_ref (match_operand 0 "" ""))
+		      (pc)))]
+  ""
+  {
+    gen_conditional_branch (operands, LEU);
+    DONE;
+  }
+)
+
+(define_insn "branch_zero"
+  [(set (pc)
+	(if_then_else (match_operator:SI 0 "cmp_op"
+  					 [(match_operand:SI 1 "register_operand" "d")
+                                          (const_int 0)])
+                      (match_operand:SI 2 "pc_or_label_operand" "")
+                      (match_operand:SI 3 "pc_or_label_operand" "")))
+  ]
+  ""
+  {
+    if (operands[2] != pc_rtx)           /* normal jump */
+    {				
+        switch (GET_CODE (operands[0]))
+	{
+            case EQ:  return "beqi%?\t%z1,%2";
+	    case NE:  return "bnei%?\t%z1,%2";
+            case GT:  return "bgti%?\t%z1,%2";
+	    case LE:  return "blei%?\t%z1,%2";
+	    case GE:  return "bgei%?\t%z1,%2";
+	    case LT:  return "blti%?\t%z1,%2";
+	    case GTU: return "bnei%?\t%z1,%2";
+	    case LEU: return "beqi%?\t%z1,%2";
+	    case GEU: return "bri%?\t%2";
+	    case LTU: return "# Always false LTU comparison";
+	    default:
+	       break;
+        }
+        return "b%C0i%?\t%z1,%2";
+    } else {                             /* inverted jump */
+
+        switch (GET_CODE (operands[0]))
+        {
+            case EQ:  return "bnei%?\t%z1,%3";
+	    case NE:  return "beqi%?\t%z1,%3";
+	    case GT:  return "blei%?\t%z1,%3";
+	    case LE:  return "bgti%?\t%z1,%3";
+	    case GE:  return "blti%?\t%z1,%3";
+	    case LT:  return "bgei%?\t%z1,%3";
+	    case GTU: return "beqi%?\t%z1,%3";
+	    case LEU: return "bnei%?\t%z1,%3";
+	    case GEU: return "# Always false GEU comparison";
+	    case LTU: return "bri%?\t%3";
+	    default:
+	       break;
+        }
+
+        return "b%N0i%?\t%z1,%3";
+    }
+  }
+  [(set_attr "type"	"branch")
+   (set_attr "mode"	"none")
+   (set_attr "length"	"4")]
+)
+
+
+
+(define_insn "branch_compare_imm_uns"
+  [(set (pc)
+	(if_then_else (match_operator:SI 0 "unsigned_cmp_op"
+					 [(match_operand:SI 1 "register_operand" "d")
+                                          (match_operand:SI 2 "immediate_operand" "i")])
+                      (match_operand:SI 3 "pc_or_label_operand" "")
+                      (match_operand:SI 4 "pc_or_label_operand" "")))
+  (clobber(reg:SI R_TMP))]
+  ""
+  {
+    if (operands[3] != pc_rtx)                                  /* normal jump */
+    {                               
+        switch (GET_CODE (operands[0]))
+        {
+            case GTU: return  "addi\tr18,r0,%2\;cmpu\tr18,%z1,r18\;blti%?\tr18,%3";
+            case LEU: return  "addi\tr18,r0,%2\;cmpu\tr18,%z1,r18\;bgei%?\tr18,%3";
+            case GEU: return  "addi\tr18,r0,%2\;cmpu\tr18,r18,%z1\;bgei%?\tr18,%3";
+            case LTU: return  "addi\tr18,r0,%2\;cmpu\tr18,r18,%z1\;blti%?\tr18,%3";
+            default:
+                break;
+        }
+
+        fatal_insn ("branch_compare_imm_uns: ", operands[0]);
+   }
+   else {                                                       /* inverted jump */
+        switch (GET_CODE (operands[0]))
+        {
+            case GTU: return "addi\tr18,r0,%2\;cmpu\tr18,%z1,r18\;bgei%?\tr18,%3";
+            case LEU: return "addi\tr18,r0,%2\;cmpu\tr18,%z1,r18\;blti%?\tr18,%3";
+            case GEU: return "addi\tr18,r0,%2\;cmpu\tr18,r18,%z1\;blti%?\tr18,%3";
+            case LTU: return "addi\tr18,r0,%2\;cmpu\tr18,r18,%z1\;bgei%?\tr18,%3";
+            default:
+                break;
+        }
+
+        fatal_insn ("branch_compare_imm_uns: ", operands[0]);
+    }
+  }
+  [(set_attr "type"	"branch")   ;; Bit of a hack here. It's a multi rather than a branch, yet, we want delay slot optimization to occur on this.
+   (set_attr "mode"	"none")
+   (set_attr "length"	"12")]
+)
+
+
+(define_insn "branch_compare_imm"
+  [(set (pc)
+	(if_then_else (match_operator:SI 0 "signed_cmp_op"
+					 [(match_operand:SI 1 "register_operand" "d")
+                                          (match_operand:SI 2 "immediate_operand" "i")])
+                      (match_operand:SI 3 "pc_or_label_operand" "")
+                      (match_operand:SI 4 "pc_or_label_operand" "")))
+  (clobber(reg:SI R_TMP))]
+  ""
+  {
+    if (operands[3] != pc_rtx) {                                /* normal jump */
+        switch (GET_CODE (operands[0]))
+        {
+            case GT: return  "addik\tr18,r0,%2\;cmp\tr18,%z1,r18\;blti%?\tr18,%3" ;
+            case LE: return  "addik\tr18,r0,%2\;cmp\tr18,%z1,r18\;bgei%?\tr18,%3" ;
+            case GE: return  "addik\tr18,r0,%2\;cmp\tr18,r18,%z1\;bgei%?\tr18,%3" ;
+            case LT: return  "addik\tr18,r0,%2\;cmp\tr18,r18,%z1\;blti%?\tr18,%3" ;
+            default:
+                break;
+        }
+        if ((GET_CODE(operands[0]) != NE) && (GET_CODE(operands[0]) != EQ)) 
+             fatal_insn ("branch_compare_imm: ", operands[0]);
+
+        return  "xori\tr18,%z1,%2\;b%C0i%?\tr18,%3";            /* Handle NE, EQ */
+    }
+    else {                                                      /* inverted jump */
+        switch (GET_CODE (operands[0]))
+        {
+            case GT: return  "addik\tr18,r0,%2\;cmp\tr18,%z1,r18\;bgei%?\tr18,%4";
+            case LE: return  "addik\tr18,r0,%2\;cmp\tr18,%z1,r18\;blti%?\tr18,%4";
+            case GE: return  "addik\tr18,r0,%2\;cmp\tr18,r18,%z1\;blti%?\tr18,%4";
+            case LT: return  "addik\tr18,r0,%2\;cmp\tr18,r18,%z1\;bgei%?\tr18,%4";
+            default:
+                break;
+        }
+        if ((GET_CODE(operands[0]) != NE) && (GET_CODE(operands[0]) != EQ))
+             fatal_insn ("branch_compare_imm: ", operands[0]);
+
+        return  "xori\tr18,%z1,%2\;b%N0i%?\tr18,%4";            /* Handle NE, EQ */
+    }
+  }
+  [(set_attr "type"	"branch")   ;; Bit of a hack here. It's a multi rather than a branch, yet, we want delay slot optimization to occur on this.
+   (set_attr "mode"	"none")
+   (set_attr "length"	"12")]
+)
+
+
+(define_insn "branch_compare"
+  [(set (pc)
+        (if_then_else (match_operator:SI 0 "cmp_op"
+                                         [(match_operand:SI 1 "register_operand" "d")
+                                          (match_operand:SI 2 "register_operand" "d")
+                                         ])
+                      (match_operand:SI 3 "pc_or_label_operand" "")
+                      (match_operand:SI 4 "pc_or_label_operand" "")))
+  (clobber(reg:SI R_TMP))]
+  ""
+  {
+
+    if (operands[3] != pc_rtx)
+    {                                                           /* normal jump */
+        switch (GET_CODE (operands[0]))
+        {
+
+            case GT: return  "cmp\tr18,%z1,%z2\;blti%?\tr18,%3";
+            case LE: return  "cmp\tr18,%z1,%z2\;bgei%?\tr18,%3";
+            case GE: return  "cmp\tr18,%z2,%z1\;bgei%?\tr18,%3";
+            case LT: return  "cmp\tr18,%z2,%z1\;blti%?\tr18,%3";
+            case GTU:return  "cmpu\tr18,%z1,%z2\;blti%?\tr18,%3";
+            case LEU:return  "cmpu\tr18,%z1,%z2\;bgei%?\tr18,%3";
+            case GEU:return  "cmpu\tr18,%z2,%z1\;bgei%?\tr18,%3";
+            case LTU:return  "cmpu\tr18,%z2,%z1\;blti%?\tr18,%3";
+            default:
+                break;
+        }
+        return  "rsubk\tr18,%z2,%z1\;b%C0i%?\tr18,%3";
+    }
+    else
+    {                                                           /* inverted jump */
+        switch (GET_CODE (operands[0]))
+        {
+            case GT: return  "cmp\tr18,%z1,%z2\;bgei%?\tr18,%3" ;
+            case LE: return  "cmp\tr18,%z1,%z2\;blti%?\tr18,%3" ;
+            case GE: return  "cmp\tr18,%z2,%z1\;blti%?\tr18,%3" ;
+            case LT: return  "cmp\tr18,%z2,%z1\;bgei%?\tr18,%3" ;
+            case GTU:return  "cmpu\tr18,%z1,%z2\;bgei%?\tr18,%3";
+            case LEU:return  "cmpu\tr18,%z1,%z2\;blti%?\tr18,%3";
+            case GEU:return  "cmpu\tr18,%z2,%z1\;blti%?\tr18,%3";
+            case LTU:return  "cmpu\tr18,%z2,%z1\;bgei%?\tr18,%3";
+            default:
+                break;
+        }
+
+        return  "cmp\tr18,%z1,%z2\;b%N0i%?\tr18,%4 ";
+    }
+  }
+  [(set_attr "type"	"branch")
+   (set_attr "mode"	"none")
+   (set_attr "length"	"12")]
+)
+
+;;----------------------------------------------------------------
+;; Unconditional branches
+;;----------------------------------------------------------------
+(define_insn "jump"
+  [(set (pc)
+	(label_ref (match_operand 0 "" "")))]
+  ""
+  {
+    if (GET_CODE (operands[0]) == REG)
+        return "br%?\t%0";
+    else	
+        return "bri%?\t%l0";
+  }
+  [(set_attr "type"	"jump")
+  (set_attr "mode"	"none")
+  (set_attr "length"	"2")])
+
+(define_expand "indirect_jump"
+  [(set (pc) (match_operand 0 "register_operand" "d"))]
+  ""
+  {
+    rtx dest;
+
+    if (operands[0])		/* eliminate unused code warnings */
+    {
+        dest = operands[0];
+        if (GET_CODE (dest) != REG || GET_MODE (dest) != Pmode)
+            operands[0] = copy_to_mode_reg (Pmode, dest);
+
+        if (!(Pmode == DImode))
+            emit_jump_insn (gen_indirect_jump_internal1 (operands[0]));
+        else
+            emit_jump_insn (gen_indirect_jump_internal2 (operands[0]));
+        DONE;
+    }
+  }
+)
+
+;; Indirect jumps. Jump to register values. Assuming absolute jumps
+
+(define_insn "indirect_jump_internal1"
+  [(set (pc) (match_operand:SI 0 "register_operand" "d"))]
+  "!(Pmode == DImode)"
+  "bra%?\t%0"
+  [(set_attr "type"	"jump")
+  (set_attr "mode"	"none")
+  (set_attr "length"	"2")])
+
+(define_insn "indirect_jump_internal2"
+  [(set (pc) (match_operand:DI 0 "register_operand" "d"))]
+  "Pmode == DImode"
+  "bra%?\t%0"
+  [(set_attr "type"	"jump")
+  (set_attr "mode"	"none")
+  (set_attr "length"	"2")])
+
+(define_expand "tablejump"
+  [(set (pc)
+	(match_operand 0 "register_operand" "d"))
+  (use (label_ref (match_operand 1 "" "")))]
+  ""
+  {
+    if (operands[0])		/* eliminate unused code warnings */
+    {
+        if (GET_MODE (operands[0]) != Pmode)
+            abort ();
+
+        if (! flag_pic)
+	{
+            if (!(Pmode == DImode))
+                emit_jump_insn (gen_tablejump_internal1 (operands[0], operands[1]));
+            else
+                emit_jump_insn (gen_tablejump_internal2 (operands[0], operands[1]));
+	}
+        else
+	{
+            if (!(Pmode == DImode))
+                emit_jump_insn (gen_tablejump_internal3 (operands[0], operands[1]));
+            else
+                emit_jump_insn (gen_tablejump_internal4 (operands[0], operands[1]));
+	}
+        DONE;
+    }
+  }
+)
+
+(define_insn "tablejump_internal1"
+  [(set (pc)
+	(match_operand:SI 0 "register_operand" "d"))
+  (use (label_ref (match_operand 1 "" "")))]
+  "!(Pmode == DImode)"
+  "bra%?\t%0 "
+  [(set_attr "type"	"jump")
+  (set_attr "mode"	"none")
+  (set_attr "length"	"1")])
+
+(define_insn "tablejump_internal2"
+  [(set (pc)
+	(match_operand:DI 0 "register_operand" "d"))
+  (use (label_ref (match_operand 1 "" "")))]
+  "Pmode == DImode"
+  "bra%?\t%0"
+  [(set_attr "type"	"jump")
+  (set_attr "mode"	"none")
+  (set_attr "length"	"2")])
+
+(define_expand "tablejump_internal3"
+  [(parallel [(set (pc)
+		   (plus:SI (match_operand:SI 0 "register_operand" "d")
+			    (label_ref:SI (match_operand:SI 1 "" ""))))
+             (use (label_ref:SI (match_dup 1)))])]
+  ""
+  ""
+)
+
+;; need to change for MicroBlaze PIC
+(define_insn ""
+ [(set (pc)
+	(plus:SI (match_operand:SI 0 "register_operand" "d")
+		 (label_ref:SI (match_operand 1 "" ""))))
+  (use (label_ref:SI (match_dup 1)))]
+ "!(Pmode == DImode) && next_active_insn (insn) != 0
+  && GET_CODE (PATTERN (next_active_insn (insn))) == ADDR_DIFF_VEC
+  && PREV_INSN (next_active_insn (insn)) == operands[1]
+  && flag_pic"
+  {
+    /* .cpadd expands to add REG,REG,$gp when pic, and nothing when not pic.  
+     */
+    output_asm_insn ("addk\t%0,%0,r20",operands);
+    return "bra%?\t%0";
+    /*  output_asm_insn (".cpadd\t%0", operands);
+        return "MICROBLAZEbri%?11\t%0";
+     */
+}
+ [(set_attr "type"	"jump")
+  (set_attr "mode"	"none")
+  (set_attr "length"	"2")])
+
+(define_expand "tablejump_internal4"
+  [(parallel [(set (pc)
+		   (plus:DI (match_operand:DI 0 "register_operand" "d")
+			    (label_ref:DI (match_operand:SI 1 "" ""))))
+             (use (label_ref:DI (match_dup 1)))])]
+  ""
+  ""
+)
+
+;;; Make sure that this only matches the insn before ADDR_DIFF_VEC.  Otherwise
+;;; it is not valid.  ??? With the USE, the condition tests may not be required
+;;; any longer.
+
+(define_insn ""
+  [(set (pc)
+	(plus:DI (match_operand:DI 0 "register_operand" "d")
+		 (label_ref:DI (match_operand 1 "" ""))))
+  (use (label_ref:DI (match_dup 1)))]
+  "Pmode == DImode && next_active_insn (insn) != 0
+   && GET_CODE (PATTERN (next_active_insn (insn))) == ADDR_DIFF_VEC
+   && PREV_INSN (next_active_insn (insn)) == operands[1]"
+  "MICROBLAZEbri%?12\t%0"
+  [(set_attr "type"	"jump")
+  (set_attr "mode"	"none")
+  (set_attr "length"	"2")])
+
+
+(define_expand "builtin_setjmp_setup"
+  [(unspec [(match_operand 0 "register_operand" "r")] 20)]
+  "TARGET_ABICALLS"
+  {
+    if (Pmode == DImode)
+        emit_insn (gen_builtin_setjmp_setup_64 (operands[0]));
+    else
+        emit_insn (gen_builtin_setjmp_setup_32 (operands[0]));
+    DONE;
+  }
+)
+
+(define_expand "builtin_setjmp_setup_32"
+  [(set (mem:SI (plus:SI (match_operand:SI 0 "register_operand" "r")
+                         (const_int 12)))
+        (reg:SI 28))]
+  "TARGET_ABICALLS && ! (Pmode == DImode)"
+  ""
+)
+
+(define_expand "builtin_setjmp_setup_64"
+  [(set (mem:DI (plus:DI (match_operand:DI 0 "register_operand" "r")
+                         (const_int 24)))
+        (reg:DI 28))]
+  "TARGET_ABICALLS && Pmode == DImode"
+  ""
+)
+
+;; For o32/n32/n64, we need to arrange for longjmp to put the 
+;; target address in t9 so that we can use it for loading $gp.
+
+(define_expand "builtin_longjmp"
+  [(unspec_volatile [(match_operand 0 "register_operand" "r")] 3)]
+  "TARGET_ABICALLS"
+  {
+    /* The elements of the buffer are, in order:  */
+    int W = (Pmode == DImode ? 8 : 4);
+    rtx fp = gen_rtx_MEM (Pmode, operands[0]);
+    rtx lab = gen_rtx_MEM (Pmode, plus_constant (operands[0], 1*W));
+    rtx stack = gen_rtx_MEM (Pmode, plus_constant (operands[0], 2*W));
+    rtx gpv = gen_rtx_MEM (Pmode, plus_constant (operands[0], 3*W));
+    rtx pv = gen_rtx_REG (Pmode, 25);
+    rtx gp = gen_rtx_REG (Pmode, 28);
+    
+    /* This bit is the same as expand_builtin_longjmp.  */
+    emit_move_insn (hard_frame_pointer_rtx, fp);
+    emit_move_insn (pv, lab);
+    emit_stack_restore (SAVE_NONLOCAL, stack, NULL_RTX);
+    emit_move_insn (gp, gpv);
+    emit_insn (gen_rtx_USE (VOIDmode, hard_frame_pointer_rtx));
+    emit_insn (gen_rtx_USE (VOIDmode, stack_pointer_rtx));
+    emit_insn (gen_rtx_USE (VOIDmode, gp));
+    emit_indirect_jump (pv);
+    DONE;
+  }
+)
+
+;;----------------------------------------------------------------
+;; Function prologue/epilogue and stack allocation
+;;----------------------------------------------------------------
+(define_expand "prologue"
+  [(const_int 1)]
+  ""
+  {
+      microblaze_expand_prologue ();
+      DONE;
+  }
+)
+
+(define_expand "epilogue"
+  [(use (const_int 0))]
+  ""
+  {
+      microblaze_expand_epilogue ();
+      DONE;
+  }
+)
+
+;; An insn to allocate new stack space for dynamic use (e.g., alloca).
+;; We copy the return address, decrement the stack pointer and save the 
+;; return address again at the new stack top 
+
+(define_expand "allocate_stack"
+  [(set (match_operand 0 "register_operand" "=r")
+	(minus (reg 1) (match_operand 1 "register_operand" "")))
+   (set (reg 1)
+	(minus (reg 1) (match_dup 1)))]
+  ""
+  { 
+    rtx retaddr = gen_rtx_MEM (Pmode, stack_pointer_rtx);
+    rtx rtmp    = gen_rtx_REG (SImode, R_TMP);
+    rtx neg_op0;
+
+    emit_move_insn (rtmp, retaddr);
+    if (GET_CODE (operands[1]) != CONST_INT)
+    {
+        neg_op0 = gen_reg_rtx (Pmode);
+	emit_insn (gen_negsi2 (neg_op0, operands[1]));
+    } else
+        neg_op0 = GEN_INT (- INTVAL (operands[1]));
+
+    emit_insn (gen_addsi3 (stack_pointer_rtx, stack_pointer_rtx, neg_op0));
+    emit_move_insn (gen_rtx_MEM (Pmode, stack_pointer_rtx), rtmp);
+    emit_move_insn (operands[0], virtual_stack_dynamic_rtx);
+    emit_insn (gen_rtx_CLOBBER (SImode, rtmp));
+    DONE;
+  }
+)
+
+;; These patterns say how to save and restore the stack pointer.  We need not
+;; save the stack pointer at function/block level since we are careful to
+;; use a different method for getting the return address off the stack
+
+(define_expand "save_stack_function"
+  [(match_operand 0 "" "")
+   (match_operand 1 "" "")]
+  ""
+  "DONE;")
+
+(define_expand "restore_stack_function"
+  [(match_operand 0 "" "")
+   (match_operand 1 "" "")]
+  ""
+  "DONE;")
+
+(define_expand "save_stack_block"
+  [(match_operand 0 "" "")
+   (match_operand 1 "" "")]
+  ""
+  "DONE;")
+
+(define_expand "restore_stack_block"
+  [(match_operand 0 "" "")
+   (match_operand 1 "" "")]
+  ""
+  "DONE;")
+
+;; Trivial return.  Make it look like a normal return insn as that
+;; allows jump optimizations to work better .
+(define_insn "return"
+  [(return)]
+  "microblaze_can_use_return_insn ()"
+  { 
+    if (microblaze_is_interrupt_handler())
+        return "rtid\tr14, 0\;%#";
+    else
+        return "rtsd\tr15, 8\;%#";
+  }
+  [(set_attr "type"	"jump")
+  (set_attr "mode"	"none")
+  (set_attr "length"	"2")])
+
+;; Normal return.
+;; We match any mode for the return address, so that this will work with
+;; both 32 bit and 64 bit targets.
+
+(define_insn "return_internal"
+  [(parallel[(use (match_operand:SI 0 "register_operand" ""))
+             (return)])]
+  ""
+  {	
+    if (microblaze_is_interrupt_handler())
+        return "rtid\tr14,0 \;%#";
+    else
+        return "rtsd\tr15,8 \;%#";
+  }
+  [(set_attr "type"	"jump")
+  (set_attr "mode"	"none")
+  (set_attr "length"	"2")])
+
+
+;; Block any insns from across this point
+;; Useful to group sequences together.
+(define_insn "blockage"
+  [(unspec_volatile [(const_int 0)] 0)]
+  ""
+  ""
+  [(set_attr "type"	"unknown")
+  (set_attr "mode"	"none")
+  (set_attr "length"	"0")])
+
+  
+;;----------------------------------------------------------------
+;; Function calls
+;;----------------------------------------------------------------
+
+;; calls.c now passes a third argument, make saber happy
+
+(define_expand "call"
+  [(parallel [(call (match_operand 0 "memory_operand" "m")
+		    (match_operand 1 "" "i"))
+             (clobber (reg:SI R_SR))
+             (use (match_operand 2 "" "")) ;; next_arg_reg
+             (use (match_operand 3 "" ""))])] ;; struct_value_size_rtx
+  ""
+  {
+    rtx addr;
+    if (operands[0])		/* eliminate unused code warnings */
+    {
+        addr = XEXP (operands[0], 0);
+        if ((GET_CODE (addr) != REG && (!CONSTANT_ADDRESS_P (addr) || TARGET_LONG_CALLS))
+	    || ! call_insn_operand (addr, VOIDmode))
+            XEXP (operands[0], 0) = copy_to_mode_reg (Pmode, addr);
+
+        /* In order to pass small structures by value in registers
+	   compatibly with the MICROBLAZE compiler, we need to shift the value
+	   into the high part of the register.  Function_arg has encoded
+	   a PARALLEL rtx, holding a vector of adjustments to be made
+	   as the next_arg_reg variable, so we split up the insns,
+	   and emit them separately.  */
+
+        if (operands[2] != (rtx)0 && GET_CODE (operands[2]) == PARALLEL)
+	{
+            rtvec adjust = XVEC (operands[2], 0);
+            int num = GET_NUM_ELEM (adjust);
+            int i;
+
+            for (i = 0; i < num; i++)
+                emit_insn (RTVEC_ELT (adjust, i));
+	}
+
+        emit_call_insn (gen_call_internal0 (operands[0], operands[1],
+                                            gen_rtx (REG, SImode, GP_REG_FIRST + MB_ABI_SUB_RETURN_ADDR_REGNUM)));
+
+        DONE;
+    }
+  }
+)
+
+(define_expand "call_internal0"
+  [(parallel [(call (match_operand 0 "" "")
+		    (match_operand 1 "" ""))
+             (clobber (match_operand:SI 2 "" ""))])]
+  ""
+  {
+    /*      fprintf(stderr,"expand \t call_internal0  \n");*/
+  }
+)
+
+(define_insn "call_internal1"
+  [(call (mem (match_operand:SI 0 "call_insn_operand" "ri"))
+	 (match_operand:SI 1 "" "i"))
+  (clobber (reg:SI R_SR))]
+  "!TARGET_ABICALLS && !TARGET_LONG_CALLS"
+  {
+    register rtx target = operands[0];
+    register rtx target2=gen_rtx (REG, Pmode,GP_REG_FIRST + MB_ABI_SUB_RETURN_ADDR_REGNUM);
+    /*  fprintf(stderr,"insn\t call_internal1  \n");	*/
+    if (GET_CODE (target) == SYMBOL_REF) {
+        gen_rtx_CLOBBER(VOIDmode,target2);
+        return "brlid\tr15,%0\;%#";
+    } else if (GET_CODE (target) == CONST_INT)
+        return "la\t%@,r0,%0\;brald\tr15,%@\;%#";
+    else if (GET_CODE(target) == REG)
+        return "brald\tr15,%0\;%#";	
+    else {
+        fprintf(stderr,"Unsupported call insn\n");
+        return NULL;
+    }
+  }
+  [(set_attr "type"	"call")
+  (set_attr "mode"	"none")
+  (set_attr "length"	"4")])
+
+(define_insn "call_internal2"
+  [(call (mem (match_operand 0 "call_insn_operand" "ri"))
+	 (match_operand 1 "" "i"))
+  (clobber (match_operand:SI 2 "register_operand" "=d"))]
+  "TARGET_ABICALLS && !TARGET_LONG_CALLS"
+  {
+    register rtx target = operands[0];
+    /*  fprintf(stderr,"expand \t call_internal2  \n");*/
+    if (GET_CODE (target) == SYMBOL_REF)
+    {
+        if (GET_MODE (target) == SImode)
+            return "la\t%^,%0\;brlid\t%2,%^";
+        else
+            return "dla\t%^,%0\;MICROBLAZEjal\t%2,%^";
+    }
+    else if (GET_CODE (target) == CONST_INT)
+        return "li\t%^,%0\;MICROBLAZEjal\t%2,%^";
+    else if (REGNO (target) != PIC_FUNCTION_ADDR_REGNUM)
+        return "move\t%^,%0\;MICROBLAZEjal\t%2,%^";
+    else
+        return "MICROBLAZEjal\t%2,%0";
+  }
+  [(set_attr "type"	"call")
+  (set_attr "mode"	"none")
+  (set_attr "length"	"2")])
+
+(define_insn "call_internal3a"
+  [(call (mem:SI (match_operand:SI 0 "register_operand" "r"))
+	 (match_operand 1 "" "i"))
+  (clobber (match_operand:SI 2 "register_operand" "=d"))]
+  "!(Pmode == DImode) && !TARGET_ABICALLS && TARGET_LONG_CALLS"
+  "MICROBLAZEjal\t%2,%0"
+  [(set_attr "type"	"call")
+  (set_attr "mode"	"none")
+  (set_attr "length"	"1")])
+
+(define_insn "call_internal3b"
+  [(call (mem:DI (match_operand:DI 0 "register_operand" "r"))
+	 (match_operand 1 "" "i"))
+  (clobber (match_operand:SI 2 "register_operand" "=d"))]
+  "Pmode == DImode && !TARGET_ABICALLS && TARGET_LONG_CALLS"
+  "MICROBLAZEjal\t%2,%0"
+  [(set_attr "type"	"call")
+  (set_attr "mode"	"none")
+  (set_attr "length"	"1")])
+
+(define_insn "call_internal4a"
+  [(call (mem:SI (match_operand:SI 0 "register_operand" "r"))
+	 (match_operand 1 "" "i"))
+  (clobber (match_operand:SI 2 "register_operand" "=d"))]
+  "!(Pmode == DImode) && TARGET_ABICALLS && TARGET_LONG_CALLS"
+  {
+    if (REGNO (operands[0]) != PIC_FUNCTION_ADDR_REGNUM)
+        return "move\t%^,%0\;MICROBLAZEjal\t%2,%^";
+    else
+        return "MICROBLAZEjal\t%2,%0";
+  }
+  [(set_attr "type"	"call")
+  (set_attr "mode"	"none")
+  (set_attr "length"	"2")])
+
+(define_insn "call_internal4b"
+  [(call (mem:DI (match_operand:DI 0 "register_operand" "r"))
+	 (match_operand 1 "" "i"))
+  (clobber (match_operand:SI 2 "register_operand" "=d"))]
+  "Pmode == DImode && TARGET_ABICALLS && TARGET_LONG_CALLS"
+  {
+    if (REGNO (operands[0]) != PIC_FUNCTION_ADDR_REGNUM)
+        return "move\t%^,%0\;MICROBLAZEjal\t%2,%^";
+    else
+        return "MICROBLAZEjal\t%2,%0";
+  }
+  [(set_attr "type"	"call")
+  (set_attr "mode"	"none")
+  (set_attr "length"	"2")])
+
+;; calls.c now passes a fourth argument, make saber happy
+
+(define_expand "call_value"
+  [(parallel [(set (match_operand 0 "register_operand" "=df")
+		   (call (match_operand 1 "memory_operand" "m")
+			 (match_operand 2 "" "i")))
+             (clobber (reg:SI R_SR))
+             (use (match_operand 3 "" ""))])] ;; next_arg_reg
+  ""
+  {
+    rtx addr;
+    if (operands[0])		/* eliminate unused code warning */
+    {
+        addr = XEXP (operands[1], 0);
+        if ((GET_CODE (addr) != REG && (!CONSTANT_ADDRESS_P (addr) || TARGET_LONG_CALLS))
+            || ! call_insn_operand (addr, VOIDmode))
+            XEXP (operands[1], 0) = copy_to_mode_reg (Pmode, addr);
+
+        /* In order to pass small structures by value in registers
+	   compatibly with the MICROBLAZE compiler, we need to shift the value
+	   into the high part of the register.  Function_arg has encoded
+	   a PARALLEL rtx, holding a vector of adjustments to be made
+	   as the next_arg_reg variable, so we split up the insns,
+	   and emit them separately.  */
+
+        if (operands[3] != (rtx)0 && GET_CODE (operands[3]) == PARALLEL)
+	{
+            rtvec adjust = XVEC (operands[3], 0);
+	    int num = GET_NUM_ELEM (adjust);
+	    int i;
+
+	    for (i = 0; i < num; i++)
+	        emit_insn (RTVEC_ELT (adjust, i));
+	}
+
+        /* We have a call returning a DImode structure in an FP reg.
+	   Strip off the now unnecessary PARALLEL.  */
+      
+        if (GET_CODE (operands[0]) == PARALLEL)
+            operands[0] = XEXP (XVECEXP (operands[0], 0, 0), 0);
+
+        emit_call_insn (gen_call_value_internal0 (operands[0], operands[1], operands[2],
+		        gen_rtx (REG, SImode, GP_REG_FIRST + MB_ABI_SUB_RETURN_ADDR_REGNUM)));
+
+        DONE;
+    }
+  }
+)
+
+
+(define_expand "call_value_internal0"
+  [(parallel [(set (match_operand 0 "" "")
+		   (call (match_operand 1 "" "")
+			 (match_operand 2 "" "")))
+             (clobber (match_operand:SI 3 "" ""))
+             ])]
+  ""
+  {
+    /* fprintf(stderr,"expand \t call_value_internal0  \n"); */
+  }
+)
+
+(define_expand "call_value_expand"
+  [(parallel[(set (match_operand 0 "register_operand" "=df")
+                  (call (mem (match_operand 1 "call_insn_operand" "ri"))
+                        (match_operand 2 "" "i")))
+             (clobber (match_operand:SI 3 "register_operand" "=d"))])]
+  "!TARGET_ABICALLS && !TARGET_LONG_CALLS"
+  {
+    register rtx target = operands[1];
+    register rtx target2=gen_rtx (REG, Pmode,GP_REG_FIRST + MB_ABI_SUB_RETURN_ADDR_REGNUM);
+
+    rtx operand0 = operands[0]; 
+    rtx operand1 = operands[1];
+    rtx operand2 = operands[2]; 
+
+    /*  fprintf(stderr,"expand \t call_value_expand  \n");*/
+
+    if (GET_CODE (target) == SYMBOL_REF) {
+    /*	fprintf(stderr,"In call_value_expand\n");*/
+        gen_call_value_intern (operand0,operand1,operand2,target2);
+	gen_rtx_CLOBBER(VOIDmode,target2);
+	DONE;
+    }
+    else
+        return "MICROBLAZEjal\t%3,%1 #Illegal Code ";
+  }
+)
+
+
+(define_insn "call_value_intern"
+  [(parallel[(set (match_operand:SI 0 "register_operand" "=df")
+                  (call (mem (match_operand:SI 1 "call_insn_operand" "ri"))
+                        (match_operand:SI 2 "" "i")))
+             (clobber (match_operand:SI 3 "register_operand" "=d"))])]
+  ""
+  { 
+    register rtx target = operands[1];
+    register rtx target2=gen_rtx (REG, Pmode,GP_REG_FIRST + MB_ABI_SUB_RETURN_ADDR_REGNUM);
+
+    if (GET_CODE (target) == SYMBOL_REF){
+	gen_rtx_CLOBBER(VOIDmode,target2);
+	return "brlid\tr15,%1\;%#";
+    }
+    else if (GET_CODE (target) == CONST_INT)
+        return "la\t%@,r0,%1\;brald\tr15,%@\;%#";
+    else if (GET_CODE(target) == REG)
+        return "brald\tr15,%1\;%#";	
+    else 
+        fprintf(stderr,"Unsupported call insn\n");
+  }
+  [(set_attr "type"	"call")
+  (set_attr "mode"	"none")
+  (set_attr "length"	"4")])
+
+
+(define_insn "call_value_internal1"
+  [(set (match_operand 0 "register_operand" "=df")
+        (call (mem (match_operand 1 "call_insn_operand" "ri"))
+              (match_operand 2 "" "i")))
+  (clobber (match_operand:SI 3 "register_operand" "=d"))]
+  "!TARGET_ABICALLS && !TARGET_LONG_CALLS"
+  {
+    register rtx target = operands[1];
+
+    if (CONSTANT_ADDRESS_P (target))
+        return "brlid\tr15,%1 ## Need to verify this\;%#";
+    else
+        return "brald\t%3,%1 ## NEed to verify this 2\;%#";
+  }
+  [(set_attr "type"	"call")
+   (set_attr "mode"	"none")
+   (set_attr "length"	"4")]
+)
+
+(define_insn "call_value_internal2"
+  [(set (match_operand 0 "register_operand" "=df")
+        (call (mem (match_operand 1 "call_insn_operand" "ri"))
+              (match_operand 2 "" "i")))
+  (clobber (match_operand:SI 3 "register_operand" "=d"))]
+  "TARGET_ABICALLS && !TARGET_LONG_CALLS"
+  {
+    register rtx target = operands[1];
+
+    if (GET_CODE (target) == SYMBOL_REF)
+    {
+        if (GET_MODE (target) == SImode)
+            return "la\t%^,%1\;MICROBLAZEjal\t%3,%^";
+        else
+            return "dla\t%^,%1\;MICROBLAZEjal\t%3,%^";
+    }
+    else if (GET_CODE (target) == CONST_INT)
+        return "li\t%^,%1\;MICROBLAZEjal\t%3,%^";
+    else if (REGNO (target) != PIC_FUNCTION_ADDR_REGNUM)
+        return "move\t%^,%1\;MICROBLAZEjal\t%3,%^";
+    else
+        return "MICROBLAZEjal\t%3,%1";
+  }
+  [(set_attr "type"	"call")
+  (set_attr "mode"	"none")
+  (set_attr "length"	"2")])
+
+
+
+;; Call subroutine returning any type.
+(define_expand "untyped_call"
+  [(parallel [(call (match_operand 0 "" "")
+		    (const_int 0))
+             (match_operand 1 "" "")
+             (match_operand 2 "" "")])]
+  ""
+  {
+    if (operands[0])		/* silence statement not reached warnings */
+    {
+        int i;
+
+        emit_call_insn (gen_call (operands[0], const0_rtx, NULL, const0_rtx));
+
+        for (i = 0; i < XVECLEN (operands[2], 0); i++)
+	{
+	    rtx set = XVECEXP (operands[2], 0, i);
+	    emit_move_insn (SET_DEST (set), SET_SRC (set));
+	}
+
+        emit_insn (gen_blockage ());
+        DONE;
+      }
+  }
+)
+
+;;----------------------------------------------------------------
+;; Misc.
+;;----------------------------------------------------------------
+
+(define_insn "nop"
+  [(const_int 0)]
+  ""
+  "nop"
+  [(set_attr "type"	"nop")
+  (set_attr "mode"	"none")
+  (set_attr "length"	"4")])
+
+
+;; This does not work reliably. Fails on very -simple- functions
+;; CR 202429
+
+;;(define_expand "stack_check"
+;;   [(const_int 0)
+;;    (use     (reg:SI R_SP))
+;;    (use     (reg:SI R_TMP))]
+;;   "TARGET_STACK_CHECK"
+;;   {
+
+;;     /* Define RTX */
+;;     rtx reg18_rtx = gen_rtx_REG (SImode, R_TMP);
+;;     rtx heap_rtx  = gen_rtx_SYMBOL_REF (SImode, "_malloc_base_addr");
+;;     rtx heap_mem_rtx 
+;;                   = gen_rtx_raw_MEM (SImode, heap_rtx);
+;;     rtx label_ref = gen_rtx_SYMBOL_REF (SImode, "_stack_overflow_exit");
+;;     rtx jinsn, insn;
+
+;;     /* Load Heap */
+;;     emit_move_insn (reg18_rtx, heap_mem_rtx);
+
+;;     /* Negate the heap value */
+;;     emit_insn (gen_negsi2 (reg18_rtx, reg18_rtx));
+
+;;     /* Add the negated heap value to the Stack value */
+;;     insn = emit_insn (gen_addsi3_internal (reg18_rtx, reg18_rtx, stack_pointer_rtx));
+
+;;     /*  Check for overflow and jump  
+;;         if (reg_18 < 0 ) Jump to stack_overflow_exit else continue; 
+;;     */
+;;     jinsn = emit_jump_insn_after (gen_branch_zero (gen_rtx_LE (VOIDmode, reg18_rtx, const0_rtx),
+;;                                                    reg18_rtx, label_ref, pc_rtx), insn);
+;;     DONE;
+;;   }
+;; )     
--- /dev/null
+++ b/gcc/config/microblaze/modsi3.asm
@@ -0,0 +1,74 @@
+###################################-*-asm*- 
+# 
+# Copyright (c) 2001 Xilinx, Inc.  All rights reserved. 
+# 
+# Xilinx, Inc. CONFIDENTIAL 
+# 
+# modsi3.asm 
+# 
+# modulo operation for 32 bit integers.
+#	Input :	op1 in Reg r5
+#		op2 in Reg r6
+#	Output: op1 mod op2 in Reg r3
+# 
+# $Header: /devl/xcs/repo/env/Jobs/MDT/sw/ThirdParty/gnu/src/gcc/src-3.4/gcc/config/microblaze/modsi3.s,v 1.1.2.6 2005/11/15 23:32:47 salindac Exp $
+# 
+#######################################
+
+	.globl	__modsi3
+	.ent	__modsi3
+__modsi3:
+	.frame	r1,0,r15	
+
+	addik	r1,r1,-16
+	swi	r28,r1,0
+	swi	r29,r1,4
+	swi	r30,r1,8
+	swi	r31,r1,12
+
+	BEQI	r6,$LaDiv_By_Zero       # Div_by_Zero   # Division Error
+	BEQI	r5,$LaResult_Is_Zero    # Result is Zero 
+	BGEId	r5,$LaR5_Pos 
+	ADD	r28,r5,r0               # Get the sign of the result [ Depends only on the first arg]
+	RSUBI	r5,r5,0	                # Make r5 positive
+$LaR5_Pos:
+	BGEI	r6,$LaR6_Pos
+	RSUBI	r6,r6,0	    # Make r6 positive
+$LaR6_Pos:
+	ADDIK	r3,r0,0      # Clear mod
+	ADDIK	r30,r0,0     # clear div
+	ADDIK	r29,r0,32    # Initialize the loop count
+   # First part try to find the first '1' in the r5
+$LaDIV1:
+	ADD	r5,r5,r5         # left shift logical r5
+	BGEID	r5,$LaDIV1       #
+	ADDIK	r29,r29,-1
+$LaDIV2:
+	ADD	r5,r5,r5         # left shift logical  r5 get the '1' into the Carry
+	ADDC	r3,r3,r3         # Move that bit into the Mod register
+	rSUB	r31,r6,r3        # Try to subtract (r30 a r6)
+	BLTi	r31,$LaMOD_TOO_SMALL
+	OR	r3,r0,r31       # Move the r31 to mod since the result was positive
+	ADDIK	r30,r30,1
+$LaMOD_TOO_SMALL:
+	ADDIK	r29,r29,-1
+	BEQi	r29,$LaLOOP_END
+	ADD	r30,r30,r30         # Shift in the '1' into div
+	BRI	$LaDIV2          # Div2
+$LaLOOP_END:
+	BGEI	r28,$LaRETURN_HERE
+	BRId	$LaRETURN_HERE
+	rsubi	r3,r3,0 # Negate the result
+$LaDiv_By_Zero:
+$LaResult_Is_Zero:
+	or	r3,r0,r0        # set result to 0 [Both mod as well as div are 0]
+$LaRETURN_HERE:
+# Restore values of CSRs and that of r3 and the divisor and the dividend
+	lwi	r28,r1,0
+	lwi	r29,r1,4
+	lwi	r30,r1,8
+	lwi	r31,r1,12
+	rtsd	r15,8
+	addik	r1,r1,16
+        .end __modsi3
+	
--- /dev/null
+++ b/gcc/config/microblaze/muldi3_hard.asm
@@ -0,0 +1,127 @@
+###################################-*-asm*- 
+# 
+# Copyright (c) 2001 Xilinx, Inc.  All rights reserved. 
+# 
+# Xilinx, Inc. CONFIDENTIAL 
+# 
+# muldi3_hard.asm 
+# 
+# Multiply operation for 64 bit integers, for devices with hard multiply
+#	Input :	Operand1[H] in Reg r5
+#		Operand1[L] in Reg r6		
+#		Operand2[H] in Reg r7
+#		Operand2[L] in Reg r8	
+#	Output: Result[H] in Reg r3
+#		Result[L] in Reg r4	
+# 
+# Explaination:
+#
+# 	Both the input numbers are divided into 16 bit number as follows
+#		op1 = A B C D
+# 		op2 = E F G H
+#	result =    D * H 
+#		 + (C * H + D * G) << 16
+#		 + (B * H + C * G + D * F) << 32
+#		 + (A * H + B * G + C * F + D * E) << 48 
+#
+# 	Only 64 bits of the output are considered
+#
+# $Header: /devl/xcs/repo/env/Jobs/MDT/sw/ThirdParty/gnu/src/gcc/src-3.4/gcc/config/microblaze/muldi3_hard.s,v 1.1.2.6 2005/11/15 23:32:47 salindac Exp $ 
+# 
+#######################################
+
+	.globl	muldi3_hardproc
+	.ent	muldi3_hardproc
+muldi3_hardproc:
+	addi	r1,r1,-40
+
+#  Save the input operands on the caller's stack
+	swi	r5,r1,44
+	swi	r6,r1,48
+	swi	r7,r1,52
+	swi	r8,r1,56
+
+# Store all the callee saved registers 
+	sw	r20,r1,r0
+	swi	r21,r1,4
+	swi	r22,r1,8
+	swi	r23,r1,12
+	swi	r24,r1,16
+	swi	r25,r1,20
+	swi	r26,r1,24
+	swi	r27,r1,28
+
+# Load all the 16 bit values for A thru H
+	lhui	r20,r1,44   # A
+	lhui	r21,r1,46   # B
+	lhui	r22,r1,48   # C
+	lhui	r23,r1,50   # D
+	lhui	r24,r1,52   # E
+	lhui	r25,r1,54   # F
+	lhui	r26,r1,56   # G
+	lhui	r27,r1,58   # H
+
+# D * H ==> LSB of the result on stack ==> Store1
+	mul	r9,r23,r27
+	swi	r9,r1,36    # Pos2 and Pos3
+
+# Hi (Store1) + C * H + D * G ==> Store2 ==> Pos1 and Pos2
+# Store the carry generated in position 2 for Pos 3
+	lhui	r11,r1,36   # Pos2
+	mul	r9,r22,r27   # C * H
+	mul	r10,r23,r26  # D * G
+	add	r9,r9,r10
+	addc	r12,r0,r0
+	add	r9,r9,r11
+	addc	r12,r12,r0    # Store the Carry
+	shi	r9,r1,36    # Store Pos2
+	swi	r9,r1,32 
+	lhui	r11,r1,32
+	shi	r11,r1,34   # Store Pos1
+
+# Hi (Store2) + B * H + C * G + D * F ==> Store3 ==> Pos0 and Pos1
+	mul	r9,r21,r27  # B * H
+	mul	r10,r22,r26 # C * G
+	mul	r7,r23,r25 # D * F	
+	add	r9,r9,r11
+	add	r9,r9,r10
+	add	r9,r9,r7
+	swi	r9,r1,32   # Pos0 and Pos1
+
+# Hi (Store3) + A * H + B * G + C * F + D * E ==> Store3 ==> Pos0
+	lhui	r11,r1,32  # Pos0
+	mul	r9,r20,r27  # A * H
+	mul	r10,r21,r26 # B * G
+	mul	r7,r22,r25 # C * F
+	mul	r8,r23,r24 # D * E
+	add	r9,r9,r11
+	add 	r9,r9,r10
+	add	r9,r9,r7
+	add	r9,r9,r8
+	sext16	r9,r9       # Sign extend the MSB
+	shi	r9,r1,32
+
+# Move results to r3 and r4
+	lhui	r3,r1,32
+	add	r3,r3,r12
+	shi	r3,r1,32
+	lwi	r3,r1,32  # Hi Part
+	lwi	r4,r1,36  # Lo Part
+
+# Restore Callee saved registers
+	lw	r20,r1,r0
+	lwi	r21,r1,4
+	lwi	r22,r1,8
+	lwi	r23,r1,12
+	lwi	r24,r1,16
+	lwi	r25,r1,20
+	lwi	r26,r1,24
+	lwi	r27,r1,28
+
+# Restore Frame and return	
+	rtsd	r15,8
+	addi	r1,r1,40
+
+.end muldi3_hardproc 
+	
+
--- /dev/null
+++ b/gcc/config/microblaze/mulsi3.asm
@@ -0,0 +1,51 @@
+###################################-*-asm*- 
+# 
+# Copyright (c) 2001 Xilinx, Inc.  All rights reserved. 
+# 
+# Xilinx, Inc. CONFIDENTIAL 
+# 
+# mulsi3.asm 
+# 
+# Multiply operation for 32 bit integers.
+#	Input :	Operand1 in Reg r5
+#		Operand2 in Reg r6
+#	Output: Result [op1 * op2] in Reg r3
+# 
+# $Header: /devl/xcs/repo/env/Jobs/MDT/sw/ThirdParty/gnu/src/gcc/src-3.4/gcc/config/microblaze/mulsi3.s,v 1.1.2.6 2005/11/15 23:32:47 salindac Exp $
+# 
+#######################################
+
+	.globl	__mulsi3
+	.ent	__mulsi3
+__mulsi3:
+	.frame	r1,0,r15
+	add	r3,r0,r0
+	BEQI	r5,$L_Result_Is_Zero      # Multiply by Zero
+	BEQI	r6,$L_Result_Is_Zero      # Multiply by Zero
+	BGEId	r5,$L_R5_Pos 
+	XOR	r4,r5,r6                  # Get the sign of the result
+	RSUBI	r5,r5,0	                  # Make r5 positive
+$L_R5_Pos:
+	BGEI	r6,$L_R6_Pos
+	RSUBI	r6,r6,0	                  # Make r6 positive
+$L_R6_Pos:	
+	bri	$L1
+$L2:	
+	add	r5,r5,r5
+$L1:	
+	srl	r6,r6
+	addc	r7,r0,r0
+	beqi	r7,$L2
+	bneid	r6,$L2
+	add	r3,r3,r5	
+	blti	r4,$L_NegateResult			
+	rtsd	r15,8
+	nop
+$L_NegateResult:
+	rtsd	r15,8
+	rsub	r3,r3,r0
+$L_Result_Is_Zero:
+	rtsd	r15,8
+	addi	r3,r0,0
+	.end __mulsi3
+	
\ No newline at end of file
--- /dev/null
+++ b/gcc/config/microblaze/stack_overflow_exit.asm
@@ -0,0 +1,34 @@
+###################################-*-asm*- 
+# 
+# Copyright (c) 2001 Xilinx, Inc.  All rights reserved. 
+# 
+# Xilinx, Inc. CONFIDENTIAL 
+# 
+# stack_overflow_exit.asm
+# 
+# Checks for stack overflows and sets the global variable 
+# stack_overflow_error with the value of current stack pointer
+#
+# This routine exits from the program
+# 
+# $Header: /devl/xcs/repo/env/Jobs/MDT/sw/ThirdParty/gnu/src/gcc/src-3.4/gcc/config/microblaze/stack_overflow_exit.s,v 1.1.2.6 2005/11/15 23:32:47 salindac Exp $
+# 
+#######################################
+
+	.globl	_stack_overflow_error
+	.data
+	.align	2
+	.type	_stack_overflow_error,@object
+	.size	_stack_overflow_error,4
+_stack_overflow_error:
+	.data32	0
+
+	.text 
+	.globl	_stack_overflow_exit	
+	.ent	_stack_overflow_exit
+
+_stack_overflow_exit:
+	swi	r1,r0,_stack_overflow_error
+	bri	exit
+
+	.end 	_stack_overflow_exit
--- /dev/null
+++ b/gcc/config/microblaze/t-microblaze
@@ -0,0 +1,45 @@
+# Other functions
+LIB2FUNCS_EXTRA = $(srcdir)/config/microblaze/divsi3_table.c \
+		  $(srcdir)/config/microblaze/stack_overflow_exit.asm \
+		  $(srcdir)/config/microblaze/mulsi3.asm \
+		  $(srcdir)/config/microblaze/modsi3.asm \
+		  $(srcdir)/config/microblaze/umodsi3.asm \
+		  $(srcdir)/config/microblaze/divsi3.asm \
+		  $(srcdir)/config/microblaze/udivsi3.asm 
+
+# For C++ crtstuff
+EXTRA_MULTILIB_PARTS = crtbegin$(objext) crtend$(objext) crti$(objext) crtn$(objext) 
+
+# Build multiple copies of ?crt{i,n}.o, one for each target switch.
+$(T)crti$(objext): crti.asm
+	$(GCC_FOR_TARGET) $(GCC_CFLAGS) $(INCLUDES) $(MULTILIB_CFLAGS) -c crti.asm -o $(T)crti$(objext)
+
+$(T)crtn$(objext): crtn.asm
+	$(GCC_FOR_TARGET) $(GCC_CFLAGS) $(INCLUDES) $(MULTILIB_CFLAGS) -c crtn.asm -o $(T)crtn$(objext)
+
+# Assemble startup files.
+crti.asm: $(srcdir)/config/microblaze/crti.asm
+	cat $(srcdir)/config/microblaze/crti.asm > crti.asm
+
+crtn.asm: $(srcdir)/config/microblaze/crtn.asm
+	cat $(srcdir)/config/microblaze/crtn.asm > crtn.asm
+
+# We want fine grained libraries, so use the new code
+# to build the floating point emulation libraries.
+FPBIT = fp-bit.c
+DPBIT = dp-bit.c
+
+fp-bit.c: $(srcdir)/config/fp-bit.c
+	echo '#define FLOAT' > fp-bit.c
+	cat $(srcdir)/config/fp-bit.c >> fp-bit.c
+
+dp-bit.c: $(srcdir)/config/fp-bit.c
+	cat $(srcdir)/config/fp-bit.c > dp-bit.c
+
+
+MULTILIB_OPTIONS = mxl-barrel-shift mno-xl-soft-mul
+MULTILIB_DIRNAMES = bs m
+
+## Unfortunately, this does not work. We have to find a way to do this. 
+## Otherwise, -xl-blazeit will cause only the base libgcc to be picked up always.
+## MULTILIB_MATCHES = Zxl-blazeit=mxl-barrel-shift Zxl-blazeit=mno-xl-soft-mul
\ No newline at end of file
--- /dev/null
+++ b/gcc/config/microblaze/udivsi3.asm
@@ -0,0 +1,85 @@
+###################################-*-asm*- 
+# 
+# Copyright (c) 2001 Xilinx, Inc.  All rights reserved. 
+# 
+# Xilinx, Inc. CONFIDENTIAL 
+# 
+# udivsi3.asm 
+# 
+# Unsigned divide operation.
+#	Input :	Divisor in Reg r5
+#		Dividend in Reg r6
+#	Output: Result in Reg r3
+# 
+# $Header: /devl/xcs/repo/env/Jobs/MDT/sw/ThirdParty/gnu/src/gcc/src-3.4/gcc/config/microblaze/udivsi3.s,v 1.1.2.6 2005/11/15 23:32:47 salindac Exp $
+# 
+#######################################
+	
+	.globl	__udivsi3
+	.ent	__udivsi3
+__udivsi3:
+	.frame	r1,0,r15	
+
+	addik r1,r1,-12
+	swi r29,r1,0
+	swi r30,r1,4
+	swi r31,r1,8
+
+	BEQI r6,$LaDiv_By_Zero         # Div_by_Zero   # Division Error
+	BEQId r5,$LaResult_Is_Zero     # Result is Zero 
+	ADDIK r30,r0,0                  # Clear mod
+	ADDIK r29,r0,32  # Initialize the loop count
+
+# Check if r6 and r5 are equal # if yes, return 1
+	rsub 	r18,r5,r6
+	beqid	r18,$LaRETURN_HERE
+	addik	r3,r0,1
+
+# Check if (uns)r6 is greater than (uns)r5. In that case, just return 0
+	xor	r18,r5,r6
+	bgeid	r18,16
+	add	r3,r0,r0  		# We would anyways clear r3
+	blti	r6,$LaRETURN_HERE       # r6[bit 31 = 1] hence is greater
+	bri	$LCheckr6
+	rsub	r18,r6,r5 # MICROBLAZEcmp
+	blti	r18,$LaRETURN_HERE
+
+# If r6 [bit 31] is set, then return result as 1
+$LCheckr6:
+	bgti	r6,$LaDIV0
+	brid	$LaRETURN_HERE
+	addik	r3,r0,1
+
+   # First part try to find the first '1' in the r5
+$LaDIV0:
+	BLTI r5,$LaDIV2	
+$LaDIV1:
+	ADD r5,r5,r5     # left shift logical r5
+	BGEID r5,$LaDIV1       #
+	ADDIK r29,r29,-1
+$LaDIV2:
+	ADD r5,r5,r5     # left shift logical  r5 get the '1' into the Carry
+	ADDC r30,r30,r30 # Move that bit into the Mod register
+	rSUB r31,r6,r30 # Try to subtract (r30 a r6)
+	BLTi r31,$LaMOD_TOO_SMALL
+	OR  r30,r0,r31  # Move the r31 to mod since the result was positive
+	ADDIK r3,r3,1
+$LaMOD_TOO_SMALL:
+	ADDIK r29,r29,-1
+	BEQi r29,$LaLOOP_END
+	ADD r3,r3,r3 # Shift in the '1' into div
+	BRI $LaDIV2   # Div2
+$LaLOOP_END:
+	BRI $LaRETURN_HERE
+$LaDiv_By_Zero:
+$LaResult_Is_Zero:
+	or r3,r0,r0 # set result to 0
+$LaRETURN_HERE:
+# Restore values of CSRs and that of r3 and the divisor and the dividend
+	lwi r29,r1,0
+	lwi r30,r1,4
+	lwi r31,r1,8
+	rtsd r15,8
+	addik r1,r1,12
+        .end __udivsi3
+	
--- /dev/null
+++ b/gcc/config/microblaze/umodsi3.asm
@@ -0,0 +1,90 @@
+###################################-*-asm*- 
+# 
+# Copyright (c) 2001 Xilinx, Inc.  All rights reserved. 
+# 
+# Xilinx, Inc. CONFIDENTIAL 
+# 
+# umodsi3.asm 
+#
+# Unsigned modulo operation for 32 bit integers.
+#	Input :	op1 in Reg r5
+#		op2 in Reg r6
+#	Output: op1 mod op2 in Reg r3
+# 
+# Unsigned mod operation.
+# 
+# $Header: /devl/xcs/repo/env/Jobs/MDT/sw/ThirdParty/gnu/src/gcc/src-3.4/gcc/config/microblaze/umodsi3.s,v 1.1.2.6 2005/11/15 23:32:47 salindac Exp $
+# 
+#######################################
+	
+	.globl	__umodsi3
+	.ent	__umodsi3
+__umodsi3:
+	.frame	r1,0,r15	
+
+	addik	r1,r1,-12
+	swi	r29,r1,0
+	swi	r30,r1,4
+	swi	r31,r1,8
+
+	BEQI	r6,$LaDiv_By_Zero         # Div_by_Zero   # Division Error
+	BEQId	r5,$LaResult_Is_Zero     # Result is Zero 
+	ADDIK 	r3,r0,0                  # Clear div
+	ADDIK 	r30,r0,0     	# clear mod
+	ADDIK 	r29,r0,32       # Initialize the loop count
+
+# Check if r6 and r5 are equal # if yes, return 0
+	rsub 	r18,r5,r6
+	beqi	r18,$LaRETURN_HERE
+
+# Check if (uns)r6 is greater than (uns)r5. In that case, just return r5
+	xor	r18,r5,r6
+	bgeid	r18,16
+	addik	r3,r5,0
+	blti	r6,$LaRETURN_HERE
+	bri	$LCheckr6
+	rsub	r18,r5,r6 # MICROBLAZEcmp
+	bgti	r18,$LaRETURN_HERE
+
+# If r6 [bit 31] is set, then return result as r5-r6
+$LCheckr6:
+	bgtid	r6,$LaDIV0
+	addik	r3,r0,0
+	addik	r18,r0,0x7fffffff
+	and	r5,r5,r18
+	and 	r6,r6,r18
+	brid	$LaRETURN_HERE
+	rsub	r3,r6,r5
+# First part: try to find the first '1' in the r5
+$LaDIV0:
+	BLTI	r5,$LaDIV2
+$LaDIV1:
+	ADD 	r5,r5,r5     # left shift logical r5
+	BGEID 	r5,$LaDIV1   #
+	ADDIK 	r29,r29,-1
+$LaDIV2:
+	ADD 	r5,r5,r5     # left shift logical  r5 get the '1' into the Carry
+	ADDC 	r3,r3,r3     # Move that bit into the Mod register
+	rSUB 	r31,r6,r3    # Try to subtract (r3 a r6)
+	BLTi 	r31,$LaMOD_TOO_SMALL
+	OR  	r3,r0,r31    # Move the r31 to mod since the result was positive
+	ADDIK 	r30,r30,1
+$LaMOD_TOO_SMALL:
+	ADDIK 	r29,r29,-1
+	BEQi 	r29,$LaLOOP_END
+	ADD 	r30,r30,r30 # Shift in the '1' into div
+	BRI 	$LaDIV2     # Div2
+$LaLOOP_END:
+	BRI 	$LaRETURN_HERE
+$LaDiv_By_Zero:
+$LaResult_Is_Zero:
+	or 	r3,r0,r0   # set result to 0
+$LaRETURN_HERE:
+# Restore values of CSRs and that of r3 and the divisor and the dividend
+	lwi 	r29,r1,0
+	lwi 	r30,r1,4
+	lwi 	r31,r1,8
+	rtsd 	r15,8
+	addik 	r1,r1,12
+.end __umodsi3
+	
--- /dev/null
+++ b/gcc/config/microblaze/x-microblaze
@@ -0,0 +1,20 @@
+# Define CC and OLDCC as the same, so that the tests:
+#	if [ x"$(OLDCC)" = x"$(CC)" ] ...
+#
+# will succeed (if OLDCC != CC, it is assumed that GCC is
+# being used in secondary stage builds).  We need to pass
+# the -Wf,-XNg1500 option so the compiler can compile the
+# G++ file cp-parse.c.  Otherwise it complains about
+# too many case statements.  The -Olimit is so the user
+# can use -O2.  Down with fixed size tables!
+
+CC		= $(OLDCC)
+OPT		= -O1
+OLDCC		= cc -Wf,-XNg1500,-XNh2000 -Olimit 3000 $(OPT)
+
+# The bison output files are machine-indep,
+# so different flags for a particular machine are not useful.
+#BISONFLAGS	= -l
+
+# This is so we can link collect2 running native.
+CLIB		= -lmld
--- /dev/null
+++ b/gcc/config/microblaze/xm-microblaze.h
@@ -0,0 +1,76 @@
+/* Configuration for GNU C-compiler for MICROBLAZE Rx000 family
+   Copyright (C) 1989, 1990, 1991, 1993, 1997 Free Software Foundation, Inc.
+
+This file is part of GNU CC.
+
+GNU CC is free software; you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation; either version 2, or (at your option)
+any later version.
+
+GNU CC is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with GNU CC; see the file COPYING.  If not, write to
+the Free Software Foundation, 59 Temple Place - Suite 330,
+Boston, MA 02111-1307, USA.  */
+
+
+/* #defines that need visibility everywhere.  */
+#define FALSE 0
+#define TRUE 1
+
+/* This describes the machine the compiler is hosted on.  */
+#define HOST_BITS_PER_CHAR 8
+#define HOST_BITS_PER_SHORT 16
+#define HOST_BITS_PER_INT 32
+#define HOST_BITS_PER_LONG 32
+#define HOST_BITS_PER_LONGLONG 64
+
+#if !defined(MICROBLAZEEL) && !defined(__MICROBLAZEEL__)
+#define HOST_WORDS_BIG_ENDIAN
+#endif
+
+/* Enable host-conditionals for MICROBLAZE machines.  */
+#ifndef MICROBLAZE
+#define MICROBLAZE 1
+#endif
+
+/* A code distinguishing the floating point format of the host
+   machine.  There are three defined values: IEEE_FLOAT_FORMAT,
+   VAX_FLOAT_FORMAT, and UNKNOWN_FLOAT_FORMAT.  */
+
+#define HOST_FLOAT_FORMAT IEEE_FLOAT_FORMAT
+
+/* target machine dependencies.
+   tm.h is a symbolic link to the actual target specific file.   */
+#include "tm.h"
+
+/* Arguments to use with `exit'.  */
+#define SUCCESS_EXIT_CODE 0
+#define FATAL_EXIT_CODE 33
+
+#ifndef __GNUC__
+/* The MICROBLAZE compiler gets it wrong, and treats enumerated bitfields
+   as signed quantities, making it impossible to use an 8-bit enum
+   for compiling GNU C++.  */
+#define ONLY_INT_FIELDS 1
+#endif
+
+#ifndef MICROBLAZE_OVERRIDE_ALLOCA
+#ifndef __GNUC__
+#define USE_C_ALLOCA
+
+#ifdef __STDC__
+extern void * alloca ();
+#else
+extern char * alloca ();
+#endif
+
+/* for the emacs version of alloca */
+#define STACK_DIRECTION	-1
+#endif
+#endif /* not MICROBLAZE_OVERRIDE_ALLOCA */
--- /dev/null
+++ b/gcc/fold-const.c.orig
@@ -0,0 +1,11862 @@
+/* Fold a constant sub-tree into a single node for C-compiler
+   Copyright (C) 1987, 1988, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999,
+   2000, 2001, 2002, 2003, 2004, 2005 Free Software Foundation, Inc.
+
+This file is part of GCC.
+
+GCC is free software; you can redistribute it and/or modify it under
+the terms of the GNU General Public License as published by the Free
+Software Foundation; either version 2, or (at your option) any later
+version.
+
+GCC is distributed in the hope that it will be useful, but WITHOUT ANY
+WARRANTY; without even the implied warranty of MERCHANTABILITY or
+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+for more details.
+
+You should have received a copy of the GNU General Public License
+along with GCC; see the file COPYING.  If not, write to the Free
+Software Foundation, 51 Franklin Street, Fifth Floor, Boston, MA
+02110-1301, USA.  */
+
+/*@@ This file should be rewritten to use an arbitrary precision
+  @@ representation for "struct tree_int_cst" and "struct tree_real_cst".
+  @@ Perhaps the routines could also be used for bc/dc, and made a lib.
+  @@ The routines that translate from the ap rep should
+  @@ warn if precision et. al. is lost.
+  @@ This would also make life easier when this technology is used
+  @@ for cross-compilers.  */
+
+/* The entry points in this file are fold, size_int_wide, size_binop
+   and force_fit_type.
+
+   fold takes a tree as argument and returns a simplified tree.
+
+   size_binop takes a tree code for an arithmetic operation
+   and two operands that are trees, and produces a tree for the
+   result, assuming the type comes from `sizetype'.
+
+   size_int takes an integer value, and creates a tree constant
+   with type from `sizetype'.
+
+   force_fit_type takes a constant, an overflowable flag and prior
+   overflow indicators.  It forces the value to fit the type and sets
+   TREE_OVERFLOW and TREE_CONSTANT_OVERFLOW as appropriate.  */
+
+#include "config.h"
+#include "system.h"
+#include "coretypes.h"
+#include "tm.h"
+#include "flags.h"
+#include "tree.h"
+#include "real.h"
+#include "rtl.h"
+#include "expr.h"
+#include "tm_p.h"
+#include "toplev.h"
+#include "ggc.h"
+#include "hashtab.h"
+#include "langhooks.h"
+#include "md5.h"
+
+/* The following constants represent a bit based encoding of GCC's
+   comparison operators.  This encoding simplifies transformations
+   on relational comparison operators, such as AND and OR.  */
+enum comparison_code {
+  COMPCODE_FALSE = 0,
+  COMPCODE_LT = 1,
+  COMPCODE_EQ = 2,
+  COMPCODE_LE = 3,
+  COMPCODE_GT = 4,
+  COMPCODE_LTGT = 5,
+  COMPCODE_GE = 6,
+  COMPCODE_ORD = 7,
+  COMPCODE_UNORD = 8,
+  COMPCODE_UNLT = 9,
+  COMPCODE_UNEQ = 10,
+  COMPCODE_UNLE = 11,
+  COMPCODE_UNGT = 12,
+  COMPCODE_NE = 13,
+  COMPCODE_UNGE = 14,
+  COMPCODE_TRUE = 15
+};
+
+static void encode (HOST_WIDE_INT *, unsigned HOST_WIDE_INT, HOST_WIDE_INT);
+static void decode (HOST_WIDE_INT *, unsigned HOST_WIDE_INT *, HOST_WIDE_INT *);
+static bool negate_mathfn_p (enum built_in_function);
+static bool negate_expr_p (tree);
+static tree negate_expr (tree);
+static tree split_tree (tree, enum tree_code, tree *, tree *, tree *, int);
+static tree associate_trees (tree, tree, enum tree_code, tree);
+static tree const_binop (enum tree_code, tree, tree, int);
+static enum comparison_code comparison_to_compcode (enum tree_code);
+static enum tree_code compcode_to_comparison (enum comparison_code);
+static tree combine_comparisons (enum tree_code, enum tree_code,
+				 enum tree_code, tree, tree, tree);
+static int truth_value_p (enum tree_code);
+static int operand_equal_for_comparison_p (tree, tree, tree);
+static int twoval_comparison_p (tree, tree *, tree *, int *);
+static tree eval_subst (tree, tree, tree, tree, tree);
+static tree pedantic_omit_one_operand (tree, tree, tree);
+static tree distribute_bit_expr (enum tree_code, tree, tree, tree);
+static tree make_bit_field_ref (tree, tree, int, int, int);
+static tree optimize_bit_field_compare (enum tree_code, tree, tree, tree);
+static tree decode_field_reference (tree, HOST_WIDE_INT *, HOST_WIDE_INT *,
+				    enum machine_mode *, int *, int *,
+				    tree *, tree *);
+static int all_ones_mask_p (tree, int);
+static tree sign_bit_p (tree, tree);
+static int simple_operand_p (tree);
+static tree range_binop (enum tree_code, tree, tree, int, tree, int);
+static tree make_range (tree, int *, tree *, tree *);
+static tree build_range_check (tree, tree, int, tree, tree);
+static int merge_ranges (int *, tree *, tree *, int, tree, tree, int, tree,
+			 tree);
+static tree fold_range_test (enum tree_code, tree, tree, tree);
+static tree fold_cond_expr_with_comparison (tree, tree, tree, tree);
+static tree unextend (tree, int, int, tree);
+static tree fold_truthop (enum tree_code, tree, tree, tree);
+static tree optimize_minmax_comparison (enum tree_code, tree, tree, tree);
+static tree extract_muldiv (tree, tree, enum tree_code, tree);
+static tree extract_muldiv_1 (tree, tree, enum tree_code, tree);
+static int multiple_of_p (tree, tree, tree);
+static tree fold_binary_op_with_conditional_arg (enum tree_code, tree,
+						 tree, tree,
+						 tree, tree, int);
+static bool fold_real_zero_addition_p (tree, tree, int);
+static tree fold_mathfn_compare (enum built_in_function, enum tree_code,
+				 tree, tree, tree);
+static tree fold_inf_compare (enum tree_code, tree, tree, tree);
+static tree fold_div_compare (enum tree_code, tree, tree, tree);
+static bool reorder_operands_p (tree, tree);
+static tree fold_negate_const (tree, tree);
+static tree fold_not_const (tree, tree);
+static tree fold_relational_const (enum tree_code, tree, tree, tree);
+
+/* We know that A1 + B1 = SUM1, using 2's complement arithmetic and ignoring
+   overflow.  Suppose A, B and SUM have the same respective signs as A1, B1,
+   and SUM1.  Then this yields nonzero if overflow occurred during the
+   addition.
+
+   Overflow occurs if A and B have the same sign, but A and SUM differ in
+   sign.  Use `^' to test whether signs differ, and `< 0' to isolate the
+   sign.  */
+#define OVERFLOW_SUM_SIGN(a, b, sum) ((~((a) ^ (b)) & ((a) ^ (sum))) < 0)
+
+/* To do constant folding on INTEGER_CST nodes requires two-word arithmetic.
+   We do that by representing the two-word integer in 4 words, with only
+   HOST_BITS_PER_WIDE_INT / 2 bits stored in each word, as a positive
+   number.  The value of the word is LOWPART + HIGHPART * BASE.  */
+
+#define LOWPART(x) \
+  ((x) & (((unsigned HOST_WIDE_INT) 1 << (HOST_BITS_PER_WIDE_INT / 2)) - 1))
+#define HIGHPART(x) \
+  ((unsigned HOST_WIDE_INT) (x) >> HOST_BITS_PER_WIDE_INT / 2)
+#define BASE ((unsigned HOST_WIDE_INT) 1 << HOST_BITS_PER_WIDE_INT / 2)
+
+/* Unpack a two-word integer into 4 words.
+   LOW and HI are the integer, as two `HOST_WIDE_INT' pieces.
+   WORDS points to the array of HOST_WIDE_INTs.  */
+
+static void
+encode (HOST_WIDE_INT *words, unsigned HOST_WIDE_INT low, HOST_WIDE_INT hi)
+{
+  words[0] = LOWPART (low);
+  words[1] = HIGHPART (low);
+  words[2] = LOWPART (hi);
+  words[3] = HIGHPART (hi);
+}
+
+/* Pack an array of 4 words into a two-word integer.
+   WORDS points to the array of words.
+   The integer is stored into *LOW and *HI as two `HOST_WIDE_INT' pieces.  */
+
+static void
+decode (HOST_WIDE_INT *words, unsigned HOST_WIDE_INT *low,
+	HOST_WIDE_INT *hi)
+{
+  *low = words[0] + words[1] * BASE;
+  *hi = words[2] + words[3] * BASE;
+}
+
+/* T is an INT_CST node.  OVERFLOWABLE indicates if we are interested
+   in overflow of the value, when >0 we are only interested in signed
+   overflow, for <0 we are interested in any overflow.  OVERFLOWED
+   indicates whether overflow has already occurred.  CONST_OVERFLOWED
+   indicates whether constant overflow has already occurred.  We force
+   T's value to be within range of T's type (by setting to 0 or 1 all
+   the bits outside the type's range).  We set TREE_OVERFLOWED if,
+  	OVERFLOWED is nonzero,
+	or OVERFLOWABLE is >0 and signed overflow occurs
+	or OVERFLOWABLE is <0 and any overflow occurs
+   We set TREE_CONSTANT_OVERFLOWED if,
+        CONST_OVERFLOWED is nonzero
+	or we set TREE_OVERFLOWED.
+  We return either the original T, or a copy.  */
+
+tree
+force_fit_type (tree t, int overflowable,
+		bool overflowed, bool overflowed_const)
+{
+  unsigned HOST_WIDE_INT low;
+  HOST_WIDE_INT high;
+  unsigned int prec;
+  int sign_extended_type;
+
+  gcc_assert (TREE_CODE (t) == INTEGER_CST);
+
+  low = TREE_INT_CST_LOW (t);
+  high = TREE_INT_CST_HIGH (t);
+
+  if (POINTER_TYPE_P (TREE_TYPE (t))
+      || TREE_CODE (TREE_TYPE (t)) == OFFSET_TYPE)
+    prec = POINTER_SIZE;
+  else
+    prec = TYPE_PRECISION (TREE_TYPE (t));
+  /* Size types *are* sign extended.  */
+  sign_extended_type = (!TYPE_UNSIGNED (TREE_TYPE (t))
+			|| (TREE_CODE (TREE_TYPE (t)) == INTEGER_TYPE
+			    && TYPE_IS_SIZETYPE (TREE_TYPE (t))));
+
+  /* First clear all bits that are beyond the type's precision.  */
+
+  if (prec >= 2 * HOST_BITS_PER_WIDE_INT)
+    ;
+  else if (prec > HOST_BITS_PER_WIDE_INT)
+    high &= ~((HOST_WIDE_INT) (-1) << (prec - HOST_BITS_PER_WIDE_INT));
+  else
+    {
+      high = 0;
+      if (prec < HOST_BITS_PER_WIDE_INT)
+	low &= ~((HOST_WIDE_INT) (-1) << prec);
+    }
+
+  if (!sign_extended_type)
+    /* No sign extension */;
+  else if (prec >= 2 * HOST_BITS_PER_WIDE_INT)
+    /* Correct width already.  */;
+  else if (prec > HOST_BITS_PER_WIDE_INT)
+    {
+      /* Sign extend top half? */
+      if (high & ((unsigned HOST_WIDE_INT)1
+		  << (prec - HOST_BITS_PER_WIDE_INT - 1)))
+	high |= (HOST_WIDE_INT) (-1) << (prec - HOST_BITS_PER_WIDE_INT);
+    }
+  else if (prec == HOST_BITS_PER_WIDE_INT)
+    {
+      if ((HOST_WIDE_INT)low < 0)
+	high = -1;
+    }
+  else
+    {
+      /* Sign extend bottom half? */
+      if (low & ((unsigned HOST_WIDE_INT)1 << (prec - 1)))
+	{
+	  high = -1;
+	  low |= (HOST_WIDE_INT)(-1) << prec;
+	}
+    }
+
+  /* If the value changed, return a new node.  */
+  if (overflowed || overflowed_const
+      || low != TREE_INT_CST_LOW (t) || high != TREE_INT_CST_HIGH (t))
+    {
+      t = build_int_cst_wide (TREE_TYPE (t), low, high);
+
+      if (overflowed
+	  || overflowable < 0
+	  || (overflowable > 0 && sign_extended_type))
+	{
+	  t = copy_node (t);
+	  TREE_OVERFLOW (t) = 1;
+	  TREE_CONSTANT_OVERFLOW (t) = 1;
+	}
+      else if (overflowed_const)
+	{
+	  t = copy_node (t);
+	  TREE_CONSTANT_OVERFLOW (t) = 1;
+	}
+    }
+
+  return t;
+}
+
+/* Add two doubleword integers with doubleword result.
+   Each argument is given as two `HOST_WIDE_INT' pieces.
+   One argument is L1 and H1; the other, L2 and H2.
+   The value is stored as two `HOST_WIDE_INT' pieces in *LV and *HV.  */
+
+int
+add_double (unsigned HOST_WIDE_INT l1, HOST_WIDE_INT h1,
+	    unsigned HOST_WIDE_INT l2, HOST_WIDE_INT h2,
+	    unsigned HOST_WIDE_INT *lv, HOST_WIDE_INT *hv)
+{
+  unsigned HOST_WIDE_INT l;
+  HOST_WIDE_INT h;
+
+  l = l1 + l2;
+  h = h1 + h2 + (l < l1);
+
+  *lv = l;
+  *hv = h;
+  return OVERFLOW_SUM_SIGN (h1, h2, h);
+}
+
+/* Negate a doubleword integer with doubleword result.
+   Return nonzero if the operation overflows, assuming it's signed.
+   The argument is given as two `HOST_WIDE_INT' pieces in L1 and H1.
+   The value is stored as two `HOST_WIDE_INT' pieces in *LV and *HV.  */
+
+int
+neg_double (unsigned HOST_WIDE_INT l1, HOST_WIDE_INT h1,
+	    unsigned HOST_WIDE_INT *lv, HOST_WIDE_INT *hv)
+{
+  if (l1 == 0)
+    {
+      *lv = 0;
+      *hv = - h1;
+      return (*hv & h1) < 0;
+    }
+  else
+    {
+      *lv = -l1;
+      *hv = ~h1;
+      return 0;
+    }
+}
+
+/* Multiply two doubleword integers with doubleword result.
+   Return nonzero if the operation overflows, assuming it's signed.
+   Each argument is given as two `HOST_WIDE_INT' pieces.
+   One argument is L1 and H1; the other, L2 and H2.
+   The value is stored as two `HOST_WIDE_INT' pieces in *LV and *HV.  */
+
+int
+mul_double (unsigned HOST_WIDE_INT l1, HOST_WIDE_INT h1,
+	    unsigned HOST_WIDE_INT l2, HOST_WIDE_INT h2,
+	    unsigned HOST_WIDE_INT *lv, HOST_WIDE_INT *hv)
+{
+  HOST_WIDE_INT arg1[4];
+  HOST_WIDE_INT arg2[4];
+  HOST_WIDE_INT prod[4 * 2];
+  unsigned HOST_WIDE_INT carry;
+  int i, j, k;
+  unsigned HOST_WIDE_INT toplow, neglow;
+  HOST_WIDE_INT tophigh, neghigh;
+
+  encode (arg1, l1, h1);
+  encode (arg2, l2, h2);
+
+  memset (prod, 0, sizeof prod);
+
+  for (i = 0; i < 4; i++)
+    {
+      carry = 0;
+      for (j = 0; j < 4; j++)
+	{
+	  k = i + j;
+	  /* This product is <= 0xFFFE0001, the sum <= 0xFFFF0000.  */
+	  carry += arg1[i] * arg2[j];
+	  /* Since prod[p] < 0xFFFF, this sum <= 0xFFFFFFFF.  */
+	  carry += prod[k];
+	  prod[k] = LOWPART (carry);
+	  carry = HIGHPART (carry);
+	}
+      prod[i + 4] = carry;
+    }
+
+  decode (prod, lv, hv);	/* This ignores prod[4] through prod[4*2-1] */
+
+  /* Check for overflow by calculating the top half of the answer in full;
+     it should agree with the low half's sign bit.  */
+  decode (prod + 4, &toplow, &tophigh);
+  if (h1 < 0)
+    {
+      neg_double (l2, h2, &neglow, &neghigh);
+      add_double (neglow, neghigh, toplow, tophigh, &toplow, &tophigh);
+    }
+  if (h2 < 0)
+    {
+      neg_double (l1, h1, &neglow, &neghigh);
+      add_double (neglow, neghigh, toplow, tophigh, &toplow, &tophigh);
+    }
+  return (*hv < 0 ? ~(toplow & tophigh) : toplow | tophigh) != 0;
+}
+
+/* Shift the doubleword integer in L1, H1 left by COUNT places
+   keeping only PREC bits of result.
+   Shift right if COUNT is negative.
+   ARITH nonzero specifies arithmetic shifting; otherwise use logical shift.
+   Store the value as two `HOST_WIDE_INT' pieces in *LV and *HV.  */
+
+void
+lshift_double (unsigned HOST_WIDE_INT l1, HOST_WIDE_INT h1,
+	       HOST_WIDE_INT count, unsigned int prec,
+	       unsigned HOST_WIDE_INT *lv, HOST_WIDE_INT *hv, int arith)
+{
+  unsigned HOST_WIDE_INT signmask;
+
+  if (count < 0)
+    {
+      rshift_double (l1, h1, -count, prec, lv, hv, arith);
+      return;
+    }
+
+  if (SHIFT_COUNT_TRUNCATED)
+    count %= prec;
+
+  if (count >= 2 * HOST_BITS_PER_WIDE_INT)
+    {
+      /* Shifting by the host word size is undefined according to the
+	 ANSI standard, so we must handle this as a special case.  */
+      *hv = 0;
+      *lv = 0;
+    }
+  else if (count >= HOST_BITS_PER_WIDE_INT)
+    {
+      *hv = l1 << (count - HOST_BITS_PER_WIDE_INT);
+      *lv = 0;
+    }
+  else
+    {
+      *hv = (((unsigned HOST_WIDE_INT) h1 << count)
+	     | (l1 >> (HOST_BITS_PER_WIDE_INT - count - 1) >> 1));
+      *lv = l1 << count;
+    }
+
+  /* Sign extend all bits that are beyond the precision.  */
+
+  signmask = -((prec > HOST_BITS_PER_WIDE_INT
+		? ((unsigned HOST_WIDE_INT) *hv
+		   >> (prec - HOST_BITS_PER_WIDE_INT - 1))
+		: (*lv >> (prec - 1))) & 1);
+
+  if (prec >= 2 * HOST_BITS_PER_WIDE_INT)
+    ;
+  else if (prec >= HOST_BITS_PER_WIDE_INT)
+    {
+      *hv &= ~((HOST_WIDE_INT) (-1) << (prec - HOST_BITS_PER_WIDE_INT));
+      *hv |= signmask << (prec - HOST_BITS_PER_WIDE_INT);
+    }
+  else
+    {
+      *hv = signmask;
+      *lv &= ~((unsigned HOST_WIDE_INT) (-1) << prec);
+      *lv |= signmask << prec;
+    }
+}
+
+/* Shift the doubleword integer in L1, H1 right by COUNT places
+   keeping only PREC bits of result.  COUNT must be positive.
+   ARITH nonzero specifies arithmetic shifting; otherwise use logical shift.
+   Store the value as two `HOST_WIDE_INT' pieces in *LV and *HV.  */
+
+void
+rshift_double (unsigned HOST_WIDE_INT l1, HOST_WIDE_INT h1,
+	       HOST_WIDE_INT count, unsigned int prec,
+	       unsigned HOST_WIDE_INT *lv, HOST_WIDE_INT *hv,
+	       int arith)
+{
+  unsigned HOST_WIDE_INT signmask;
+
+  signmask = (arith
+	      ? -((unsigned HOST_WIDE_INT) h1 >> (HOST_BITS_PER_WIDE_INT - 1))
+	      : 0);
+
+  if (SHIFT_COUNT_TRUNCATED)
+    count %= prec;
+
+  if (count >= 2 * HOST_BITS_PER_WIDE_INT)
+    {
+      /* Shifting by the host word size is undefined according to the
+	 ANSI standard, so we must handle this as a special case.  */
+      *hv = 0;
+      *lv = 0;
+    }
+  else if (count >= HOST_BITS_PER_WIDE_INT)
+    {
+      *hv = 0;
+      *lv = (unsigned HOST_WIDE_INT) h1 >> (count - HOST_BITS_PER_WIDE_INT);
+    }
+  else
+    {
+      *hv = (unsigned HOST_WIDE_INT) h1 >> count;
+      *lv = ((l1 >> count)
+	     | ((unsigned HOST_WIDE_INT) h1 << (HOST_BITS_PER_WIDE_INT - count - 1) << 1));
+    }
+
+  /* Zero / sign extend all bits that are beyond the precision.  */
+
+  if (count >= (HOST_WIDE_INT)prec)
+    {
+      *hv = signmask;
+      *lv = signmask;
+    }
+  else if ((prec - count) >= 2 * HOST_BITS_PER_WIDE_INT)
+    ;
+  else if ((prec - count) >= HOST_BITS_PER_WIDE_INT)
+    {
+      *hv &= ~((HOST_WIDE_INT) (-1) << (prec - count - HOST_BITS_PER_WIDE_INT));
+      *hv |= signmask << (prec - count - HOST_BITS_PER_WIDE_INT);
+    }
+  else
+    {
+      *hv = signmask;
+      *lv &= ~((unsigned HOST_WIDE_INT) (-1) << (prec - count));
+      *lv |= signmask << (prec - count);
+    }
+}
+
+/* Rotate the doubleword integer in L1, H1 left by COUNT places
+   keeping only PREC bits of result.
+   Rotate right if COUNT is negative.
+   Store the value as two `HOST_WIDE_INT' pieces in *LV and *HV.  */
+
+void
+lrotate_double (unsigned HOST_WIDE_INT l1, HOST_WIDE_INT h1,
+		HOST_WIDE_INT count, unsigned int prec,
+		unsigned HOST_WIDE_INT *lv, HOST_WIDE_INT *hv)
+{
+  unsigned HOST_WIDE_INT s1l, s2l;
+  HOST_WIDE_INT s1h, s2h;
+
+  count %= prec;
+  if (count < 0)
+    count += prec;
+
+  lshift_double (l1, h1, count, prec, &s1l, &s1h, 0);
+  rshift_double (l1, h1, prec - count, prec, &s2l, &s2h, 0);
+  *lv = s1l | s2l;
+  *hv = s1h | s2h;
+}
+
+/* Rotate the doubleword integer in L1, H1 left by COUNT places
+   keeping only PREC bits of result.  COUNT must be positive.
+   Store the value as two `HOST_WIDE_INT' pieces in *LV and *HV.  */
+
+void
+rrotate_double (unsigned HOST_WIDE_INT l1, HOST_WIDE_INT h1,
+		HOST_WIDE_INT count, unsigned int prec,
+		unsigned HOST_WIDE_INT *lv, HOST_WIDE_INT *hv)
+{
+  unsigned HOST_WIDE_INT s1l, s2l;
+  HOST_WIDE_INT s1h, s2h;
+
+  count %= prec;
+  if (count < 0)
+    count += prec;
+
+  rshift_double (l1, h1, count, prec, &s1l, &s1h, 0);
+  lshift_double (l1, h1, prec - count, prec, &s2l, &s2h, 0);
+  *lv = s1l | s2l;
+  *hv = s1h | s2h;
+}
+
+/* Divide doubleword integer LNUM, HNUM by doubleword integer LDEN, HDEN
+   for a quotient (stored in *LQUO, *HQUO) and remainder (in *LREM, *HREM).
+   CODE is a tree code for a kind of division, one of
+   TRUNC_DIV_EXPR, FLOOR_DIV_EXPR, CEIL_DIV_EXPR, ROUND_DIV_EXPR
+   or EXACT_DIV_EXPR
+   It controls how the quotient is rounded to an integer.
+   Return nonzero if the operation overflows.
+   UNS nonzero says do unsigned division.  */
+
+int
+div_and_round_double (enum tree_code code, int uns,
+		      unsigned HOST_WIDE_INT lnum_orig, /* num == numerator == dividend */
+		      HOST_WIDE_INT hnum_orig,
+		      unsigned HOST_WIDE_INT lden_orig, /* den == denominator == divisor */
+		      HOST_WIDE_INT hden_orig,
+		      unsigned HOST_WIDE_INT *lquo,
+		      HOST_WIDE_INT *hquo, unsigned HOST_WIDE_INT *lrem,
+		      HOST_WIDE_INT *hrem)
+{
+  int quo_neg = 0;
+  HOST_WIDE_INT num[4 + 1];	/* extra element for scaling.  */
+  HOST_WIDE_INT den[4], quo[4];
+  int i, j;
+  unsigned HOST_WIDE_INT work;
+  unsigned HOST_WIDE_INT carry = 0;
+  unsigned HOST_WIDE_INT lnum = lnum_orig;
+  HOST_WIDE_INT hnum = hnum_orig;
+  unsigned HOST_WIDE_INT lden = lden_orig;
+  HOST_WIDE_INT hden = hden_orig;
+  int overflow = 0;
+
+  if (hden == 0 && lden == 0)
+    overflow = 1, lden = 1;
+
+  /* Calculate quotient sign and convert operands to unsigned.  */
+  if (!uns)
+    {
+      if (hnum < 0)
+	{
+	  quo_neg = ~ quo_neg;
+	  /* (minimum integer) / (-1) is the only overflow case.  */
+	  if (neg_double (lnum, hnum, &lnum, &hnum)
+	      && ((HOST_WIDE_INT) lden & hden) == -1)
+	    overflow = 1;
+	}
+      if (hden < 0)
+	{
+	  quo_neg = ~ quo_neg;
+	  neg_double (lden, hden, &lden, &hden);
+	}
+    }
+
+  if (hnum == 0 && hden == 0)
+    {				/* single precision */
+      *hquo = *hrem = 0;
+      /* This unsigned division rounds toward zero.  */
+      *lquo = lnum / lden;
+      goto finish_up;
+    }
+
+  if (hnum == 0)
+    {				/* trivial case: dividend < divisor */
+      /* hden != 0 already checked.  */
+      *hquo = *lquo = 0;
+      *hrem = hnum;
+      *lrem = lnum;
+      goto finish_up;
+    }
+
+  memset (quo, 0, sizeof quo);
+
+  memset (num, 0, sizeof num);	/* to zero 9th element */
+  memset (den, 0, sizeof den);
+
+  encode (num, lnum, hnum);
+  encode (den, lden, hden);
+
+  /* Special code for when the divisor < BASE.  */
+  if (hden == 0 && lden < (unsigned HOST_WIDE_INT) BASE)
+    {
+      /* hnum != 0 already checked.  */
+      for (i = 4 - 1; i >= 0; i--)
+	{
+	  work = num[i] + carry * BASE;
+	  quo[i] = work / lden;
+	  carry = work % lden;
+	}
+    }
+  else
+    {
+      /* Full double precision division,
+	 with thanks to Don Knuth's "Seminumerical Algorithms".  */
+      int num_hi_sig, den_hi_sig;
+      unsigned HOST_WIDE_INT quo_est, scale;
+
+      /* Find the highest nonzero divisor digit.  */
+      for (i = 4 - 1;; i--)
+	if (den[i] != 0)
+	  {
+	    den_hi_sig = i;
+	    break;
+	  }
+
+      /* Insure that the first digit of the divisor is at least BASE/2.
+	 This is required by the quotient digit estimation algorithm.  */
+
+      scale = BASE / (den[den_hi_sig] + 1);
+      if (scale > 1)
+	{		/* scale divisor and dividend */
+	  carry = 0;
+	  for (i = 0; i <= 4 - 1; i++)
+	    {
+	      work = (num[i] * scale) + carry;
+	      num[i] = LOWPART (work);
+	      carry = HIGHPART (work);
+	    }
+
+	  num[4] = carry;
+	  carry = 0;
+	  for (i = 0; i <= 4 - 1; i++)
+	    {
+	      work = (den[i] * scale) + carry;
+	      den[i] = LOWPART (work);
+	      carry = HIGHPART (work);
+	      if (den[i] != 0) den_hi_sig = i;
+	    }
+	}
+
+      num_hi_sig = 4;
+
+      /* Main loop */
+      for (i = num_hi_sig - den_hi_sig - 1; i >= 0; i--)
+	{
+	  /* Guess the next quotient digit, quo_est, by dividing the first
+	     two remaining dividend digits by the high order quotient digit.
+	     quo_est is never low and is at most 2 high.  */
+	  unsigned HOST_WIDE_INT tmp;
+
+	  num_hi_sig = i + den_hi_sig + 1;
+	  work = num[num_hi_sig] * BASE + num[num_hi_sig - 1];
+	  if (num[num_hi_sig] != den[den_hi_sig])
+	    quo_est = work / den[den_hi_sig];
+	  else
+	    quo_est = BASE - 1;
+
+	  /* Refine quo_est so it's usually correct, and at most one high.  */
+	  tmp = work - quo_est * den[den_hi_sig];
+	  if (tmp < BASE
+	      && (den[den_hi_sig - 1] * quo_est
+		  > (tmp * BASE + num[num_hi_sig - 2])))
+	    quo_est--;
+
+	  /* Try QUO_EST as the quotient digit, by multiplying the
+	     divisor by QUO_EST and subtracting from the remaining dividend.
+	     Keep in mind that QUO_EST is the I - 1st digit.  */
+
+	  carry = 0;
+	  for (j = 0; j <= den_hi_sig; j++)
+	    {
+	      work = quo_est * den[j] + carry;
+	      carry = HIGHPART (work);
+	      work = num[i + j] - LOWPART (work);
+	      num[i + j] = LOWPART (work);
+	      carry += HIGHPART (work) != 0;
+	    }
+
+	  /* If quo_est was high by one, then num[i] went negative and
+	     we need to correct things.  */
+	  if (num[num_hi_sig] < (HOST_WIDE_INT) carry)
+	    {
+	      quo_est--;
+	      carry = 0;		/* add divisor back in */
+	      for (j = 0; j <= den_hi_sig; j++)
+		{
+		  work = num[i + j] + den[j] + carry;
+		  carry = HIGHPART (work);
+		  num[i + j] = LOWPART (work);
+		}
+
+	      num [num_hi_sig] += carry;
+	    }
+
+	  /* Store the quotient digit.  */
+	  quo[i] = quo_est;
+	}
+    }
+
+  decode (quo, lquo, hquo);
+
+ finish_up:
+  /* If result is negative, make it so.  */
+  if (quo_neg)
+    neg_double (*lquo, *hquo, lquo, hquo);
+
+  /* Compute trial remainder:  rem = num - (quo * den)  */
+  mul_double (*lquo, *hquo, lden_orig, hden_orig, lrem, hrem);
+  neg_double (*lrem, *hrem, lrem, hrem);
+  add_double (lnum_orig, hnum_orig, *lrem, *hrem, lrem, hrem);
+
+  switch (code)
+    {
+    case TRUNC_DIV_EXPR:
+    case TRUNC_MOD_EXPR:	/* round toward zero */
+    case EXACT_DIV_EXPR:	/* for this one, it shouldn't matter */
+      return overflow;
+
+    case FLOOR_DIV_EXPR:
+    case FLOOR_MOD_EXPR:	/* round toward negative infinity */
+      if (quo_neg && (*lrem != 0 || *hrem != 0))   /* ratio < 0 && rem != 0 */
+	{
+	  /* quo = quo - 1;  */
+	  add_double (*lquo, *hquo, (HOST_WIDE_INT) -1, (HOST_WIDE_INT)  -1,
+		      lquo, hquo);
+	}
+      else
+	return overflow;
+      break;
+
+    case CEIL_DIV_EXPR:
+    case CEIL_MOD_EXPR:		/* round toward positive infinity */
+      if (!quo_neg && (*lrem != 0 || *hrem != 0))  /* ratio > 0 && rem != 0 */
+	{
+	  add_double (*lquo, *hquo, (HOST_WIDE_INT) 1, (HOST_WIDE_INT) 0,
+		      lquo, hquo);
+	}
+      else
+	return overflow;
+      break;
+
+    case ROUND_DIV_EXPR:
+    case ROUND_MOD_EXPR:	/* round to closest integer */
+      {
+	unsigned HOST_WIDE_INT labs_rem = *lrem;
+	HOST_WIDE_INT habs_rem = *hrem;
+	unsigned HOST_WIDE_INT labs_den = lden, ltwice;
+	HOST_WIDE_INT habs_den = hden, htwice;
+
+	/* Get absolute values.  */
+	if (*hrem < 0)
+	  neg_double (*lrem, *hrem, &labs_rem, &habs_rem);
+	if (hden < 0)
+	  neg_double (lden, hden, &labs_den, &habs_den);
+
+	/* If (2 * abs (lrem) >= abs (lden)) */
+	mul_double ((HOST_WIDE_INT) 2, (HOST_WIDE_INT) 0,
+		    labs_rem, habs_rem, &ltwice, &htwice);
+
+	if (((unsigned HOST_WIDE_INT) habs_den
+	     < (unsigned HOST_WIDE_INT) htwice)
+	    || (((unsigned HOST_WIDE_INT) habs_den
+		 == (unsigned HOST_WIDE_INT) htwice)
+		&& (labs_den < ltwice)))
+	  {
+	    if (*hquo < 0)
+	      /* quo = quo - 1;  */
+	      add_double (*lquo, *hquo,
+			  (HOST_WIDE_INT) -1, (HOST_WIDE_INT) -1, lquo, hquo);
+	    else
+	      /* quo = quo + 1; */
+	      add_double (*lquo, *hquo, (HOST_WIDE_INT) 1, (HOST_WIDE_INT) 0,
+			  lquo, hquo);
+	  }
+	else
+	  return overflow;
+      }
+      break;
+
+    default:
+      gcc_unreachable ();
+    }
+
+  /* Compute true remainder:  rem = num - (quo * den)  */
+  mul_double (*lquo, *hquo, lden_orig, hden_orig, lrem, hrem);
+  neg_double (*lrem, *hrem, lrem, hrem);
+  add_double (lnum_orig, hnum_orig, *lrem, *hrem, lrem, hrem);
+  return overflow;
+}
+
+/* If ARG2 divides ARG1 with zero remainder, carries out the division
+   of type CODE and returns the quotient.
+   Otherwise returns NULL_TREE.  */
+
+static tree
+div_if_zero_remainder (enum tree_code code, tree arg1, tree arg2)
+{
+  unsigned HOST_WIDE_INT int1l, int2l;
+  HOST_WIDE_INT int1h, int2h;
+  unsigned HOST_WIDE_INT quol, reml;
+  HOST_WIDE_INT quoh, remh;
+  tree type = TREE_TYPE (arg1);
+  int uns = TYPE_UNSIGNED (type);
+
+  int1l = TREE_INT_CST_LOW (arg1);
+  int1h = TREE_INT_CST_HIGH (arg1);
+  int2l = TREE_INT_CST_LOW (arg2);
+  int2h = TREE_INT_CST_HIGH (arg2);
+
+  div_and_round_double (code, uns, int1l, int1h, int2l, int2h,
+		  	&quol, &quoh, &reml, &remh);
+  if (remh != 0 || reml != 0)
+    return NULL_TREE;
+
+  return build_int_cst_wide (type, quol, quoh);
+}
+
+/* Return true if the built-in mathematical function specified by CODE
+   is odd, i.e. -f(x) == f(-x).  */
+
+static bool
+negate_mathfn_p (enum built_in_function code)
+{
+  switch (code)
+    {
+    case BUILT_IN_ASIN:
+    case BUILT_IN_ASINF:
+    case BUILT_IN_ASINL:
+    case BUILT_IN_ATAN:
+    case BUILT_IN_ATANF:
+    case BUILT_IN_ATANL:
+    case BUILT_IN_SIN:
+    case BUILT_IN_SINF:
+    case BUILT_IN_SINL:
+    case BUILT_IN_TAN:
+    case BUILT_IN_TANF:
+    case BUILT_IN_TANL:
+      return true;
+
+    default:
+      break;
+    }
+  return false;
+}
+
+/* Check whether we may negate an integer constant T without causing
+   overflow.  */
+
+bool
+may_negate_without_overflow_p (tree t)
+{
+  unsigned HOST_WIDE_INT val;
+  unsigned int prec;
+  tree type;
+
+  gcc_assert (TREE_CODE (t) == INTEGER_CST);
+
+  type = TREE_TYPE (t);
+  if (TYPE_UNSIGNED (type))
+    return false;
+
+  prec = TYPE_PRECISION (type);
+  if (prec > HOST_BITS_PER_WIDE_INT)
+    {
+      if (TREE_INT_CST_LOW (t) != 0)
+	return true;
+      prec -= HOST_BITS_PER_WIDE_INT;
+      val = TREE_INT_CST_HIGH (t);
+    }
+  else
+    val = TREE_INT_CST_LOW (t);
+  if (prec < HOST_BITS_PER_WIDE_INT)
+    val &= ((unsigned HOST_WIDE_INT) 1 << prec) - 1;
+  return val != ((unsigned HOST_WIDE_INT) 1 << (prec - 1));
+}
+
+/* Determine whether an expression T can be cheaply negated using
+   the function negate_expr.  */
+
+static bool
+negate_expr_p (tree t)
+{
+  tree type;
+
+  if (t == 0)
+    return false;
+
+  type = TREE_TYPE (t);
+
+  STRIP_SIGN_NOPS (t);
+  switch (TREE_CODE (t))
+    {
+    case INTEGER_CST:
+      if (TYPE_UNSIGNED (type) || ! flag_trapv)
+	return true;
+
+      /* Check that -CST will not overflow type.  */
+      return may_negate_without_overflow_p (t);
+
+    case REAL_CST:
+    case NEGATE_EXPR:
+      return true;
+
+    case COMPLEX_CST:
+      return negate_expr_p (TREE_REALPART (t))
+	     && negate_expr_p (TREE_IMAGPART (t));
+
+    case PLUS_EXPR:
+      if (FLOAT_TYPE_P (type) && !flag_unsafe_math_optimizations)
+	return false;
+      /* -(A + B) -> (-B) - A.  */
+      if (negate_expr_p (TREE_OPERAND (t, 1))
+	  && reorder_operands_p (TREE_OPERAND (t, 0),
+				 TREE_OPERAND (t, 1)))
+	return true;
+      /* -(A + B) -> (-A) - B.  */
+      return negate_expr_p (TREE_OPERAND (t, 0));
+
+    case MINUS_EXPR:
+      /* We can't turn -(A-B) into B-A when we honor signed zeros.  */
+      return (! FLOAT_TYPE_P (type) || flag_unsafe_math_optimizations)
+	     && reorder_operands_p (TREE_OPERAND (t, 0),
+				    TREE_OPERAND (t, 1));
+
+    case MULT_EXPR:
+      if (TYPE_UNSIGNED (TREE_TYPE (t)))
+        break;
+
+      /* Fall through.  */
+
+    case RDIV_EXPR:
+      if (! HONOR_SIGN_DEPENDENT_ROUNDING (TYPE_MODE (TREE_TYPE (t))))
+	return negate_expr_p (TREE_OPERAND (t, 1))
+	       || negate_expr_p (TREE_OPERAND (t, 0));
+      break;
+
+    case NOP_EXPR:
+      /* Negate -((double)float) as (double)(-float).  */
+      if (TREE_CODE (type) == REAL_TYPE)
+	{
+	  tree tem = strip_float_extensions (t);
+	  if (tem != t)
+	    return negate_expr_p (tem);
+	}
+      break;
+
+    case CALL_EXPR:
+      /* Negate -f(x) as f(-x).  */
+      if (negate_mathfn_p (builtin_mathfn_code (t)))
+	return negate_expr_p (TREE_VALUE (TREE_OPERAND (t, 1)));
+      break;
+
+    case RSHIFT_EXPR:
+      /* Optimize -((int)x >> 31) into (unsigned)x >> 31.  */
+      if (TREE_CODE (TREE_OPERAND (t, 1)) == INTEGER_CST)
+	{
+	  tree op1 = TREE_OPERAND (t, 1);
+	  if (TREE_INT_CST_HIGH (op1) == 0
+	      && (unsigned HOST_WIDE_INT) (TYPE_PRECISION (type) - 1)
+		 == TREE_INT_CST_LOW (op1))
+	    return true;
+	}
+      break;
+
+    default:
+      break;
+    }
+  return false;
+}
+
+/* Given T, an expression, return the negation of T.  Allow for T to be
+   null, in which case return null.  */
+
+static tree
+negate_expr (tree t)
+{
+  tree type;
+  tree tem;
+
+  if (t == 0)
+    return 0;
+
+  type = TREE_TYPE (t);
+  STRIP_SIGN_NOPS (t);
+
+  switch (TREE_CODE (t))
+    {
+    case INTEGER_CST:
+      tem = fold_negate_const (t, type);
+      if (! TREE_OVERFLOW (tem)
+	  || TYPE_UNSIGNED (type)
+	  || ! flag_trapv)
+	return tem;
+      break;
+
+    case REAL_CST:
+      tem = fold_negate_const (t, type);
+      /* Two's complement FP formats, such as c4x, may overflow.  */
+      if (! TREE_OVERFLOW (tem) || ! flag_trapping_math)
+	return fold_convert (type, tem);
+      break;
+
+    case COMPLEX_CST:
+      {
+	tree rpart = negate_expr (TREE_REALPART (t));
+	tree ipart = negate_expr (TREE_IMAGPART (t));
+
+	if ((TREE_CODE (rpart) == REAL_CST
+	     && TREE_CODE (ipart) == REAL_CST)
+	    || (TREE_CODE (rpart) == INTEGER_CST
+		&& TREE_CODE (ipart) == INTEGER_CST))
+	  return build_complex (type, rpart, ipart);
+      }
+      break;
+
+    case NEGATE_EXPR:
+      return fold_convert (type, TREE_OPERAND (t, 0));
+
+    case PLUS_EXPR:
+      if (! FLOAT_TYPE_P (type) || flag_unsafe_math_optimizations)
+	{
+	  /* -(A + B) -> (-B) - A.  */
+	  if (negate_expr_p (TREE_OPERAND (t, 1))
+	      && reorder_operands_p (TREE_OPERAND (t, 0),
+				     TREE_OPERAND (t, 1)))
+	    {
+	      tem = negate_expr (TREE_OPERAND (t, 1));
+	      tem = fold_build2 (MINUS_EXPR, TREE_TYPE (t),
+				 tem, TREE_OPERAND (t, 0));
+	      return fold_convert (type, tem);
+	    }
+
+	  /* -(A + B) -> (-A) - B.  */
+	  if (negate_expr_p (TREE_OPERAND (t, 0)))
+	    {
+	      tem = negate_expr (TREE_OPERAND (t, 0));
+	      tem = fold_build2 (MINUS_EXPR, TREE_TYPE (t),
+				 tem, TREE_OPERAND (t, 1));
+	      return fold_convert (type, tem);
+	    }
+	}
+      break;
+
+    case MINUS_EXPR:
+      /* - (A - B) -> B - A  */
+      if ((! FLOAT_TYPE_P (type) || flag_unsafe_math_optimizations)
+	  && reorder_operands_p (TREE_OPERAND (t, 0), TREE_OPERAND (t, 1)))
+	return fold_convert (type,
+			     fold_build2 (MINUS_EXPR, TREE_TYPE (t),
+					  TREE_OPERAND (t, 1),
+					  TREE_OPERAND (t, 0)));
+      break;
+
+    case MULT_EXPR:
+      if (TYPE_UNSIGNED (TREE_TYPE (t)))
+        break;
+
+      /* Fall through.  */
+
+    case RDIV_EXPR:
+      if (! HONOR_SIGN_DEPENDENT_ROUNDING (TYPE_MODE (TREE_TYPE (t))))
+	{
+	  tem = TREE_OPERAND (t, 1);
+	  if (negate_expr_p (tem))
+	    return fold_convert (type,
+				 fold_build2 (TREE_CODE (t), TREE_TYPE (t),
+					      TREE_OPERAND (t, 0),
+					      negate_expr (tem)));
+	  tem = TREE_OPERAND (t, 0);
+	  if (negate_expr_p (tem))
+	    return fold_convert (type,
+				 fold_build2 (TREE_CODE (t), TREE_TYPE (t),
+					      negate_expr (tem),
+					      TREE_OPERAND (t, 1)));
+	}
+      break;
+
+    case NOP_EXPR:
+      /* Convert -((double)float) into (double)(-float).  */
+      if (TREE_CODE (type) == REAL_TYPE)
+	{
+	  tem = strip_float_extensions (t);
+	  if (tem != t && negate_expr_p (tem))
+	    return fold_convert (type, negate_expr (tem));
+	}
+      break;
+
+    case CALL_EXPR:
+      /* Negate -f(x) as f(-x).  */
+      if (negate_mathfn_p (builtin_mathfn_code (t))
+	  && negate_expr_p (TREE_VALUE (TREE_OPERAND (t, 1))))
+	{
+	  tree fndecl, arg, arglist;
+
+	  fndecl = get_callee_fndecl (t);
+	  arg = negate_expr (TREE_VALUE (TREE_OPERAND (t, 1)));
+	  arglist = build_tree_list (NULL_TREE, arg);
+	  return build_function_call_expr (fndecl, arglist);
+	}
+      break;
+
+    case RSHIFT_EXPR:
+      /* Optimize -((int)x >> 31) into (unsigned)x >> 31.  */
+      if (TREE_CODE (TREE_OPERAND (t, 1)) == INTEGER_CST)
+	{
+	  tree op1 = TREE_OPERAND (t, 1);
+	  if (TREE_INT_CST_HIGH (op1) == 0
+	      && (unsigned HOST_WIDE_INT) (TYPE_PRECISION (type) - 1)
+		 == TREE_INT_CST_LOW (op1))
+	    {
+	      tree ntype = TYPE_UNSIGNED (type)
+			   ? lang_hooks.types.signed_type (type)
+			   : lang_hooks.types.unsigned_type (type);
+	      tree temp = fold_convert (ntype, TREE_OPERAND (t, 0));
+	      temp = fold_build2 (RSHIFT_EXPR, ntype, temp, op1);
+	      return fold_convert (type, temp);
+	    }
+	}
+      break;
+
+    default:
+      break;
+    }
+
+  tem = fold_build1 (NEGATE_EXPR, TREE_TYPE (t), t);
+  return fold_convert (type, tem);
+}
+
+/* Split a tree IN into a constant, literal and variable parts that could be
+   combined with CODE to make IN.  "constant" means an expression with
+   TREE_CONSTANT but that isn't an actual constant.  CODE must be a
+   commutative arithmetic operation.  Store the constant part into *CONP,
+   the literal in *LITP and return the variable part.  If a part isn't
+   present, set it to null.  If the tree does not decompose in this way,
+   return the entire tree as the variable part and the other parts as null.
+
+   If CODE is PLUS_EXPR we also split trees that use MINUS_EXPR.  In that
+   case, we negate an operand that was subtracted.  Except if it is a
+   literal for which we use *MINUS_LITP instead.
+
+   If NEGATE_P is true, we are negating all of IN, again except a literal
+   for which we use *MINUS_LITP instead.
+
+   If IN is itself a literal or constant, return it as appropriate.
+
+   Note that we do not guarantee that any of the three values will be the
+   same type as IN, but they will have the same signedness and mode.  */
+
+static tree
+split_tree (tree in, enum tree_code code, tree *conp, tree *litp,
+	    tree *minus_litp, int negate_p)
+{
+  tree var = 0;
+
+  *conp = 0;
+  *litp = 0;
+  *minus_litp = 0;
+
+  /* Strip any conversions that don't change the machine mode or signedness.  */
+  STRIP_SIGN_NOPS (in);
+
+  if (TREE_CODE (in) == INTEGER_CST || TREE_CODE (in) == REAL_CST)
+    *litp = in;
+  else if (TREE_CODE (in) == code
+	   || (! FLOAT_TYPE_P (TREE_TYPE (in))
+	       /* We can associate addition and subtraction together (even
+		  though the C standard doesn't say so) for integers because
+		  the value is not affected.  For reals, the value might be
+		  affected, so we can't.  */
+	       && ((code == PLUS_EXPR && TREE_CODE (in) == MINUS_EXPR)
+		   || (code == MINUS_EXPR && TREE_CODE (in) == PLUS_EXPR))))
+    {
+      tree op0 = TREE_OPERAND (in, 0);
+      tree op1 = TREE_OPERAND (in, 1);
+      int neg1_p = TREE_CODE (in) == MINUS_EXPR;
+      int neg_litp_p = 0, neg_conp_p = 0, neg_var_p = 0;
+
+      /* First see if either of the operands is a literal, then a constant.  */
+      if (TREE_CODE (op0) == INTEGER_CST || TREE_CODE (op0) == REAL_CST)
+	*litp = op0, op0 = 0;
+      else if (TREE_CODE (op1) == INTEGER_CST || TREE_CODE (op1) == REAL_CST)
+	*litp = op1, neg_litp_p = neg1_p, op1 = 0;
+
+      if (op0 != 0 && TREE_CONSTANT (op0))
+	*conp = op0, op0 = 0;
+      else if (op1 != 0 && TREE_CONSTANT (op1))
+	*conp = op1, neg_conp_p = neg1_p, op1 = 0;
+
+      /* If we haven't dealt with either operand, this is not a case we can
+	 decompose.  Otherwise, VAR is either of the ones remaining, if any.  */
+      if (op0 != 0 && op1 != 0)
+	var = in;
+      else if (op0 != 0)
+	var = op0;
+      else
+	var = op1, neg_var_p = neg1_p;
+
+      /* Now do any needed negations.  */
+      if (neg_litp_p)
+	*minus_litp = *litp, *litp = 0;
+      if (neg_conp_p)
+	*conp = negate_expr (*conp);
+      if (neg_var_p)
+	var = negate_expr (var);
+    }
+  else if (TREE_CONSTANT (in))
+    *conp = in;
+  else
+    var = in;
+
+  if (negate_p)
+    {
+      if (*litp)
+	*minus_litp = *litp, *litp = 0;
+      else if (*minus_litp)
+	*litp = *minus_litp, *minus_litp = 0;
+      *conp = negate_expr (*conp);
+      var = negate_expr (var);
+    }
+
+  return var;
+}
+
+/* Re-associate trees split by the above function.  T1 and T2 are either
+   expressions to associate or null.  Return the new expression, if any.  If
+   we build an operation, do it in TYPE and with CODE.  */
+
+static tree
+associate_trees (tree t1, tree t2, enum tree_code code, tree type)
+{
+  if (t1 == 0)
+    return t2;
+  else if (t2 == 0)
+    return t1;
+
+  /* If either input is CODE, a PLUS_EXPR, or a MINUS_EXPR, don't
+     try to fold this since we will have infinite recursion.  But do
+     deal with any NEGATE_EXPRs.  */
+  if (TREE_CODE (t1) == code || TREE_CODE (t2) == code
+      || TREE_CODE (t1) == MINUS_EXPR || TREE_CODE (t2) == MINUS_EXPR)
+    {
+      if (code == PLUS_EXPR)
+	{
+	  if (TREE_CODE (t1) == NEGATE_EXPR)
+	    return build2 (MINUS_EXPR, type, fold_convert (type, t2),
+			   fold_convert (type, TREE_OPERAND (t1, 0)));
+	  else if (TREE_CODE (t2) == NEGATE_EXPR)
+	    return build2 (MINUS_EXPR, type, fold_convert (type, t1),
+			   fold_convert (type, TREE_OPERAND (t2, 0)));
+	  else if (integer_zerop (t2))
+	    return fold_convert (type, t1);
+	}
+      else if (code == MINUS_EXPR)
+	{
+	  if (integer_zerop (t2))
+	    return fold_convert (type, t1);
+	}
+
+      return build2 (code, type, fold_convert (type, t1),
+		     fold_convert (type, t2));
+    }
+
+  return fold_build2 (code, type, fold_convert (type, t1),
+		      fold_convert (type, t2));
+}
+
+/* Combine two integer constants ARG1 and ARG2 under operation CODE
+   to produce a new constant.
+
+   If NOTRUNC is nonzero, do not truncate the result to fit the data type.  */
+
+tree
+int_const_binop (enum tree_code code, tree arg1, tree arg2, int notrunc)
+{
+  unsigned HOST_WIDE_INT int1l, int2l;
+  HOST_WIDE_INT int1h, int2h;
+  unsigned HOST_WIDE_INT low;
+  HOST_WIDE_INT hi;
+  unsigned HOST_WIDE_INT garbagel;
+  HOST_WIDE_INT garbageh;
+  tree t;
+  tree type = TREE_TYPE (arg1);
+  int uns = TYPE_UNSIGNED (type);
+  int is_sizetype
+    = (TREE_CODE (type) == INTEGER_TYPE && TYPE_IS_SIZETYPE (type));
+  int overflow = 0;
+
+  int1l = TREE_INT_CST_LOW (arg1);
+  int1h = TREE_INT_CST_HIGH (arg1);
+  int2l = TREE_INT_CST_LOW (arg2);
+  int2h = TREE_INT_CST_HIGH (arg2);
+
+  switch (code)
+    {
+    case BIT_IOR_EXPR:
+      low = int1l | int2l, hi = int1h | int2h;
+      break;
+
+    case BIT_XOR_EXPR:
+      low = int1l ^ int2l, hi = int1h ^ int2h;
+      break;
+
+    case BIT_AND_EXPR:
+      low = int1l & int2l, hi = int1h & int2h;
+      break;
+
+    case RSHIFT_EXPR:
+      int2l = -int2l;
+    case LSHIFT_EXPR:
+      /* It's unclear from the C standard whether shifts can overflow.
+	 The following code ignores overflow; perhaps a C standard
+	 interpretation ruling is needed.  */
+      lshift_double (int1l, int1h, int2l, TYPE_PRECISION (type),
+		     &low, &hi, !uns);
+      break;
+
+    case RROTATE_EXPR:
+      int2l = - int2l;
+    case LROTATE_EXPR:
+      lrotate_double (int1l, int1h, int2l, TYPE_PRECISION (type),
+		      &low, &hi);
+      break;
+
+    case PLUS_EXPR:
+      overflow = add_double (int1l, int1h, int2l, int2h, &low, &hi);
+      break;
+
+    case MINUS_EXPR:
+      neg_double (int2l, int2h, &low, &hi);
+      add_double (int1l, int1h, low, hi, &low, &hi);
+      overflow = OVERFLOW_SUM_SIGN (hi, int2h, int1h);
+      break;
+
+    case MULT_EXPR:
+      overflow = mul_double (int1l, int1h, int2l, int2h, &low, &hi);
+      break;
+
+    case TRUNC_DIV_EXPR:
+    case FLOOR_DIV_EXPR: case CEIL_DIV_EXPR:
+    case EXACT_DIV_EXPR:
+      /* This is a shortcut for a common special case.  */
+      if (int2h == 0 && (HOST_WIDE_INT) int2l > 0
+	  && ! TREE_CONSTANT_OVERFLOW (arg1)
+	  && ! TREE_CONSTANT_OVERFLOW (arg2)
+	  && int1h == 0 && (HOST_WIDE_INT) int1l >= 0)
+	{
+	  if (code == CEIL_DIV_EXPR)
+	    int1l += int2l - 1;
+
+	  low = int1l / int2l, hi = 0;
+	  break;
+	}
+
+      /* ... fall through ...  */
+
+    case ROUND_DIV_EXPR:
+      if (int2h == 0 && int2l == 1)
+	{
+	  low = int1l, hi = int1h;
+	  break;
+	}
+      if (int1l == int2l && int1h == int2h
+	  && ! (int1l == 0 && int1h == 0))
+	{
+	  low = 1, hi = 0;
+	  break;
+	}
+      overflow = div_and_round_double (code, uns, int1l, int1h, int2l, int2h,
+				       &low, &hi, &garbagel, &garbageh);
+      break;
+
+    case TRUNC_MOD_EXPR:
+    case FLOOR_MOD_EXPR: case CEIL_MOD_EXPR:
+      /* This is a shortcut for a common special case.  */
+      if (int2h == 0 && (HOST_WIDE_INT) int2l > 0
+	  && ! TREE_CONSTANT_OVERFLOW (arg1)
+	  && ! TREE_CONSTANT_OVERFLOW (arg2)
+	  && int1h == 0 && (HOST_WIDE_INT) int1l >= 0)
+	{
+	  if (code == CEIL_MOD_EXPR)
+	    int1l += int2l - 1;
+	  low = int1l % int2l, hi = 0;
+	  break;
+	}
+
+      /* ... fall through ...  */
+
+    case ROUND_MOD_EXPR:
+      overflow = div_and_round_double (code, uns,
+				       int1l, int1h, int2l, int2h,
+				       &garbagel, &garbageh, &low, &hi);
+      break;
+
+    case MIN_EXPR:
+    case MAX_EXPR:
+      if (uns)
+	low = (((unsigned HOST_WIDE_INT) int1h
+		< (unsigned HOST_WIDE_INT) int2h)
+	       || (((unsigned HOST_WIDE_INT) int1h
+		    == (unsigned HOST_WIDE_INT) int2h)
+		   && int1l < int2l));
+      else
+	low = (int1h < int2h
+	       || (int1h == int2h && int1l < int2l));
+
+      if (low == (code == MIN_EXPR))
+	low = int1l, hi = int1h;
+      else
+	low = int2l, hi = int2h;
+      break;
+
+    default:
+      gcc_unreachable ();
+    }
+
+  t = build_int_cst_wide (TREE_TYPE (arg1), low, hi);
+
+  if (notrunc)
+    {
+      /* Propagate overflow flags ourselves.  */
+      if (((!uns || is_sizetype) && overflow)
+	  | TREE_OVERFLOW (arg1) | TREE_OVERFLOW (arg2))
+	{
+	  t = copy_node (t);
+	  TREE_OVERFLOW (t) = 1;
+	  TREE_CONSTANT_OVERFLOW (t) = 1;
+	}
+      else if (TREE_CONSTANT_OVERFLOW (arg1) | TREE_CONSTANT_OVERFLOW (arg2))
+	{
+	  t = copy_node (t);
+	  TREE_CONSTANT_OVERFLOW (t) = 1;
+	}
+    }
+  else
+    t = force_fit_type (t, 1,
+			((!uns || is_sizetype) && overflow)
+			| TREE_OVERFLOW (arg1) | TREE_OVERFLOW (arg2),
+			TREE_CONSTANT_OVERFLOW (arg1)
+			| TREE_CONSTANT_OVERFLOW (arg2));
+
+  return t;
+}
+
+/* Combine two constants ARG1 and ARG2 under operation CODE to produce a new
+   constant.  We assume ARG1 and ARG2 have the same data type, or at least
+   are the same kind of constant and the same machine mode.
+
+   If NOTRUNC is nonzero, do not truncate the result to fit the data type.  */
+
+static tree
+const_binop (enum tree_code code, tree arg1, tree arg2, int notrunc)
+{
+  STRIP_NOPS (arg1);
+  STRIP_NOPS (arg2);
+
+  if (TREE_CODE (arg1) == INTEGER_CST)
+    return int_const_binop (code, arg1, arg2, notrunc);
+
+  if (TREE_CODE (arg1) == REAL_CST)
+    {
+      enum machine_mode mode;
+      REAL_VALUE_TYPE d1;
+      REAL_VALUE_TYPE d2;
+      REAL_VALUE_TYPE value;
+      REAL_VALUE_TYPE result;
+      bool inexact;
+      tree t, type;
+
+      d1 = TREE_REAL_CST (arg1);
+      d2 = TREE_REAL_CST (arg2);
+
+      type = TREE_TYPE (arg1);
+      mode = TYPE_MODE (type);
+
+      /* Don't perform operation if we honor signaling NaNs and
+	 either operand is a NaN.  */
+      if (HONOR_SNANS (mode)
+	  && (REAL_VALUE_ISNAN (d1) || REAL_VALUE_ISNAN (d2)))
+	return NULL_TREE;
+
+      /* Don't perform operation if it would raise a division
+	 by zero exception.  */
+      if (code == RDIV_EXPR
+	  && REAL_VALUES_EQUAL (d2, dconst0)
+	  && (flag_trapping_math || ! MODE_HAS_INFINITIES (mode)))
+	return NULL_TREE;
+
+      /* If either operand is a NaN, just return it.  Otherwise, set up
+	 for floating-point trap; we return an overflow.  */
+      if (REAL_VALUE_ISNAN (d1))
+	return arg1;
+      else if (REAL_VALUE_ISNAN (d2))
+	return arg2;
+
+      inexact = real_arithmetic (&value, code, &d1, &d2);
+      real_convert (&result, mode, &value);
+
+      /* Don't constant fold this floating point operation if
+	 the result has overflowed and flag_trapping_math.  */
+
+      if (flag_trapping_math
+	  && MODE_HAS_INFINITIES (mode)
+	  && REAL_VALUE_ISINF (result)
+	  && !REAL_VALUE_ISINF (d1)
+	  && !REAL_VALUE_ISINF (d2))
+	return NULL_TREE;
+
+      /* Don't constant fold this floating point operation if the
+	 result may dependent upon the run-time rounding mode and
+	 flag_rounding_math is set, or if GCC's software emulation
+	 is unable to accurately represent the result.  */
+      
+      if ((flag_rounding_math
+	   || (REAL_MODE_FORMAT_COMPOSITE_P (mode)
+	       && !flag_unsafe_math_optimizations))
+	  && (inexact || !real_identical (&result, &value)))
+	return NULL_TREE;
+
+      t = build_real (type, result);
+
+      TREE_OVERFLOW (t) = TREE_OVERFLOW (arg1) | TREE_OVERFLOW (arg2);
+      TREE_CONSTANT_OVERFLOW (t)
+	= TREE_OVERFLOW (t)
+	  | TREE_CONSTANT_OVERFLOW (arg1)
+	  | TREE_CONSTANT_OVERFLOW (arg2);
+      return t;
+    }
+  if (TREE_CODE (arg1) == COMPLEX_CST)
+    {
+      tree type = TREE_TYPE (arg1);
+      tree r1 = TREE_REALPART (arg1);
+      tree i1 = TREE_IMAGPART (arg1);
+      tree r2 = TREE_REALPART (arg2);
+      tree i2 = TREE_IMAGPART (arg2);
+      tree t;
+
+      switch (code)
+	{
+	case PLUS_EXPR:
+	  t = build_complex (type,
+			     const_binop (PLUS_EXPR, r1, r2, notrunc),
+			     const_binop (PLUS_EXPR, i1, i2, notrunc));
+	  break;
+
+	case MINUS_EXPR:
+	  t = build_complex (type,
+			     const_binop (MINUS_EXPR, r1, r2, notrunc),
+			     const_binop (MINUS_EXPR, i1, i2, notrunc));
+	  break;
+
+	case MULT_EXPR:
+	  t = build_complex (type,
+			     const_binop (MINUS_EXPR,
+					  const_binop (MULT_EXPR,
+						       r1, r2, notrunc),
+					  const_binop (MULT_EXPR,
+						       i1, i2, notrunc),
+					  notrunc),
+			     const_binop (PLUS_EXPR,
+					  const_binop (MULT_EXPR,
+						       r1, i2, notrunc),
+					  const_binop (MULT_EXPR,
+						       i1, r2, notrunc),
+					  notrunc));
+	  break;
+
+	case RDIV_EXPR:
+	  {
+	    tree t1, t2, real, imag;
+	    tree magsquared
+	      = const_binop (PLUS_EXPR,
+			     const_binop (MULT_EXPR, r2, r2, notrunc),
+			     const_binop (MULT_EXPR, i2, i2, notrunc),
+			     notrunc);
+
+	    t1 = const_binop (PLUS_EXPR,
+			      const_binop (MULT_EXPR, r1, r2, notrunc),
+			      const_binop (MULT_EXPR, i1, i2, notrunc),
+			      notrunc);
+	    t2 = const_binop (MINUS_EXPR,
+			      const_binop (MULT_EXPR, i1, r2, notrunc),
+			      const_binop (MULT_EXPR, r1, i2, notrunc),
+			      notrunc);
+
+	    if (INTEGRAL_TYPE_P (TREE_TYPE (r1)))
+	      {
+		real = const_binop (TRUNC_DIV_EXPR, t1, magsquared, notrunc);
+		imag = const_binop (TRUNC_DIV_EXPR, t2, magsquared, notrunc);
+	      }
+	    else
+	      {
+		real = const_binop (RDIV_EXPR, t1, magsquared, notrunc);
+		imag = const_binop (RDIV_EXPR, t2, magsquared, notrunc);
+		if (!real || !imag)
+		  return NULL_TREE;
+	      }
+
+	    t = build_complex (type, real, imag);
+	  }
+	  break;
+
+	default:
+	  gcc_unreachable ();
+	}
+      return t;
+    }
+  return 0;
+}
+
+/* Create a size type INT_CST node with NUMBER sign extended.  KIND
+   indicates which particular sizetype to create.  */
+
+tree
+size_int_kind (HOST_WIDE_INT number, enum size_type_kind kind)
+{
+  return build_int_cst (sizetype_tab[(int) kind], number);
+}
+
+/* Combine operands OP1 and OP2 with arithmetic operation CODE.  CODE
+   is a tree code.  The type of the result is taken from the operands.
+   Both must be the same type integer type and it must be a size type.
+   If the operands are constant, so is the result.  */
+
+tree
+size_binop (enum tree_code code, tree arg0, tree arg1)
+{
+  tree type = TREE_TYPE (arg0);
+
+  if (arg0 == error_mark_node || arg1 == error_mark_node)
+    return error_mark_node;
+
+  gcc_assert (TREE_CODE (type) == INTEGER_TYPE && TYPE_IS_SIZETYPE (type)
+	      && type == TREE_TYPE (arg1));
+
+  /* Handle the special case of two integer constants faster.  */
+  if (TREE_CODE (arg0) == INTEGER_CST && TREE_CODE (arg1) == INTEGER_CST)
+    {
+      /* And some specific cases even faster than that.  */
+      if (code == PLUS_EXPR && integer_zerop (arg0))
+	return arg1;
+      else if ((code == MINUS_EXPR || code == PLUS_EXPR)
+	       && integer_zerop (arg1))
+	return arg0;
+      else if (code == MULT_EXPR && integer_onep (arg0))
+	return arg1;
+
+      /* Handle general case of two integer constants.  */
+      return int_const_binop (code, arg0, arg1, 0);
+    }
+
+  return fold_build2 (code, type, arg0, arg1);
+}
+
+/* Given two values, either both of sizetype or both of bitsizetype,
+   compute the difference between the two values.  Return the value
+   in signed type corresponding to the type of the operands.  */
+
+tree
+size_diffop (tree arg0, tree arg1)
+{
+  tree type = TREE_TYPE (arg0);
+  tree ctype;
+
+  gcc_assert (TREE_CODE (type) == INTEGER_TYPE && TYPE_IS_SIZETYPE (type)
+	      && type == TREE_TYPE (arg1));
+
+  /* If the type is already signed, just do the simple thing.  */
+  if (!TYPE_UNSIGNED (type))
+    return size_binop (MINUS_EXPR, arg0, arg1);
+
+  ctype = type == bitsizetype ? sbitsizetype : ssizetype;
+
+  /* If either operand is not a constant, do the conversions to the signed
+     type and subtract.  The hardware will do the right thing with any
+     overflow in the subtraction.  */
+  if (TREE_CODE (arg0) != INTEGER_CST || TREE_CODE (arg1) != INTEGER_CST)
+    return size_binop (MINUS_EXPR, fold_convert (ctype, arg0),
+		       fold_convert (ctype, arg1));
+
+  /* If ARG0 is larger than ARG1, subtract and return the result in CTYPE.
+     Otherwise, subtract the other way, convert to CTYPE (we know that can't
+     overflow) and negate (which can't either).  Special-case a result
+     of zero while we're here.  */
+  if (tree_int_cst_equal (arg0, arg1))
+    return fold_convert (ctype, integer_zero_node);
+  else if (tree_int_cst_lt (arg1, arg0))
+    return fold_convert (ctype, size_binop (MINUS_EXPR, arg0, arg1));
+  else
+    return size_binop (MINUS_EXPR, fold_convert (ctype, integer_zero_node),
+		       fold_convert (ctype, size_binop (MINUS_EXPR,
+							arg1, arg0)));
+}
+
+/* A subroutine of fold_convert_const handling conversions of an
+   INTEGER_CST to another integer type.  */
+
+static tree
+fold_convert_const_int_from_int (tree type, tree arg1)
+{
+  tree t;
+
+  /* Given an integer constant, make new constant with new type,
+     appropriately sign-extended or truncated.  */
+  t = build_int_cst_wide (type, TREE_INT_CST_LOW (arg1),
+			  TREE_INT_CST_HIGH (arg1));
+
+  t = force_fit_type (t,
+		      /* Don't set the overflow when
+		      	 converting a pointer  */
+		      !POINTER_TYPE_P (TREE_TYPE (arg1)),
+		      (TREE_INT_CST_HIGH (arg1) < 0
+		       && (TYPE_UNSIGNED (type)
+			   < TYPE_UNSIGNED (TREE_TYPE (arg1))))
+		      | TREE_OVERFLOW (arg1),
+		      TREE_CONSTANT_OVERFLOW (arg1));
+
+  return t;
+}
+
+/* A subroutine of fold_convert_const handling conversions a REAL_CST
+   to an integer type.  */
+
+static tree
+fold_convert_const_int_from_real (enum tree_code code, tree type, tree arg1)
+{
+  int overflow = 0;
+  tree t;
+
+  /* The following code implements the floating point to integer
+     conversion rules required by the Java Language Specification,
+     that IEEE NaNs are mapped to zero and values that overflow
+     the target precision saturate, i.e. values greater than
+     INT_MAX are mapped to INT_MAX, and values less than INT_MIN
+     are mapped to INT_MIN.  These semantics are allowed by the
+     C and C++ standards that simply state that the behavior of
+     FP-to-integer conversion is unspecified upon overflow.  */
+
+  HOST_WIDE_INT high, low;
+  REAL_VALUE_TYPE r;
+  REAL_VALUE_TYPE x = TREE_REAL_CST (arg1);
+
+  switch (code)
+    {
+    case FIX_TRUNC_EXPR:
+      real_trunc (&r, VOIDmode, &x);
+      break;
+
+    case FIX_CEIL_EXPR:
+      real_ceil (&r, VOIDmode, &x);
+      break;
+
+    case FIX_FLOOR_EXPR:
+      real_floor (&r, VOIDmode, &x);
+      break;
+
+    case FIX_ROUND_EXPR:
+      real_round (&r, VOIDmode, &x);
+      break;
+
+    default:
+      gcc_unreachable ();
+    }
+
+  /* If R is NaN, return zero and show we have an overflow.  */
+  if (REAL_VALUE_ISNAN (r))
+    {
+      overflow = 1;
+      high = 0;
+      low = 0;
+    }
+
+  /* See if R is less than the lower bound or greater than the
+     upper bound.  */
+
+  if (! overflow)
+    {
+      tree lt = TYPE_MIN_VALUE (type);
+      REAL_VALUE_TYPE l = real_value_from_int_cst (NULL_TREE, lt);
+      if (REAL_VALUES_LESS (r, l))
+	{
+	  overflow = 1;
+	  high = TREE_INT_CST_HIGH (lt);
+	  low = TREE_INT_CST_LOW (lt);
+	}
+    }
+
+  if (! overflow)
+    {
+      tree ut = TYPE_MAX_VALUE (type);
+      if (ut)
+	{
+	  REAL_VALUE_TYPE u = real_value_from_int_cst (NULL_TREE, ut);
+	  if (REAL_VALUES_LESS (u, r))
+	    {
+	      overflow = 1;
+	      high = TREE_INT_CST_HIGH (ut);
+	      low = TREE_INT_CST_LOW (ut);
+	    }
+	}
+    }
+
+  if (! overflow)
+    REAL_VALUE_TO_INT (&low, &high, r);
+
+  t = build_int_cst_wide (type, low, high);
+
+  t = force_fit_type (t, -1, overflow | TREE_OVERFLOW (arg1),
+		      TREE_CONSTANT_OVERFLOW (arg1));
+  return t;
+}
+
+/* A subroutine of fold_convert_const handling conversions a REAL_CST
+   to another floating point type.  */
+
+static tree
+fold_convert_const_real_from_real (tree type, tree arg1)
+{
+  REAL_VALUE_TYPE value;
+  tree t;
+
+  real_convert (&value, TYPE_MODE (type), &TREE_REAL_CST (arg1));
+  t = build_real (type, value);
+
+  TREE_OVERFLOW (t) = TREE_OVERFLOW (arg1);
+  TREE_CONSTANT_OVERFLOW (t)
+    = TREE_OVERFLOW (t) | TREE_CONSTANT_OVERFLOW (arg1);
+  return t;
+}
+
+/* Attempt to fold type conversion operation CODE of expression ARG1 to
+   type TYPE.  If no simplification can be done return NULL_TREE.  */
+
+static tree
+fold_convert_const (enum tree_code code, tree type, tree arg1)
+{
+  if (TREE_TYPE (arg1) == type)
+    return arg1;
+
+  if (POINTER_TYPE_P (type) || INTEGRAL_TYPE_P (type))
+    {
+      if (TREE_CODE (arg1) == INTEGER_CST)
+	return fold_convert_const_int_from_int (type, arg1);
+      else if (TREE_CODE (arg1) == REAL_CST)
+	return fold_convert_const_int_from_real (code, type, arg1);
+    }
+  else if (TREE_CODE (type) == REAL_TYPE)
+    {
+      if (TREE_CODE (arg1) == INTEGER_CST)
+	return build_real_from_int_cst (type, arg1);
+      if (TREE_CODE (arg1) == REAL_CST)
+	return fold_convert_const_real_from_real (type, arg1);
+    }
+  return NULL_TREE;
+}
+
+/* Construct a vector of zero elements of vector type TYPE.  */
+
+static tree
+build_zero_vector (tree type)
+{
+  tree elem, list;
+  int i, units;
+
+  elem = fold_convert_const (NOP_EXPR, TREE_TYPE (type), integer_zero_node);
+  units = TYPE_VECTOR_SUBPARTS (type);
+  
+  list = NULL_TREE;
+  for (i = 0; i < units; i++)
+    list = tree_cons (NULL_TREE, elem, list);
+  return build_vector (type, list);
+}
+
+/* Convert expression ARG to type TYPE.  Used by the middle-end for
+   simple conversions in preference to calling the front-end's convert.  */
+
+tree
+fold_convert (tree type, tree arg)
+{
+  tree orig = TREE_TYPE (arg);
+  tree tem;
+
+  if (type == orig)
+    return arg;
+
+  if (TREE_CODE (arg) == ERROR_MARK
+      || TREE_CODE (type) == ERROR_MARK
+      || TREE_CODE (orig) == ERROR_MARK)
+    return error_mark_node;
+
+  if (TYPE_MAIN_VARIANT (type) == TYPE_MAIN_VARIANT (orig)
+      || lang_hooks.types_compatible_p (TYPE_MAIN_VARIANT (type),
+					TYPE_MAIN_VARIANT (orig)))
+    return fold_build1 (NOP_EXPR, type, arg);
+
+  switch (TREE_CODE (type))
+    {
+    case INTEGER_TYPE: case CHAR_TYPE: case ENUMERAL_TYPE: case BOOLEAN_TYPE:
+    case POINTER_TYPE: case REFERENCE_TYPE:
+    case OFFSET_TYPE:
+      if (TREE_CODE (arg) == INTEGER_CST)
+	{
+	  tem = fold_convert_const (NOP_EXPR, type, arg);
+	  if (tem != NULL_TREE)
+	    return tem;
+	}
+      if (INTEGRAL_TYPE_P (orig) || POINTER_TYPE_P (orig)
+	  || TREE_CODE (orig) == OFFSET_TYPE)
+        return fold_build1 (NOP_EXPR, type, arg);
+      if (TREE_CODE (orig) == COMPLEX_TYPE)
+	{
+	  tem = fold_build1 (REALPART_EXPR, TREE_TYPE (orig), arg);
+	  return fold_convert (type, tem);
+	}
+      gcc_assert (TREE_CODE (orig) == VECTOR_TYPE
+		  && tree_int_cst_equal (TYPE_SIZE (type), TYPE_SIZE (orig)));
+      return fold_build1 (NOP_EXPR, type, arg);
+
+    case REAL_TYPE:
+      if (TREE_CODE (arg) == INTEGER_CST)
+	{
+	  tem = fold_convert_const (FLOAT_EXPR, type, arg);
+	  if (tem != NULL_TREE)
+	    return tem;
+	}
+      else if (TREE_CODE (arg) == REAL_CST)
+	{
+	  tem = fold_convert_const (NOP_EXPR, type, arg);
+	  if (tem != NULL_TREE)
+	    return tem;
+	}
+
+      switch (TREE_CODE (orig))
+	{
+	case INTEGER_TYPE: case CHAR_TYPE:
+	case BOOLEAN_TYPE: case ENUMERAL_TYPE:
+	case POINTER_TYPE: case REFERENCE_TYPE:
+	  return fold_build1 (FLOAT_EXPR, type, arg);
+
+	case REAL_TYPE:
+	  return fold_build1 (flag_float_store ? CONVERT_EXPR : NOP_EXPR,
+			      type, arg);
+
+	case COMPLEX_TYPE:
+	  tem = fold_build1 (REALPART_EXPR, TREE_TYPE (orig), arg);
+	  return fold_convert (type, tem);
+
+	default:
+	  gcc_unreachable ();
+	}
+
+    case COMPLEX_TYPE:
+      switch (TREE_CODE (orig))
+	{
+	case INTEGER_TYPE: case CHAR_TYPE:
+	case BOOLEAN_TYPE: case ENUMERAL_TYPE:
+	case POINTER_TYPE: case REFERENCE_TYPE:
+	case REAL_TYPE:
+	  return build2 (COMPLEX_EXPR, type,
+			 fold_convert (TREE_TYPE (type), arg),
+			 fold_convert (TREE_TYPE (type), integer_zero_node));
+	case COMPLEX_TYPE:
+	  {
+	    tree rpart, ipart;
+
+	    if (TREE_CODE (arg) == COMPLEX_EXPR)
+	      {
+		rpart = fold_convert (TREE_TYPE (type), TREE_OPERAND (arg, 0));
+		ipart = fold_convert (TREE_TYPE (type), TREE_OPERAND (arg, 1));
+		return fold_build2 (COMPLEX_EXPR, type, rpart, ipart);
+	      }
+
+	    arg = save_expr (arg);
+	    rpart = fold_build1 (REALPART_EXPR, TREE_TYPE (orig), arg);
+	    ipart = fold_build1 (IMAGPART_EXPR, TREE_TYPE (orig), arg);
+	    rpart = fold_convert (TREE_TYPE (type), rpart);
+	    ipart = fold_convert (TREE_TYPE (type), ipart);
+	    return fold_build2 (COMPLEX_EXPR, type, rpart, ipart);
+	  }
+
+	default:
+	  gcc_unreachable ();
+	}
+
+    case VECTOR_TYPE:
+      if (integer_zerop (arg))
+	return build_zero_vector (type);
+      gcc_assert (tree_int_cst_equal (TYPE_SIZE (type), TYPE_SIZE (orig)));
+      gcc_assert (INTEGRAL_TYPE_P (orig) || POINTER_TYPE_P (orig)
+		  || TREE_CODE (orig) == VECTOR_TYPE);
+      return fold_build1 (VIEW_CONVERT_EXPR, type, arg);
+
+    case VOID_TYPE:
+      return fold_build1 (CONVERT_EXPR, type, fold_ignored_result (arg));
+
+    default:
+      gcc_unreachable ();
+    }
+}
+
+/* Return false if expr can be assumed not to be an lvalue, true
+   otherwise.  */
+
+static bool
+maybe_lvalue_p (tree x)
+{
+  /* We only need to wrap lvalue tree codes.  */
+  switch (TREE_CODE (x))
+  {
+  case VAR_DECL:
+  case PARM_DECL:
+  case RESULT_DECL:
+  case LABEL_DECL:
+  case FUNCTION_DECL:
+  case SSA_NAME:
+
+  case COMPONENT_REF:
+  case INDIRECT_REF:
+  case ALIGN_INDIRECT_REF:
+  case MISALIGNED_INDIRECT_REF:
+  case ARRAY_REF:
+  case ARRAY_RANGE_REF:
+  case BIT_FIELD_REF:
+  case OBJ_TYPE_REF:
+
+  case REALPART_EXPR:
+  case IMAGPART_EXPR:
+  case PREINCREMENT_EXPR:
+  case PREDECREMENT_EXPR:
+  case SAVE_EXPR:
+  case TRY_CATCH_EXPR:
+  case WITH_CLEANUP_EXPR:
+  case COMPOUND_EXPR:
+  case MODIFY_EXPR:
+  case TARGET_EXPR:
+  case COND_EXPR:
+  case BIND_EXPR:
+  case MIN_EXPR:
+  case MAX_EXPR:
+    break;
+
+  default:
+    /* Assume the worst for front-end tree codes.  */
+    if ((int)TREE_CODE (x) >= NUM_TREE_CODES)
+      break;
+    return false;
+  }
+
+  return true;
+}
+
+/* Return an expr equal to X but certainly not valid as an lvalue.  */
+
+tree
+non_lvalue (tree x)
+{
+  /* While we are in GIMPLE, NON_LVALUE_EXPR doesn't mean anything to
+     us.  */
+  if (in_gimple_form)
+    return x;
+
+  if (! maybe_lvalue_p (x))
+    return x;
+  return build1 (NON_LVALUE_EXPR, TREE_TYPE (x), x);
+}
+
+/* Nonzero means lvalues are limited to those valid in pedantic ANSI C.
+   Zero means allow extended lvalues.  */
+
+int pedantic_lvalues;
+
+/* When pedantic, return an expr equal to X but certainly not valid as a
+   pedantic lvalue.  Otherwise, return X.  */
+
+static tree
+pedantic_non_lvalue (tree x)
+{
+  if (pedantic_lvalues)
+    return non_lvalue (x);
+  else
+    return x;
+}
+
+/* Given a tree comparison code, return the code that is the logical inverse
+   of the given code.  It is not safe to do this for floating-point
+   comparisons, except for NE_EXPR and EQ_EXPR, so we receive a machine mode
+   as well: if reversing the comparison is unsafe, return ERROR_MARK.  */
+
+enum tree_code
+invert_tree_comparison (enum tree_code code, bool honor_nans)
+{
+  if (honor_nans && flag_trapping_math)
+    return ERROR_MARK;
+
+  switch (code)
+    {
+    case EQ_EXPR:
+      return NE_EXPR;
+    case NE_EXPR:
+      return EQ_EXPR;
+    case GT_EXPR:
+      return honor_nans ? UNLE_EXPR : LE_EXPR;
+    case GE_EXPR:
+      return honor_nans ? UNLT_EXPR : LT_EXPR;
+    case LT_EXPR:
+      return honor_nans ? UNGE_EXPR : GE_EXPR;
+    case LE_EXPR:
+      return honor_nans ? UNGT_EXPR : GT_EXPR;
+    case LTGT_EXPR:
+      return UNEQ_EXPR;
+    case UNEQ_EXPR:
+      return LTGT_EXPR;
+    case UNGT_EXPR:
+      return LE_EXPR;
+    case UNGE_EXPR:
+      return LT_EXPR;
+    case UNLT_EXPR:
+      return GE_EXPR;
+    case UNLE_EXPR:
+      return GT_EXPR;
+    case ORDERED_EXPR:
+      return UNORDERED_EXPR;
+    case UNORDERED_EXPR:
+      return ORDERED_EXPR;
+    default:
+      gcc_unreachable ();
+    }
+}
+
+/* Similar, but return the comparison that results if the operands are
+   swapped.  This is safe for floating-point.  */
+
+enum tree_code
+swap_tree_comparison (enum tree_code code)
+{
+  switch (code)
+    {
+    case EQ_EXPR:
+    case NE_EXPR:
+    case ORDERED_EXPR:
+    case UNORDERED_EXPR:
+    case LTGT_EXPR:
+    case UNEQ_EXPR:
+      return code;
+    case GT_EXPR:
+      return LT_EXPR;
+    case GE_EXPR:
+      return LE_EXPR;
+    case LT_EXPR:
+      return GT_EXPR;
+    case LE_EXPR:
+      return GE_EXPR;
+    case UNGT_EXPR:
+      return UNLT_EXPR;
+    case UNGE_EXPR:
+      return UNLE_EXPR;
+    case UNLT_EXPR:
+      return UNGT_EXPR;
+    case UNLE_EXPR:
+      return UNGE_EXPR;
+    default:
+      gcc_unreachable ();
+    }
+}
+
+
+/* Convert a comparison tree code from an enum tree_code representation
+   into a compcode bit-based encoding.  This function is the inverse of
+   compcode_to_comparison.  */
+
+static enum comparison_code
+comparison_to_compcode (enum tree_code code)
+{
+  switch (code)
+    {
+    case LT_EXPR:
+      return COMPCODE_LT;
+    case EQ_EXPR:
+      return COMPCODE_EQ;
+    case LE_EXPR:
+      return COMPCODE_LE;
+    case GT_EXPR:
+      return COMPCODE_GT;
+    case NE_EXPR:
+      return COMPCODE_NE;
+    case GE_EXPR:
+      return COMPCODE_GE;
+    case ORDERED_EXPR:
+      return COMPCODE_ORD;
+    case UNORDERED_EXPR:
+      return COMPCODE_UNORD;
+    case UNLT_EXPR:
+      return COMPCODE_UNLT;
+    case UNEQ_EXPR:
+      return COMPCODE_UNEQ;
+    case UNLE_EXPR:
+      return COMPCODE_UNLE;
+    case UNGT_EXPR:
+      return COMPCODE_UNGT;
+    case LTGT_EXPR:
+      return COMPCODE_LTGT;
+    case UNGE_EXPR:
+      return COMPCODE_UNGE;
+    default:
+      gcc_unreachable ();
+    }
+}
+
+/* Convert a compcode bit-based encoding of a comparison operator back
+   to GCC's enum tree_code representation.  This function is the
+   inverse of comparison_to_compcode.  */
+
+static enum tree_code
+compcode_to_comparison (enum comparison_code code)
+{
+  switch (code)
+    {
+    case COMPCODE_LT:
+      return LT_EXPR;
+    case COMPCODE_EQ:
+      return EQ_EXPR;
+    case COMPCODE_LE:
+      return LE_EXPR;
+    case COMPCODE_GT:
+      return GT_EXPR;
+    case COMPCODE_NE:
+      return NE_EXPR;
+    case COMPCODE_GE:
+      return GE_EXPR;
+    case COMPCODE_ORD:
+      return ORDERED_EXPR;
+    case COMPCODE_UNORD:
+      return UNORDERED_EXPR;
+    case COMPCODE_UNLT:
+      return UNLT_EXPR;
+    case COMPCODE_UNEQ:
+      return UNEQ_EXPR;
+    case COMPCODE_UNLE:
+      return UNLE_EXPR;
+    case COMPCODE_UNGT:
+      return UNGT_EXPR;
+    case COMPCODE_LTGT:
+      return LTGT_EXPR;
+    case COMPCODE_UNGE:
+      return UNGE_EXPR;
+    default:
+      gcc_unreachable ();
+    }
+}
+
+/* Return a tree for the comparison which is the combination of
+   doing the AND or OR (depending on CODE) of the two operations LCODE
+   and RCODE on the identical operands LL_ARG and LR_ARG.  Take into account
+   the possibility of trapping if the mode has NaNs, and return NULL_TREE
+   if this makes the transformation invalid.  */
+
+tree
+combine_comparisons (enum tree_code code, enum tree_code lcode,
+		     enum tree_code rcode, tree truth_type,
+		     tree ll_arg, tree lr_arg)
+{
+  bool honor_nans = HONOR_NANS (TYPE_MODE (TREE_TYPE (ll_arg)));
+  enum comparison_code lcompcode = comparison_to_compcode (lcode);
+  enum comparison_code rcompcode = comparison_to_compcode (rcode);
+  enum comparison_code compcode;
+
+  switch (code)
+    {
+    case TRUTH_AND_EXPR: case TRUTH_ANDIF_EXPR:
+      compcode = lcompcode & rcompcode;
+      break;
+
+    case TRUTH_OR_EXPR: case TRUTH_ORIF_EXPR:
+      compcode = lcompcode | rcompcode;
+      break;
+
+    default:
+      return NULL_TREE;
+    }
+
+  if (!honor_nans)
+    {
+      /* Eliminate unordered comparisons, as well as LTGT and ORD
+	 which are not used unless the mode has NaNs.  */
+      compcode &= ~COMPCODE_UNORD;
+      if (compcode == COMPCODE_LTGT)
+	compcode = COMPCODE_NE;
+      else if (compcode == COMPCODE_ORD)
+	compcode = COMPCODE_TRUE;
+    }
+   else if (flag_trapping_math)
+     {
+	/* Check that the original operation and the optimized ones will trap
+	   under the same condition.  */
+	bool ltrap = (lcompcode & COMPCODE_UNORD) == 0
+		     && (lcompcode != COMPCODE_EQ)
+		     && (lcompcode != COMPCODE_ORD);
+	bool rtrap = (rcompcode & COMPCODE_UNORD) == 0
+		     && (rcompcode != COMPCODE_EQ)
+		     && (rcompcode != COMPCODE_ORD);
+	bool trap = (compcode & COMPCODE_UNORD) == 0
+		    && (compcode != COMPCODE_EQ)
+		    && (compcode != COMPCODE_ORD);
+
+        /* In a short-circuited boolean expression the LHS might be
+	   such that the RHS, if evaluated, will never trap.  For
+	   example, in ORD (x, y) && (x < y), we evaluate the RHS only
+	   if neither x nor y is NaN.  (This is a mixed blessing: for
+	   example, the expression above will never trap, hence
+	   optimizing it to x < y would be invalid).  */
+        if ((code == TRUTH_ORIF_EXPR && (lcompcode & COMPCODE_UNORD))
+            || (code == TRUTH_ANDIF_EXPR && !(lcompcode & COMPCODE_UNORD)))
+          rtrap = false;
+
+        /* If the comparison was short-circuited, and only the RHS
+	   trapped, we may now generate a spurious trap.  */
+	if (rtrap && !ltrap
+	    && (code == TRUTH_ANDIF_EXPR || code == TRUTH_ORIF_EXPR))
+	  return NULL_TREE;
+
+	/* If we changed the conditions that cause a trap, we lose.  */
+	if ((ltrap || rtrap) != trap)
+	  return NULL_TREE;
+      }
+
+  if (compcode == COMPCODE_TRUE)
+    return constant_boolean_node (true, truth_type);
+  else if (compcode == COMPCODE_FALSE)
+    return constant_boolean_node (false, truth_type);
+  else
+    return fold_build2 (compcode_to_comparison (compcode),
+			truth_type, ll_arg, lr_arg);
+}
+
+/* Return nonzero if CODE is a tree code that represents a truth value.  */
+
+static int
+truth_value_p (enum tree_code code)
+{
+  return (TREE_CODE_CLASS (code) == tcc_comparison
+	  || code == TRUTH_AND_EXPR || code == TRUTH_ANDIF_EXPR
+	  || code == TRUTH_OR_EXPR || code == TRUTH_ORIF_EXPR
+	  || code == TRUTH_XOR_EXPR || code == TRUTH_NOT_EXPR);
+}
+
+/* Return nonzero if two operands (typically of the same tree node)
+   are necessarily equal.  If either argument has side-effects this
+   function returns zero.  FLAGS modifies behavior as follows:
+
+   If OEP_ONLY_CONST is set, only return nonzero for constants.
+   This function tests whether the operands are indistinguishable;
+   it does not test whether they are equal using C's == operation.
+   The distinction is important for IEEE floating point, because
+   (1) -0.0 and 0.0 are distinguishable, but -0.0==0.0, and
+   (2) two NaNs may be indistinguishable, but NaN!=NaN.
+
+   If OEP_ONLY_CONST is unset, a VAR_DECL is considered equal to itself
+   even though it may hold multiple values during a function.
+   This is because a GCC tree node guarantees that nothing else is
+   executed between the evaluation of its "operands" (which may often
+   be evaluated in arbitrary order).  Hence if the operands themselves
+   don't side-effect, the VAR_DECLs, PARM_DECLs etc... must hold the
+   same value in each operand/subexpression.  Hence leaving OEP_ONLY_CONST
+   unset means assuming isochronic (or instantaneous) tree equivalence.
+   Unless comparing arbitrary expression trees, such as from different
+   statements, this flag can usually be left unset.
+
+   If OEP_PURE_SAME is set, then pure functions with identical arguments
+   are considered the same.  It is used when the caller has other ways
+   to ensure that global memory is unchanged in between.  */
+
+int
+operand_equal_p (tree arg0, tree arg1, unsigned int flags)
+{
+  /* If either is ERROR_MARK, they aren't equal.  */
+  if (TREE_CODE (arg0) == ERROR_MARK || TREE_CODE (arg1) == ERROR_MARK)
+    return 0;
+
+  /* If both types don't have the same signedness, then we can't consider
+     them equal.  We must check this before the STRIP_NOPS calls
+     because they may change the signedness of the arguments.  */
+  if (TYPE_UNSIGNED (TREE_TYPE (arg0)) != TYPE_UNSIGNED (TREE_TYPE (arg1)))
+    return 0;
+
+  STRIP_NOPS (arg0);
+  STRIP_NOPS (arg1);
+
+  if (TREE_CODE (arg0) != TREE_CODE (arg1)
+      /* This is needed for conversions and for COMPONENT_REF.
+	 Might as well play it safe and always test this.  */
+      || TREE_CODE (TREE_TYPE (arg0)) == ERROR_MARK
+      || TREE_CODE (TREE_TYPE (arg1)) == ERROR_MARK
+      || TYPE_MODE (TREE_TYPE (arg0)) != TYPE_MODE (TREE_TYPE (arg1)))
+    return 0;
+
+  /* If ARG0 and ARG1 are the same SAVE_EXPR, they are necessarily equal.
+     We don't care about side effects in that case because the SAVE_EXPR
+     takes care of that for us. In all other cases, two expressions are
+     equal if they have no side effects.  If we have two identical
+     expressions with side effects that should be treated the same due
+     to the only side effects being identical SAVE_EXPR's, that will
+     be detected in the recursive calls below.  */
+  if (arg0 == arg1 && ! (flags & OEP_ONLY_CONST)
+      && (TREE_CODE (arg0) == SAVE_EXPR
+	  || (! TREE_SIDE_EFFECTS (arg0) && ! TREE_SIDE_EFFECTS (arg1))))
+    return 1;
+
+  /* Next handle constant cases, those for which we can return 1 even
+     if ONLY_CONST is set.  */
+  if (TREE_CONSTANT (arg0) && TREE_CONSTANT (arg1))
+    switch (TREE_CODE (arg0))
+      {
+      case INTEGER_CST:
+	return (! TREE_CONSTANT_OVERFLOW (arg0)
+		&& ! TREE_CONSTANT_OVERFLOW (arg1)
+		&& tree_int_cst_equal (arg0, arg1));
+
+      case REAL_CST:
+	return (! TREE_CONSTANT_OVERFLOW (arg0)
+		&& ! TREE_CONSTANT_OVERFLOW (arg1)
+		&& REAL_VALUES_IDENTICAL (TREE_REAL_CST (arg0),
+					  TREE_REAL_CST (arg1)));
+
+      case VECTOR_CST:
+	{
+	  tree v1, v2;
+
+	  if (TREE_CONSTANT_OVERFLOW (arg0)
+	      || TREE_CONSTANT_OVERFLOW (arg1))
+	    return 0;
+
+	  v1 = TREE_VECTOR_CST_ELTS (arg0);
+	  v2 = TREE_VECTOR_CST_ELTS (arg1);
+	  while (v1 && v2)
+	    {
+	      if (!operand_equal_p (TREE_VALUE (v1), TREE_VALUE (v2),
+				    flags))
+		return 0;
+	      v1 = TREE_CHAIN (v1);
+	      v2 = TREE_CHAIN (v2);
+	    }
+
+	  return v1 == v2;
+	}
+
+      case COMPLEX_CST:
+	return (operand_equal_p (TREE_REALPART (arg0), TREE_REALPART (arg1),
+				 flags)
+		&& operand_equal_p (TREE_IMAGPART (arg0), TREE_IMAGPART (arg1),
+				    flags));
+
+      case STRING_CST:
+	return (TREE_STRING_LENGTH (arg0) == TREE_STRING_LENGTH (arg1)
+		&& ! memcmp (TREE_STRING_POINTER (arg0),
+			      TREE_STRING_POINTER (arg1),
+			      TREE_STRING_LENGTH (arg0)));
+
+      case ADDR_EXPR:
+	return operand_equal_p (TREE_OPERAND (arg0, 0), TREE_OPERAND (arg1, 0),
+				0);
+      default:
+	break;
+      }
+
+  if (flags & OEP_ONLY_CONST)
+    return 0;
+
+/* Define macros to test an operand from arg0 and arg1 for equality and a
+   variant that allows null and views null as being different from any
+   non-null value.  In the latter case, if either is null, the both
+   must be; otherwise, do the normal comparison.  */
+#define OP_SAME(N) operand_equal_p (TREE_OPERAND (arg0, N),	\
+				    TREE_OPERAND (arg1, N), flags)
+
+#define OP_SAME_WITH_NULL(N)				\
+  ((!TREE_OPERAND (arg0, N) || !TREE_OPERAND (arg1, N))	\
+   ? TREE_OPERAND (arg0, N) == TREE_OPERAND (arg1, N) : OP_SAME (N))
+
+  switch (TREE_CODE_CLASS (TREE_CODE (arg0)))
+    {
+    case tcc_unary:
+      /* Two conversions are equal only if signedness and modes match.  */
+      switch (TREE_CODE (arg0))
+        {
+        case NOP_EXPR:
+        case CONVERT_EXPR:
+        case FIX_CEIL_EXPR:
+        case FIX_TRUNC_EXPR:
+        case FIX_FLOOR_EXPR:
+        case FIX_ROUND_EXPR:
+	  if (TYPE_UNSIGNED (TREE_TYPE (arg0))
+	      != TYPE_UNSIGNED (TREE_TYPE (arg1)))
+	    return 0;
+	  break;
+	default:
+	  break;
+	}
+
+      return OP_SAME (0);
+
+
+    case tcc_comparison:
+    case tcc_binary:
+      if (OP_SAME (0) && OP_SAME (1))
+	return 1;
+
+      /* For commutative ops, allow the other order.  */
+      return (commutative_tree_code (TREE_CODE (arg0))
+	      && operand_equal_p (TREE_OPERAND (arg0, 0),
+				  TREE_OPERAND (arg1, 1), flags)
+	      && operand_equal_p (TREE_OPERAND (arg0, 1),
+				  TREE_OPERAND (arg1, 0), flags));
+
+    case tcc_reference:
+      /* If either of the pointer (or reference) expressions we are
+	 dereferencing contain a side effect, these cannot be equal.  */
+      if (TREE_SIDE_EFFECTS (arg0)
+	  || TREE_SIDE_EFFECTS (arg1))
+	return 0;
+
+      switch (TREE_CODE (arg0))
+	{
+	case INDIRECT_REF:
+	case ALIGN_INDIRECT_REF:
+	case MISALIGNED_INDIRECT_REF:
+	case REALPART_EXPR:
+	case IMAGPART_EXPR:
+	  return OP_SAME (0);
+
+	case ARRAY_REF:
+	case ARRAY_RANGE_REF:
+	  /* Operands 2 and 3 may be null.  */
+	  return (OP_SAME (0)
+		  && OP_SAME (1)
+		  && OP_SAME_WITH_NULL (2)
+		  && OP_SAME_WITH_NULL (3));
+
+	case COMPONENT_REF:
+	  /* Handle operand 2 the same as for ARRAY_REF.  Operand 0
+	     may be NULL when we're called to compare MEM_EXPRs.  */
+	  return OP_SAME_WITH_NULL (0)
+		 && OP_SAME (1)
+		 && OP_SAME_WITH_NULL (2);
+
+	case BIT_FIELD_REF:
+	  return OP_SAME (0) && OP_SAME (1) && OP_SAME (2);
+
+	default:
+	  return 0;
+	}
+
+    case tcc_expression:
+      switch (TREE_CODE (arg0))
+	{
+	case ADDR_EXPR:
+	case TRUTH_NOT_EXPR:
+	  return OP_SAME (0);
+
+	case TRUTH_ANDIF_EXPR:
+	case TRUTH_ORIF_EXPR:
+	  return OP_SAME (0) && OP_SAME (1);
+
+	case TRUTH_AND_EXPR:
+	case TRUTH_OR_EXPR:
+	case TRUTH_XOR_EXPR:
+	  if (OP_SAME (0) && OP_SAME (1))
+	    return 1;
+
+	  /* Otherwise take into account this is a commutative operation.  */
+	  return (operand_equal_p (TREE_OPERAND (arg0, 0),
+				   TREE_OPERAND (arg1, 1), flags)
+		  && operand_equal_p (TREE_OPERAND (arg0, 1),
+				      TREE_OPERAND (arg1, 0), flags));
+
+	case CALL_EXPR:
+	  /* If the CALL_EXPRs call different functions, then they
+	     clearly can not be equal.  */
+	  if (!OP_SAME (0))
+	    return 0;
+
+	  {
+	    unsigned int cef = call_expr_flags (arg0);
+	    if (flags & OEP_PURE_SAME)
+	      cef &= ECF_CONST | ECF_PURE;
+	    else
+	      cef &= ECF_CONST;
+	    if (!cef)
+	      return 0;
+	  }
+
+	  /* Now see if all the arguments are the same.  operand_equal_p
+	     does not handle TREE_LIST, so we walk the operands here
+	     feeding them to operand_equal_p.  */
+	  arg0 = TREE_OPERAND (arg0, 1);
+	  arg1 = TREE_OPERAND (arg1, 1);
+	  while (arg0 && arg1)
+	    {
+	      if (! operand_equal_p (TREE_VALUE (arg0), TREE_VALUE (arg1),
+				     flags))
+		return 0;
+
+	      arg0 = TREE_CHAIN (arg0);
+	      arg1 = TREE_CHAIN (arg1);
+	    }
+
+	  /* If we get here and both argument lists are exhausted
+	     then the CALL_EXPRs are equal.  */
+	  return ! (arg0 || arg1);
+
+	default:
+	  return 0;
+	}
+
+    case tcc_declaration:
+      /* Consider __builtin_sqrt equal to sqrt.  */
+      return (TREE_CODE (arg0) == FUNCTION_DECL
+	      && DECL_BUILT_IN (arg0) && DECL_BUILT_IN (arg1)
+	      && DECL_BUILT_IN_CLASS (arg0) == DECL_BUILT_IN_CLASS (arg1)
+	      && DECL_FUNCTION_CODE (arg0) == DECL_FUNCTION_CODE (arg1));
+
+    default:
+      return 0;
+    }
+
+#undef OP_SAME
+#undef OP_SAME_WITH_NULL
+}
+
+/* Similar to operand_equal_p, but see if ARG0 might have been made by
+   shorten_compare from ARG1 when ARG1 was being compared with OTHER.
+
+   When in doubt, return 0.  */
+
+static int
+operand_equal_for_comparison_p (tree arg0, tree arg1, tree other)
+{
+  int unsignedp1, unsignedpo;
+  tree primarg0, primarg1, primother;
+  unsigned int correct_width;
+
+  if (operand_equal_p (arg0, arg1, 0))
+    return 1;
+
+  if (! INTEGRAL_TYPE_P (TREE_TYPE (arg0))
+      || ! INTEGRAL_TYPE_P (TREE_TYPE (arg1)))
+    return 0;
+
+  /* Discard any conversions that don't change the modes of ARG0 and ARG1
+     and see if the inner values are the same.  This removes any
+     signedness comparison, which doesn't matter here.  */
+  primarg0 = arg0, primarg1 = arg1;
+  STRIP_NOPS (primarg0);
+  STRIP_NOPS (primarg1);
+  if (operand_equal_p (primarg0, primarg1, 0))
+    return 1;
+
+  /* Duplicate what shorten_compare does to ARG1 and see if that gives the
+     actual comparison operand, ARG0.
+
+     First throw away any conversions to wider types
+     already present in the operands.  */
+
+  primarg1 = get_narrower (arg1, &unsignedp1);
+  primother = get_narrower (other, &unsignedpo);
+
+  correct_width = TYPE_PRECISION (TREE_TYPE (arg1));
+  if (unsignedp1 == unsignedpo
+      && TYPE_PRECISION (TREE_TYPE (primarg1)) < correct_width
+      && TYPE_PRECISION (TREE_TYPE (primother)) < correct_width)
+    {
+      tree type = TREE_TYPE (arg0);
+
+      /* Make sure shorter operand is extended the right way
+	 to match the longer operand.  */
+      primarg1 = fold_convert (lang_hooks.types.signed_or_unsigned_type
+			       (unsignedp1, TREE_TYPE (primarg1)), primarg1);
+
+      if (operand_equal_p (arg0, fold_convert (type, primarg1), 0))
+	return 1;
+    }
+
+  return 0;
+}
+
+/* See if ARG is an expression that is either a comparison or is performing
+   arithmetic on comparisons.  The comparisons must only be comparing
+   two different values, which will be stored in *CVAL1 and *CVAL2; if
+   they are nonzero it means that some operands have already been found.
+   No variables may be used anywhere else in the expression except in the
+   comparisons.  If SAVE_P is true it means we removed a SAVE_EXPR around
+   the expression and save_expr needs to be called with CVAL1 and CVAL2.
+
+   If this is true, return 1.  Otherwise, return zero.  */
+
+static int
+twoval_comparison_p (tree arg, tree *cval1, tree *cval2, int *save_p)
+{
+  enum tree_code code = TREE_CODE (arg);
+  enum tree_code_class class = TREE_CODE_CLASS (code);
+
+  /* We can handle some of the tcc_expression cases here.  */
+  if (class == tcc_expression && code == TRUTH_NOT_EXPR)
+    class = tcc_unary;
+  else if (class == tcc_expression
+	   && (code == TRUTH_ANDIF_EXPR || code == TRUTH_ORIF_EXPR
+	       || code == COMPOUND_EXPR))
+    class = tcc_binary;
+
+  else if (class == tcc_expression && code == SAVE_EXPR
+	   && ! TREE_SIDE_EFFECTS (TREE_OPERAND (arg, 0)))
+    {
+      /* If we've already found a CVAL1 or CVAL2, this expression is
+	 two complex to handle.  */
+      if (*cval1 || *cval2)
+	return 0;
+
+      class = tcc_unary;
+      *save_p = 1;
+    }
+
+  switch (class)
+    {
+    case tcc_unary:
+      return twoval_comparison_p (TREE_OPERAND (arg, 0), cval1, cval2, save_p);
+
+    case tcc_binary:
+      return (twoval_comparison_p (TREE_OPERAND (arg, 0), cval1, cval2, save_p)
+	      && twoval_comparison_p (TREE_OPERAND (arg, 1),
+				      cval1, cval2, save_p));
+
+    case tcc_constant:
+      return 1;
+
+    case tcc_expression:
+      if (code == COND_EXPR)
+	return (twoval_comparison_p (TREE_OPERAND (arg, 0),
+				     cval1, cval2, save_p)
+		&& twoval_comparison_p (TREE_OPERAND (arg, 1),
+					cval1, cval2, save_p)
+		&& twoval_comparison_p (TREE_OPERAND (arg, 2),
+					cval1, cval2, save_p));
+      return 0;
+
+    case tcc_comparison:
+      /* First see if we can handle the first operand, then the second.  For
+	 the second operand, we know *CVAL1 can't be zero.  It must be that
+	 one side of the comparison is each of the values; test for the
+	 case where this isn't true by failing if the two operands
+	 are the same.  */
+
+      if (operand_equal_p (TREE_OPERAND (arg, 0),
+			   TREE_OPERAND (arg, 1), 0))
+	return 0;
+
+      if (*cval1 == 0)
+	*cval1 = TREE_OPERAND (arg, 0);
+      else if (operand_equal_p (*cval1, TREE_OPERAND (arg, 0), 0))
+	;
+      else if (*cval2 == 0)
+	*cval2 = TREE_OPERAND (arg, 0);
+      else if (operand_equal_p (*cval2, TREE_OPERAND (arg, 0), 0))
+	;
+      else
+	return 0;
+
+      if (operand_equal_p (*cval1, TREE_OPERAND (arg, 1), 0))
+	;
+      else if (*cval2 == 0)
+	*cval2 = TREE_OPERAND (arg, 1);
+      else if (operand_equal_p (*cval2, TREE_OPERAND (arg, 1), 0))
+	;
+      else
+	return 0;
+
+      return 1;
+
+    default:
+      return 0;
+    }
+}
+
+/* ARG is a tree that is known to contain just arithmetic operations and
+   comparisons.  Evaluate the operations in the tree substituting NEW0 for
+   any occurrence of OLD0 as an operand of a comparison and likewise for
+   NEW1 and OLD1.  */
+
+static tree
+eval_subst (tree arg, tree old0, tree new0, tree old1, tree new1)
+{
+  tree type = TREE_TYPE (arg);
+  enum tree_code code = TREE_CODE (arg);
+  enum tree_code_class class = TREE_CODE_CLASS (code);
+
+  /* We can handle some of the tcc_expression cases here.  */
+  if (class == tcc_expression && code == TRUTH_NOT_EXPR)
+    class = tcc_unary;
+  else if (class == tcc_expression
+	   && (code == TRUTH_ANDIF_EXPR || code == TRUTH_ORIF_EXPR))
+    class = tcc_binary;
+
+  switch (class)
+    {
+    case tcc_unary:
+      return fold_build1 (code, type,
+			  eval_subst (TREE_OPERAND (arg, 0),
+				      old0, new0, old1, new1));
+
+    case tcc_binary:
+      return fold_build2 (code, type,
+			  eval_subst (TREE_OPERAND (arg, 0),
+				      old0, new0, old1, new1),
+			  eval_subst (TREE_OPERAND (arg, 1),
+				      old0, new0, old1, new1));
+
+    case tcc_expression:
+      switch (code)
+	{
+	case SAVE_EXPR:
+	  return eval_subst (TREE_OPERAND (arg, 0), old0, new0, old1, new1);
+
+	case COMPOUND_EXPR:
+	  return eval_subst (TREE_OPERAND (arg, 1), old0, new0, old1, new1);
+
+	case COND_EXPR:
+	  return fold_build3 (code, type,
+			      eval_subst (TREE_OPERAND (arg, 0),
+					  old0, new0, old1, new1),
+			      eval_subst (TREE_OPERAND (arg, 1),
+					  old0, new0, old1, new1),
+			      eval_subst (TREE_OPERAND (arg, 2),
+					  old0, new0, old1, new1));
+	default:
+	  break;
+	}
+      /* Fall through - ???  */
+
+    case tcc_comparison:
+      {
+	tree arg0 = TREE_OPERAND (arg, 0);
+	tree arg1 = TREE_OPERAND (arg, 1);
+
+	/* We need to check both for exact equality and tree equality.  The
+	   former will be true if the operand has a side-effect.  In that
+	   case, we know the operand occurred exactly once.  */
+
+	if (arg0 == old0 || operand_equal_p (arg0, old0, 0))
+	  arg0 = new0;
+	else if (arg0 == old1 || operand_equal_p (arg0, old1, 0))
+	  arg0 = new1;
+
+	if (arg1 == old0 || operand_equal_p (arg1, old0, 0))
+	  arg1 = new0;
+	else if (arg1 == old1 || operand_equal_p (arg1, old1, 0))
+	  arg1 = new1;
+
+	return fold_build2 (code, type, arg0, arg1);
+      }
+
+    default:
+      return arg;
+    }
+}
+
+/* Return a tree for the case when the result of an expression is RESULT
+   converted to TYPE and OMITTED was previously an operand of the expression
+   but is now not needed (e.g., we folded OMITTED * 0).
+
+   If OMITTED has side effects, we must evaluate it.  Otherwise, just do
+   the conversion of RESULT to TYPE.  */
+
+tree
+omit_one_operand (tree type, tree result, tree omitted)
+{
+  tree t = fold_convert (type, result);
+
+  if (TREE_SIDE_EFFECTS (omitted))
+    return build2 (COMPOUND_EXPR, type, fold_ignored_result (omitted), t);
+
+  return non_lvalue (t);
+}
+
+/* Similar, but call pedantic_non_lvalue instead of non_lvalue.  */
+
+static tree
+pedantic_omit_one_operand (tree type, tree result, tree omitted)
+{
+  tree t = fold_convert (type, result);
+
+  if (TREE_SIDE_EFFECTS (omitted))
+    return build2 (COMPOUND_EXPR, type, fold_ignored_result (omitted), t);
+
+  return pedantic_non_lvalue (t);
+}
+
+/* Return a tree for the case when the result of an expression is RESULT
+   converted to TYPE and OMITTED1 and OMITTED2 were previously operands
+   of the expression but are now not needed.
+
+   If OMITTED1 or OMITTED2 has side effects, they must be evaluated.
+   If both OMITTED1 and OMITTED2 have side effects, OMITTED1 is
+   evaluated before OMITTED2.  Otherwise, if neither has side effects,
+   just do the conversion of RESULT to TYPE.  */
+
+tree
+omit_two_operands (tree type, tree result, tree omitted1, tree omitted2)
+{
+  tree t = fold_convert (type, result);
+
+  if (TREE_SIDE_EFFECTS (omitted2))
+    t = build2 (COMPOUND_EXPR, type, omitted2, t);
+  if (TREE_SIDE_EFFECTS (omitted1))
+    t = build2 (COMPOUND_EXPR, type, omitted1, t);
+
+  return TREE_CODE (t) != COMPOUND_EXPR ? non_lvalue (t) : t;
+}
+
+
+/* Return a simplified tree node for the truth-negation of ARG.  This
+   never alters ARG itself.  We assume that ARG is an operation that
+   returns a truth value (0 or 1).
+
+   FIXME: one would think we would fold the result, but it causes
+   problems with the dominator optimizer.  */
+tree
+invert_truthvalue (tree arg)
+{
+  tree type = TREE_TYPE (arg);
+  enum tree_code code = TREE_CODE (arg);
+
+  if (code == ERROR_MARK)
+    return arg;
+
+  /* If this is a comparison, we can simply invert it, except for
+     floating-point non-equality comparisons, in which case we just
+     enclose a TRUTH_NOT_EXPR around what we have.  */
+
+  if (TREE_CODE_CLASS (code) == tcc_comparison)
+    {
+      tree op_type = TREE_TYPE (TREE_OPERAND (arg, 0));
+      if (FLOAT_TYPE_P (op_type)
+	  && flag_trapping_math
+	  && code != ORDERED_EXPR && code != UNORDERED_EXPR
+	  && code != NE_EXPR && code != EQ_EXPR)
+	return build1 (TRUTH_NOT_EXPR, type, arg);
+      else
+	{
+	  code = invert_tree_comparison (code,
+					 HONOR_NANS (TYPE_MODE (op_type)));
+	  if (code == ERROR_MARK)
+	    return build1 (TRUTH_NOT_EXPR, type, arg);
+	  else
+	    return build2 (code, type,
+			   TREE_OPERAND (arg, 0), TREE_OPERAND (arg, 1));
+	}
+    }
+
+  switch (code)
+    {
+    case INTEGER_CST:
+      return constant_boolean_node (integer_zerop (arg), type);
+
+    case TRUTH_AND_EXPR:
+      return build2 (TRUTH_OR_EXPR, type,
+		     invert_truthvalue (TREE_OPERAND (arg, 0)),
+		     invert_truthvalue (TREE_OPERAND (arg, 1)));
+
+    case TRUTH_OR_EXPR:
+      return build2 (TRUTH_AND_EXPR, type,
+		     invert_truthvalue (TREE_OPERAND (arg, 0)),
+		     invert_truthvalue (TREE_OPERAND (arg, 1)));
+
+    case TRUTH_XOR_EXPR:
+      /* Here we can invert either operand.  We invert the first operand
+	 unless the second operand is a TRUTH_NOT_EXPR in which case our
+	 result is the XOR of the first operand with the inside of the
+	 negation of the second operand.  */
+
+      if (TREE_CODE (TREE_OPERAND (arg, 1)) == TRUTH_NOT_EXPR)
+	return build2 (TRUTH_XOR_EXPR, type, TREE_OPERAND (arg, 0),
+		       TREE_OPERAND (TREE_OPERAND (arg, 1), 0));
+      else
+	return build2 (TRUTH_XOR_EXPR, type,
+		       invert_truthvalue (TREE_OPERAND (arg, 0)),
+		       TREE_OPERAND (arg, 1));
+
+    case TRUTH_ANDIF_EXPR:
+      return build2 (TRUTH_ORIF_EXPR, type,
+		     invert_truthvalue (TREE_OPERAND (arg, 0)),
+		     invert_truthvalue (TREE_OPERAND (arg, 1)));
+
+    case TRUTH_ORIF_EXPR:
+      return build2 (TRUTH_ANDIF_EXPR, type,
+		     invert_truthvalue (TREE_OPERAND (arg, 0)),
+		     invert_truthvalue (TREE_OPERAND (arg, 1)));
+
+    case TRUTH_NOT_EXPR:
+      return TREE_OPERAND (arg, 0);
+
+    case COND_EXPR:
+      {
+	tree arg1 = TREE_OPERAND (arg, 1);
+	tree arg2 = TREE_OPERAND (arg, 2);
+	/* A COND_EXPR may have a throw as one operand, which
+	   then has void type.  Just leave void operands
+	   as they are.  */
+	return build3 (COND_EXPR, type, TREE_OPERAND (arg, 0),
+		       VOID_TYPE_P (TREE_TYPE (arg1))
+		       ? arg1 : invert_truthvalue (arg1),
+		       VOID_TYPE_P (TREE_TYPE (arg2))
+		       ? arg2 : invert_truthvalue (arg2));
+      }
+
+    case COMPOUND_EXPR:
+      return build2 (COMPOUND_EXPR, type, TREE_OPERAND (arg, 0),
+		     invert_truthvalue (TREE_OPERAND (arg, 1)));
+
+    case NON_LVALUE_EXPR:
+      return invert_truthvalue (TREE_OPERAND (arg, 0));
+
+    case NOP_EXPR:
+      if (TREE_CODE (TREE_TYPE (arg)) == BOOLEAN_TYPE)
+        break;
+
+    case CONVERT_EXPR:
+    case FLOAT_EXPR:
+      return build1 (TREE_CODE (arg), type,
+		     invert_truthvalue (TREE_OPERAND (arg, 0)));
+
+    case BIT_AND_EXPR:
+      if (!integer_onep (TREE_OPERAND (arg, 1)))
+	break;
+      return build2 (EQ_EXPR, type, arg,
+		     fold_convert (type, integer_zero_node));
+
+    case SAVE_EXPR:
+      return build1 (TRUTH_NOT_EXPR, type, arg);
+
+    case CLEANUP_POINT_EXPR:
+      return build1 (CLEANUP_POINT_EXPR, type,
+		     invert_truthvalue (TREE_OPERAND (arg, 0)));
+
+    default:
+      break;
+    }
+  gcc_assert (TREE_CODE (TREE_TYPE (arg)) == BOOLEAN_TYPE);
+  return build1 (TRUTH_NOT_EXPR, type, arg);
+}
+
+/* Given a bit-wise operation CODE applied to ARG0 and ARG1, see if both
+   operands are another bit-wise operation with a common input.  If so,
+   distribute the bit operations to save an operation and possibly two if
+   constants are involved.  For example, convert
+	(A | B) & (A | C) into A | (B & C)
+   Further simplification will occur if B and C are constants.
+
+   If this optimization cannot be done, 0 will be returned.  */
+
+static tree
+distribute_bit_expr (enum tree_code code, tree type, tree arg0, tree arg1)
+{
+  tree common;
+  tree left, right;
+
+  if (TREE_CODE (arg0) != TREE_CODE (arg1)
+      || TREE_CODE (arg0) == code
+      || (TREE_CODE (arg0) != BIT_AND_EXPR
+	  && TREE_CODE (arg0) != BIT_IOR_EXPR))
+    return 0;
+
+  if (operand_equal_p (TREE_OPERAND (arg0, 0), TREE_OPERAND (arg1, 0), 0))
+    {
+      common = TREE_OPERAND (arg0, 0);
+      left = TREE_OPERAND (arg0, 1);
+      right = TREE_OPERAND (arg1, 1);
+    }
+  else if (operand_equal_p (TREE_OPERAND (arg0, 0), TREE_OPERAND (arg1, 1), 0))
+    {
+      common = TREE_OPERAND (arg0, 0);
+      left = TREE_OPERAND (arg0, 1);
+      right = TREE_OPERAND (arg1, 0);
+    }
+  else if (operand_equal_p (TREE_OPERAND (arg0, 1), TREE_OPERAND (arg1, 0), 0))
+    {
+      common = TREE_OPERAND (arg0, 1);
+      left = TREE_OPERAND (arg0, 0);
+      right = TREE_OPERAND (arg1, 1);
+    }
+  else if (operand_equal_p (TREE_OPERAND (arg0, 1), TREE_OPERAND (arg1, 1), 0))
+    {
+      common = TREE_OPERAND (arg0, 1);
+      left = TREE_OPERAND (arg0, 0);
+      right = TREE_OPERAND (arg1, 0);
+    }
+  else
+    return 0;
+
+  return fold_build2 (TREE_CODE (arg0), type, common,
+		      fold_build2 (code, type, left, right));
+}
+
+/* Knowing that ARG0 and ARG1 are both RDIV_EXPRs, simplify a binary operation
+   with code CODE.  This optimization is unsafe.  */
+static tree
+distribute_real_division (enum tree_code code, tree type, tree arg0, tree arg1)
+{
+  bool mul0 = TREE_CODE (arg0) == MULT_EXPR;
+  bool mul1 = TREE_CODE (arg1) == MULT_EXPR;
+
+  /* (A / C) +- (B / C) -> (A +- B) / C.  */
+  if (mul0 == mul1
+      && operand_equal_p (TREE_OPERAND (arg0, 1),
+		       TREE_OPERAND (arg1, 1), 0))
+    return fold_build2 (mul0 ? MULT_EXPR : RDIV_EXPR, type,
+			fold_build2 (code, type,
+				     TREE_OPERAND (arg0, 0),
+				     TREE_OPERAND (arg1, 0)),
+			TREE_OPERAND (arg0, 1));
+
+  /* (A / C1) +- (A / C2) -> A * (1 / C1 +- 1 / C2).  */
+  if (operand_equal_p (TREE_OPERAND (arg0, 0),
+		       TREE_OPERAND (arg1, 0), 0)
+      && TREE_CODE (TREE_OPERAND (arg0, 1)) == REAL_CST
+      && TREE_CODE (TREE_OPERAND (arg1, 1)) == REAL_CST)
+    {
+      REAL_VALUE_TYPE r0, r1;
+      r0 = TREE_REAL_CST (TREE_OPERAND (arg0, 1));
+      r1 = TREE_REAL_CST (TREE_OPERAND (arg1, 1));
+      if (!mul0)
+	real_arithmetic (&r0, RDIV_EXPR, &dconst1, &r0);
+      if (!mul1)
+        real_arithmetic (&r1, RDIV_EXPR, &dconst1, &r1);
+      real_arithmetic (&r0, code, &r0, &r1);
+      return fold_build2 (MULT_EXPR, type,
+			  TREE_OPERAND (arg0, 0),
+			  build_real (type, r0));
+    }
+
+  return NULL_TREE;
+}
+
+/* Return a BIT_FIELD_REF of type TYPE to refer to BITSIZE bits of INNER
+   starting at BITPOS.  The field is unsigned if UNSIGNEDP is nonzero.  */
+
+static tree
+make_bit_field_ref (tree inner, tree type, int bitsize, int bitpos,
+		    int unsignedp)
+{
+  tree result;
+
+  if (bitpos == 0)
+    {
+      tree size = TYPE_SIZE (TREE_TYPE (inner));
+      if ((INTEGRAL_TYPE_P (TREE_TYPE (inner))
+	   || POINTER_TYPE_P (TREE_TYPE (inner)))
+	  && host_integerp (size, 0) 
+	  && tree_low_cst (size, 0) == bitsize)
+	return fold_convert (type, inner);
+    }
+
+  result = build3 (BIT_FIELD_REF, type, inner,
+		   size_int (bitsize), bitsize_int (bitpos));
+
+  BIT_FIELD_REF_UNSIGNED (result) = unsignedp;
+
+  return result;
+}
+
+/* Optimize a bit-field compare.
+
+   There are two cases:  First is a compare against a constant and the
+   second is a comparison of two items where the fields are at the same
+   bit position relative to the start of a chunk (byte, halfword, word)
+   large enough to contain it.  In these cases we can avoid the shift
+   implicit in bitfield extractions.
+
+   For constants, we emit a compare of the shifted constant with the
+   BIT_AND_EXPR of a mask and a byte, halfword, or word of the operand being
+   compared.  For two fields at the same position, we do the ANDs with the
+   similar mask and compare the result of the ANDs.
+
+   CODE is the comparison code, known to be either NE_EXPR or EQ_EXPR.
+   COMPARE_TYPE is the type of the comparison, and LHS and RHS
+   are the left and right operands of the comparison, respectively.
+
+   If the optimization described above can be done, we return the resulting
+   tree.  Otherwise we return zero.  */
+
+static tree
+optimize_bit_field_compare (enum tree_code code, tree compare_type,
+			    tree lhs, tree rhs)
+{
+  HOST_WIDE_INT lbitpos, lbitsize, rbitpos, rbitsize, nbitpos, nbitsize;
+  tree type = TREE_TYPE (lhs);
+  tree signed_type, unsigned_type;
+  int const_p = TREE_CODE (rhs) == INTEGER_CST;
+  enum machine_mode lmode, rmode, nmode;
+  int lunsignedp, runsignedp;
+  int lvolatilep = 0, rvolatilep = 0;
+  tree linner, rinner = NULL_TREE;
+  tree mask;
+  tree offset;
+
+  /* Get all the information about the extractions being done.  If the bit size
+     if the same as the size of the underlying object, we aren't doing an
+     extraction at all and so can do nothing.  We also don't want to
+     do anything if the inner expression is a PLACEHOLDER_EXPR since we
+     then will no longer be able to replace it.  */
+  linner = get_inner_reference (lhs, &lbitsize, &lbitpos, &offset, &lmode,
+				&lunsignedp, &lvolatilep, false);
+  if (linner == lhs || lbitsize == GET_MODE_BITSIZE (lmode) || lbitsize < 0
+      || offset != 0 || TREE_CODE (linner) == PLACEHOLDER_EXPR)
+    return 0;
+
+ if (!const_p)
+   {
+     /* If this is not a constant, we can only do something if bit positions,
+	sizes, and signedness are the same.  */
+     rinner = get_inner_reference (rhs, &rbitsize, &rbitpos, &offset, &rmode,
+				   &runsignedp, &rvolatilep, false);
+
+     if (rinner == rhs || lbitpos != rbitpos || lbitsize != rbitsize
+	 || lunsignedp != runsignedp || offset != 0
+	 || TREE_CODE (rinner) == PLACEHOLDER_EXPR)
+       return 0;
+   }
+
+  /* See if we can find a mode to refer to this field.  We should be able to,
+     but fail if we can't.  */
+  nmode = get_best_mode (lbitsize, lbitpos,
+			 const_p ? TYPE_ALIGN (TREE_TYPE (linner))
+			 : MIN (TYPE_ALIGN (TREE_TYPE (linner)),
+				TYPE_ALIGN (TREE_TYPE (rinner))),
+			 word_mode, lvolatilep || rvolatilep);
+  if (nmode == VOIDmode)
+    return 0;
+
+  /* Set signed and unsigned types of the precision of this mode for the
+     shifts below.  */
+  signed_type = lang_hooks.types.type_for_mode (nmode, 0);
+  unsigned_type = lang_hooks.types.type_for_mode (nmode, 1);
+
+  /* Compute the bit position and size for the new reference and our offset
+     within it. If the new reference is the same size as the original, we
+     won't optimize anything, so return zero.  */
+  nbitsize = GET_MODE_BITSIZE (nmode);
+  nbitpos = lbitpos & ~ (nbitsize - 1);
+  lbitpos -= nbitpos;
+  if (nbitsize == lbitsize)
+    return 0;
+
+  if (BYTES_BIG_ENDIAN)
+    lbitpos = nbitsize - lbitsize - lbitpos;
+
+  /* Make the mask to be used against the extracted field.  */
+  mask = build_int_cst (unsigned_type, -1);
+  mask = force_fit_type (mask, 0, false, false);
+  mask = fold_convert (unsigned_type, mask);
+  mask = const_binop (LSHIFT_EXPR, mask, size_int (nbitsize - lbitsize), 0);
+  mask = const_binop (RSHIFT_EXPR, mask,
+		      size_int (nbitsize - lbitsize - lbitpos), 0);
+
+  if (! const_p)
+    /* If not comparing with constant, just rework the comparison
+       and return.  */
+    return build2 (code, compare_type,
+		   build2 (BIT_AND_EXPR, unsigned_type,
+			   make_bit_field_ref (linner, unsigned_type,
+					       nbitsize, nbitpos, 1),
+			   mask),
+		   build2 (BIT_AND_EXPR, unsigned_type,
+			   make_bit_field_ref (rinner, unsigned_type,
+					       nbitsize, nbitpos, 1),
+			   mask));
+
+  /* Otherwise, we are handling the constant case. See if the constant is too
+     big for the field.  Warn and return a tree of for 0 (false) if so.  We do
+     this not only for its own sake, but to avoid having to test for this
+     error case below.  If we didn't, we might generate wrong code.
+
+     For unsigned fields, the constant shifted right by the field length should
+     be all zero.  For signed fields, the high-order bits should agree with
+     the sign bit.  */
+
+  if (lunsignedp)
+    {
+      if (! integer_zerop (const_binop (RSHIFT_EXPR,
+					fold_convert (unsigned_type, rhs),
+					size_int (lbitsize), 0)))
+	{
+	  warning (0, "comparison is always %d due to width of bit-field",
+		   code == NE_EXPR);
+	  return constant_boolean_node (code == NE_EXPR, compare_type);
+	}
+    }
+  else
+    {
+      tree tem = const_binop (RSHIFT_EXPR, fold_convert (signed_type, rhs),
+			      size_int (lbitsize - 1), 0);
+      if (! integer_zerop (tem) && ! integer_all_onesp (tem))
+	{
+	  warning (0, "comparison is always %d due to width of bit-field",
+		   code == NE_EXPR);
+	  return constant_boolean_node (code == NE_EXPR, compare_type);
+	}
+    }
+
+  /* Single-bit compares should always be against zero.  */
+  if (lbitsize == 1 && ! integer_zerop (rhs))
+    {
+      code = code == EQ_EXPR ? NE_EXPR : EQ_EXPR;
+      rhs = fold_convert (type, integer_zero_node);
+    }
+
+  /* Make a new bitfield reference, shift the constant over the
+     appropriate number of bits and mask it with the computed mask
+     (in case this was a signed field).  If we changed it, make a new one.  */
+  lhs = make_bit_field_ref (linner, unsigned_type, nbitsize, nbitpos, 1);
+  if (lvolatilep)
+    {
+      TREE_SIDE_EFFECTS (lhs) = 1;
+      TREE_THIS_VOLATILE (lhs) = 1;
+    }
+
+  rhs = const_binop (BIT_AND_EXPR,
+		     const_binop (LSHIFT_EXPR,
+				  fold_convert (unsigned_type, rhs),
+				  size_int (lbitpos), 0),
+		     mask, 0);
+
+  return build2 (code, compare_type,
+		 build2 (BIT_AND_EXPR, unsigned_type, lhs, mask),
+		 rhs);
+}
+
+/* Subroutine for fold_truthop: decode a field reference.
+
+   If EXP is a comparison reference, we return the innermost reference.
+
+   *PBITSIZE is set to the number of bits in the reference, *PBITPOS is
+   set to the starting bit number.
+
+   If the innermost field can be completely contained in a mode-sized
+   unit, *PMODE is set to that mode.  Otherwise, it is set to VOIDmode.
+
+   *PVOLATILEP is set to 1 if the any expression encountered is volatile;
+   otherwise it is not changed.
+
+   *PUNSIGNEDP is set to the signedness of the field.
+
+   *PMASK is set to the mask used.  This is either contained in a
+   BIT_AND_EXPR or derived from the width of the field.
+
+   *PAND_MASK is set to the mask found in a BIT_AND_EXPR, if any.
+
+   Return 0 if this is not a component reference or is one that we can't
+   do anything with.  */
+
+static tree
+decode_field_reference (tree exp, HOST_WIDE_INT *pbitsize,
+			HOST_WIDE_INT *pbitpos, enum machine_mode *pmode,
+			int *punsignedp, int *pvolatilep,
+			tree *pmask, tree *pand_mask)
+{
+  tree outer_type = 0;
+  tree and_mask = 0;
+  tree mask, inner, offset;
+  tree unsigned_type;
+  unsigned int precision;
+
+  /* All the optimizations using this function assume integer fields.
+     There are problems with FP fields since the type_for_size call
+     below can fail for, e.g., XFmode.  */
+  if (! INTEGRAL_TYPE_P (TREE_TYPE (exp)))
+    return 0;
+
+  /* We are interested in the bare arrangement of bits, so strip everything
+     that doesn't affect the machine mode.  However, record the type of the
+     outermost expression if it may matter below.  */
+  if (TREE_CODE (exp) == NOP_EXPR
+      || TREE_CODE (exp) == CONVERT_EXPR
+      || TREE_CODE (exp) == NON_LVALUE_EXPR)
+    outer_type = TREE_TYPE (exp);
+  STRIP_NOPS (exp);
+
+  if (TREE_CODE (exp) == BIT_AND_EXPR)
+    {
+      and_mask = TREE_OPERAND (exp, 1);
+      exp = TREE_OPERAND (exp, 0);
+      STRIP_NOPS (exp); STRIP_NOPS (and_mask);
+      if (TREE_CODE (and_mask) != INTEGER_CST)
+	return 0;
+    }
+
+  inner = get_inner_reference (exp, pbitsize, pbitpos, &offset, pmode,
+			       punsignedp, pvolatilep, false);
+  if ((inner == exp && and_mask == 0)
+      || *pbitsize < 0 || offset != 0
+      || TREE_CODE (inner) == PLACEHOLDER_EXPR)
+    return 0;
+
+  /* If the number of bits in the reference is the same as the bitsize of
+     the outer type, then the outer type gives the signedness. Otherwise
+     (in case of a small bitfield) the signedness is unchanged.  */
+  if (outer_type && *pbitsize == TYPE_PRECISION (outer_type))
+    *punsignedp = TYPE_UNSIGNED (outer_type);
+
+  /* Compute the mask to access the bitfield.  */
+  unsigned_type = lang_hooks.types.type_for_size (*pbitsize, 1);
+  precision = TYPE_PRECISION (unsigned_type);
+
+  mask = build_int_cst (unsigned_type, -1);
+  mask = force_fit_type (mask, 0, false, false);
+
+  mask = const_binop (LSHIFT_EXPR, mask, size_int (precision - *pbitsize), 0);
+  mask = const_binop (RSHIFT_EXPR, mask, size_int (precision - *pbitsize), 0);
+
+  /* Merge it with the mask we found in the BIT_AND_EXPR, if any.  */
+  if (and_mask != 0)
+    mask = fold_build2 (BIT_AND_EXPR, unsigned_type,
+			fold_convert (unsigned_type, and_mask), mask);
+
+  *pmask = mask;
+  *pand_mask = and_mask;
+  return inner;
+}
+
+/* Return nonzero if MASK represents a mask of SIZE ones in the low-order
+   bit positions.  */
+
+static int
+all_ones_mask_p (tree mask, int size)
+{
+  tree type = TREE_TYPE (mask);
+  unsigned int precision = TYPE_PRECISION (type);
+  tree tmask;
+
+  tmask = build_int_cst (lang_hooks.types.signed_type (type), -1);
+  tmask = force_fit_type (tmask, 0, false, false);
+
+  return
+    tree_int_cst_equal (mask,
+			const_binop (RSHIFT_EXPR,
+				     const_binop (LSHIFT_EXPR, tmask,
+						  size_int (precision - size),
+						  0),
+				     size_int (precision - size), 0));
+}
+
+/* Subroutine for fold: determine if VAL is the INTEGER_CONST that
+   represents the sign bit of EXP's type.  If EXP represents a sign
+   or zero extension, also test VAL against the unextended type.
+   The return value is the (sub)expression whose sign bit is VAL,
+   or NULL_TREE otherwise.  */
+
+static tree
+sign_bit_p (tree exp, tree val)
+{
+  unsigned HOST_WIDE_INT mask_lo, lo;
+  HOST_WIDE_INT mask_hi, hi;
+  int width;
+  tree t;
+
+  /* Tree EXP must have an integral type.  */
+  t = TREE_TYPE (exp);
+  if (! INTEGRAL_TYPE_P (t))
+    return NULL_TREE;
+
+  /* Tree VAL must be an integer constant.  */
+  if (TREE_CODE (val) != INTEGER_CST
+      || TREE_CONSTANT_OVERFLOW (val))
+    return NULL_TREE;
+
+  width = TYPE_PRECISION (t);
+  if (width > HOST_BITS_PER_WIDE_INT)
+    {
+      hi = (unsigned HOST_WIDE_INT) 1 << (width - HOST_BITS_PER_WIDE_INT - 1);
+      lo = 0;
+
+      mask_hi = ((unsigned HOST_WIDE_INT) -1
+		 >> (2 * HOST_BITS_PER_WIDE_INT - width));
+      mask_lo = -1;
+    }
+  else
+    {
+      hi = 0;
+      lo = (unsigned HOST_WIDE_INT) 1 << (width - 1);
+
+      mask_hi = 0;
+      mask_lo = ((unsigned HOST_WIDE_INT) -1
+		 >> (HOST_BITS_PER_WIDE_INT - width));
+    }
+
+  /* We mask off those bits beyond TREE_TYPE (exp) so that we can
+     treat VAL as if it were unsigned.  */
+  if ((TREE_INT_CST_HIGH (val) & mask_hi) == hi
+      && (TREE_INT_CST_LOW (val) & mask_lo) == lo)
+    return exp;
+
+  /* Handle extension from a narrower type.  */
+  if (TREE_CODE (exp) == NOP_EXPR
+      && TYPE_PRECISION (TREE_TYPE (TREE_OPERAND (exp, 0))) < width)
+    return sign_bit_p (TREE_OPERAND (exp, 0), val);
+
+  return NULL_TREE;
+}
+
+/* Subroutine for fold_truthop: determine if an operand is simple enough
+   to be evaluated unconditionally.  */
+
+static int
+simple_operand_p (tree exp)
+{
+  /* Strip any conversions that don't change the machine mode.  */
+  STRIP_NOPS (exp);
+
+  return (CONSTANT_CLASS_P (exp)
+	  || TREE_CODE (exp) == SSA_NAME
+	  || (DECL_P (exp)
+	      && ! TREE_ADDRESSABLE (exp)
+	      && ! TREE_THIS_VOLATILE (exp)
+	      && ! DECL_NONLOCAL (exp)
+	      /* Don't regard global variables as simple.  They may be
+		 allocated in ways unknown to the compiler (shared memory,
+		 #pragma weak, etc).  */
+	      && ! TREE_PUBLIC (exp)
+	      && ! DECL_EXTERNAL (exp)
+	      /* Loading a static variable is unduly expensive, but global
+		 registers aren't expensive.  */
+	      && (! TREE_STATIC (exp) || DECL_REGISTER (exp))));
+}
+
+/* The following functions are subroutines to fold_range_test and allow it to
+   try to change a logical combination of comparisons into a range test.
+
+   For example, both
+	X == 2 || X == 3 || X == 4 || X == 5
+   and
+	X >= 2 && X <= 5
+   are converted to
+	(unsigned) (X - 2) <= 3
+
+   We describe each set of comparisons as being either inside or outside
+   a range, using a variable named like IN_P, and then describe the
+   range with a lower and upper bound.  If one of the bounds is omitted,
+   it represents either the highest or lowest value of the type.
+
+   In the comments below, we represent a range by two numbers in brackets
+   preceded by a "+" to designate being inside that range, or a "-" to
+   designate being outside that range, so the condition can be inverted by
+   flipping the prefix.  An omitted bound is represented by a "-".  For
+   example, "- [-, 10]" means being outside the range starting at the lowest
+   possible value and ending at 10, in other words, being greater than 10.
+   The range "+ [-, -]" is always true and hence the range "- [-, -]" is
+   always false.
+
+   We set up things so that the missing bounds are handled in a consistent
+   manner so neither a missing bound nor "true" and "false" need to be
+   handled using a special case.  */
+
+/* Return the result of applying CODE to ARG0 and ARG1, but handle the case
+   of ARG0 and/or ARG1 being omitted, meaning an unlimited range. UPPER0_P
+   and UPPER1_P are nonzero if the respective argument is an upper bound
+   and zero for a lower.  TYPE, if nonzero, is the type of the result; it
+   must be specified for a comparison.  ARG1 will be converted to ARG0's
+   type if both are specified.  */
+
+static tree
+range_binop (enum tree_code code, tree type, tree arg0, int upper0_p,
+	     tree arg1, int upper1_p)
+{
+  tree tem;
+  int result;
+  int sgn0, sgn1;
+
+  /* If neither arg represents infinity, do the normal operation.
+     Else, if not a comparison, return infinity.  Else handle the special
+     comparison rules. Note that most of the cases below won't occur, but
+     are handled for consistency.  */
+
+  if (arg0 != 0 && arg1 != 0)
+    {
+      tem = fold_build2 (code, type != 0 ? type : TREE_TYPE (arg0),
+			 arg0, fold_convert (TREE_TYPE (arg0), arg1));
+      STRIP_NOPS (tem);
+      return TREE_CODE (tem) == INTEGER_CST ? tem : 0;
+    }
+
+  if (TREE_CODE_CLASS (code) != tcc_comparison)
+    return 0;
+
+  /* Set SGN[01] to -1 if ARG[01] is a lower bound, 1 for upper, and 0
+     for neither.  In real maths, we cannot assume open ended ranges are
+     the same. But, this is computer arithmetic, where numbers are finite.
+     We can therefore make the transformation of any unbounded range with
+     the value Z, Z being greater than any representable number. This permits
+     us to treat unbounded ranges as equal.  */
+  sgn0 = arg0 != 0 ? 0 : (upper0_p ? 1 : -1);
+  sgn1 = arg1 != 0 ? 0 : (upper1_p ? 1 : -1);
+  switch (code)
+    {
+    case EQ_EXPR:
+      result = sgn0 == sgn1;
+      break;
+    case NE_EXPR:
+      result = sgn0 != sgn1;
+      break;
+    case LT_EXPR:
+      result = sgn0 < sgn1;
+      break;
+    case LE_EXPR:
+      result = sgn0 <= sgn1;
+      break;
+    case GT_EXPR:
+      result = sgn0 > sgn1;
+      break;
+    case GE_EXPR:
+      result = sgn0 >= sgn1;
+      break;
+    default:
+      gcc_unreachable ();
+    }
+
+  return constant_boolean_node (result, type);
+}
+
+/* Given EXP, a logical expression, set the range it is testing into
+   variables denoted by PIN_P, PLOW, and PHIGH.  Return the expression
+   actually being tested.  *PLOW and *PHIGH will be made of the same type
+   as the returned expression.  If EXP is not a comparison, we will most
+   likely not be returning a useful value and range.  */
+
+static tree
+make_range (tree exp, int *pin_p, tree *plow, tree *phigh)
+{
+  enum tree_code code;
+  tree arg0 = NULL_TREE, arg1 = NULL_TREE;
+  tree exp_type = NULL_TREE, arg0_type = NULL_TREE;
+  int in_p, n_in_p;
+  tree low, high, n_low, n_high;
+
+  /* Start with simply saying "EXP != 0" and then look at the code of EXP
+     and see if we can refine the range.  Some of the cases below may not
+     happen, but it doesn't seem worth worrying about this.  We "continue"
+     the outer loop when we've changed something; otherwise we "break"
+     the switch, which will "break" the while.  */
+
+  in_p = 0;
+  low = high = fold_convert (TREE_TYPE (exp), integer_zero_node);
+
+  while (1)
+    {
+      code = TREE_CODE (exp);
+      exp_type = TREE_TYPE (exp);
+
+      if (IS_EXPR_CODE_CLASS (TREE_CODE_CLASS (code)))
+	{
+	  if (TREE_CODE_LENGTH (code) > 0)
+	    arg0 = TREE_OPERAND (exp, 0);
+	  if (TREE_CODE_CLASS (code) == tcc_comparison
+	      || TREE_CODE_CLASS (code) == tcc_unary
+	      || TREE_CODE_CLASS (code) == tcc_binary)
+	    arg0_type = TREE_TYPE (arg0);
+	  if (TREE_CODE_CLASS (code) == tcc_binary
+	      || TREE_CODE_CLASS (code) == tcc_comparison
+	      || (TREE_CODE_CLASS (code) == tcc_expression
+		  && TREE_CODE_LENGTH (code) > 1))
+	    arg1 = TREE_OPERAND (exp, 1);
+	}
+
+      switch (code)
+	{
+	case TRUTH_NOT_EXPR:
+	  in_p = ! in_p, exp = arg0;
+	  continue;
+
+	case EQ_EXPR: case NE_EXPR:
+	case LT_EXPR: case LE_EXPR: case GE_EXPR: case GT_EXPR:
+	  /* We can only do something if the range is testing for zero
+	     and if the second operand is an integer constant.  Note that
+	     saying something is "in" the range we make is done by
+	     complementing IN_P since it will set in the initial case of
+	     being not equal to zero; "out" is leaving it alone.  */
+	  if (low == 0 || high == 0
+	      || ! integer_zerop (low) || ! integer_zerop (high)
+	      || TREE_CODE (arg1) != INTEGER_CST)
+	    break;
+
+	  switch (code)
+	    {
+	    case NE_EXPR:  /* - [c, c]  */
+	      low = high = arg1;
+	      break;
+	    case EQ_EXPR:  /* + [c, c]  */
+	      in_p = ! in_p, low = high = arg1;
+	      break;
+	    case GT_EXPR:  /* - [-, c] */
+	      low = 0, high = arg1;
+	      break;
+	    case GE_EXPR:  /* + [c, -] */
+	      in_p = ! in_p, low = arg1, high = 0;
+	      break;
+	    case LT_EXPR:  /* - [c, -] */
+	      low = arg1, high = 0;
+	      break;
+	    case LE_EXPR:  /* + [-, c] */
+	      in_p = ! in_p, low = 0, high = arg1;
+	      break;
+	    default:
+	      gcc_unreachable ();
+	    }
+
+	  /* If this is an unsigned comparison, we also know that EXP is
+	     greater than or equal to zero.  We base the range tests we make
+	     on that fact, so we record it here so we can parse existing
+	     range tests.  We test arg0_type since often the return type
+	     of, e.g. EQ_EXPR, is boolean.  */
+	  if (TYPE_UNSIGNED (arg0_type) && (low == 0 || high == 0))
+	    {
+	      if (! merge_ranges (&n_in_p, &n_low, &n_high,
+				  in_p, low, high, 1,
+				  fold_convert (arg0_type, integer_zero_node),
+				  NULL_TREE))
+		break;
+
+	      in_p = n_in_p, low = n_low, high = n_high;
+
+	      /* If the high bound is missing, but we have a nonzero low
+		 bound, reverse the range so it goes from zero to the low bound
+		 minus 1.  */
+	      if (high == 0 && low && ! integer_zerop (low))
+		{
+		  in_p = ! in_p;
+		  high = range_binop (MINUS_EXPR, NULL_TREE, low, 0,
+				      integer_one_node, 0);
+		  low = fold_convert (arg0_type, integer_zero_node);
+		}
+	    }
+
+	  exp = arg0;
+	  continue;
+
+	case NEGATE_EXPR:
+	  /* (-x) IN [a,b] -> x in [-b, -a]  */
+	  n_low = range_binop (MINUS_EXPR, exp_type,
+			       fold_convert (exp_type, integer_zero_node),
+			       0, high, 1);
+	  n_high = range_binop (MINUS_EXPR, exp_type,
+				fold_convert (exp_type, integer_zero_node),
+				0, low, 0);
+	  low = n_low, high = n_high;
+	  exp = arg0;
+	  continue;
+
+	case BIT_NOT_EXPR:
+	  /* ~ X -> -X - 1  */
+	  exp = build2 (MINUS_EXPR, exp_type, negate_expr (arg0),
+			fold_convert (exp_type, integer_one_node));
+	  continue;
+
+	case PLUS_EXPR:  case MINUS_EXPR:
+	  if (TREE_CODE (arg1) != INTEGER_CST)
+	    break;
+
+	  /* If EXP is signed, any overflow in the computation is undefined,
+	     so we don't worry about it so long as our computations on
+	     the bounds don't overflow.  For unsigned, overflow is defined
+	     and this is exactly the right thing.  */
+	  n_low = range_binop (code == MINUS_EXPR ? PLUS_EXPR : MINUS_EXPR,
+			       arg0_type, low, 0, arg1, 0);
+	  n_high = range_binop (code == MINUS_EXPR ? PLUS_EXPR : MINUS_EXPR,
+				arg0_type, high, 1, arg1, 0);
+	  if ((n_low != 0 && TREE_OVERFLOW (n_low))
+	      || (n_high != 0 && TREE_OVERFLOW (n_high)))
+	    break;
+
+	  /* Check for an unsigned range which has wrapped around the maximum
+	     value thus making n_high < n_low, and normalize it.  */
+	  if (n_low && n_high && tree_int_cst_lt (n_high, n_low))
+	    {
+	      low = range_binop (PLUS_EXPR, arg0_type, n_high, 0,
+				 integer_one_node, 0);
+	      high = range_binop (MINUS_EXPR, arg0_type, n_low, 0,
+				  integer_one_node, 0);
+
+	      /* If the range is of the form +/- [ x+1, x ], we won't
+		 be able to normalize it.  But then, it represents the
+		 whole range or the empty set, so make it
+		 +/- [ -, - ].  */
+	      if (tree_int_cst_equal (n_low, low)
+		  && tree_int_cst_equal (n_high, high))
+		low = high = 0;
+	      else
+		in_p = ! in_p;
+	    }
+	  else
+	    low = n_low, high = n_high;
+
+	  exp = arg0;
+	  continue;
+
+	case NOP_EXPR:  case NON_LVALUE_EXPR:  case CONVERT_EXPR:
+	  if (TYPE_PRECISION (arg0_type) > TYPE_PRECISION (exp_type))
+	    break;
+
+	  if (! INTEGRAL_TYPE_P (arg0_type)
+	      || (low != 0 && ! int_fits_type_p (low, arg0_type))
+	      || (high != 0 && ! int_fits_type_p (high, arg0_type)))
+	    break;
+
+	  n_low = low, n_high = high;
+
+	  if (n_low != 0)
+	    n_low = fold_convert (arg0_type, n_low);
+
+	  if (n_high != 0)
+	    n_high = fold_convert (arg0_type, n_high);
+
+
+	  /* If we're converting arg0 from an unsigned type, to exp,
+	     a signed type,  we will be doing the comparison as unsigned.
+	     The tests above have already verified that LOW and HIGH
+	     are both positive.
+
+	     So we have to ensure that we will handle large unsigned
+	     values the same way that the current signed bounds treat
+	     negative values.  */
+
+	  if (!TYPE_UNSIGNED (exp_type) && TYPE_UNSIGNED (arg0_type))
+	    {
+	      tree high_positive;
+	      tree equiv_type = lang_hooks.types.type_for_mode
+		(TYPE_MODE (arg0_type), 1);
+
+	      /* A range without an upper bound is, naturally, unbounded.
+		 Since convert would have cropped a very large value, use
+		 the max value for the destination type.  */
+	      high_positive
+		= TYPE_MAX_VALUE (equiv_type) ? TYPE_MAX_VALUE (equiv_type)
+		: TYPE_MAX_VALUE (arg0_type);
+
+	      if (TYPE_PRECISION (exp_type) == TYPE_PRECISION (arg0_type))
+		high_positive = fold_build2 (RSHIFT_EXPR, arg0_type,
+					     fold_convert (arg0_type,
+							   high_positive),
+					     fold_convert (arg0_type,
+							   integer_one_node));
+
+	      /* If the low bound is specified, "and" the range with the
+		 range for which the original unsigned value will be
+		 positive.  */
+	      if (low != 0)
+		{
+		  if (! merge_ranges (&n_in_p, &n_low, &n_high,
+				      1, n_low, n_high, 1,
+				      fold_convert (arg0_type,
+						    integer_zero_node),
+				      high_positive))
+		    break;
+
+		  in_p = (n_in_p == in_p);
+		}
+	      else
+		{
+		  /* Otherwise, "or" the range with the range of the input
+		     that will be interpreted as negative.  */
+		  if (! merge_ranges (&n_in_p, &n_low, &n_high,
+				      0, n_low, n_high, 1,
+				      fold_convert (arg0_type,
+						    integer_zero_node),
+				      high_positive))
+		    break;
+
+		  in_p = (in_p != n_in_p);
+		}
+	    }
+
+	  exp = arg0;
+	  low = n_low, high = n_high;
+	  continue;
+
+	default:
+	  break;
+	}
+
+      break;
+    }
+
+  /* If EXP is a constant, we can evaluate whether this is true or false.  */
+  if (TREE_CODE (exp) == INTEGER_CST)
+    {
+      in_p = in_p == (integer_onep (range_binop (GE_EXPR, integer_type_node,
+						 exp, 0, low, 0))
+		      && integer_onep (range_binop (LE_EXPR, integer_type_node,
+						    exp, 1, high, 1)));
+      low = high = 0;
+      exp = 0;
+    }
+
+  *pin_p = in_p, *plow = low, *phigh = high;
+  return exp;
+}
+
+/* Given a range, LOW, HIGH, and IN_P, an expression, EXP, and a result
+   type, TYPE, return an expression to test if EXP is in (or out of, depending
+   on IN_P) the range.  Return 0 if the test couldn't be created.  */
+
+static tree
+build_range_check (tree type, tree exp, int in_p, tree low, tree high)
+{
+  tree etype = TREE_TYPE (exp);
+  tree value;
+
+#ifdef HAVE_canonicalize_funcptr_for_compare
+  /* Disable this optimization for function pointer expressions
+     on targets that require function pointer canonicalization.  */
+  if (HAVE_canonicalize_funcptr_for_compare
+      && TREE_CODE (etype) == POINTER_TYPE
+      && TREE_CODE (TREE_TYPE (etype)) == FUNCTION_TYPE)
+    return NULL_TREE;
+#endif
+
+  if (! in_p)
+    {
+      value = build_range_check (type, exp, 1, low, high);
+      if (value != 0)
+        return invert_truthvalue (value);
+
+      return 0;
+    }
+
+  if (low == 0 && high == 0)
+    return fold_convert (type, integer_one_node);
+
+  if (low == 0)
+    return fold_build2 (LE_EXPR, type, exp,
+			fold_convert (etype, high));
+
+  if (high == 0)
+    return fold_build2 (GE_EXPR, type, exp,
+			fold_convert (etype, low));
+
+  if (operand_equal_p (low, high, 0))
+    return fold_build2 (EQ_EXPR, type, exp,
+			fold_convert (etype, low));
+
+  if (integer_zerop (low))
+    {
+      if (! TYPE_UNSIGNED (etype))
+	{
+	  etype = lang_hooks.types.unsigned_type (etype);
+	  high = fold_convert (etype, high);
+	  exp = fold_convert (etype, exp);
+	}
+      return build_range_check (type, exp, 1, 0, high);
+    }
+
+  /* Optimize (c>=1) && (c<=127) into (signed char)c > 0.  */
+  if (integer_onep (low) && TREE_CODE (high) == INTEGER_CST)
+    {
+      unsigned HOST_WIDE_INT lo;
+      HOST_WIDE_INT hi;
+      int prec;
+
+      prec = TYPE_PRECISION (etype);
+      if (prec <= HOST_BITS_PER_WIDE_INT)
+	{
+	  hi = 0;
+	  lo = ((unsigned HOST_WIDE_INT) 1 << (prec - 1)) - 1;
+	}
+      else
+	{
+	  hi = ((HOST_WIDE_INT) 1 << (prec - HOST_BITS_PER_WIDE_INT - 1)) - 1;
+	  lo = (unsigned HOST_WIDE_INT) -1;
+	}
+
+      if (TREE_INT_CST_HIGH (high) == hi && TREE_INT_CST_LOW (high) == lo)
+	{
+	  if (TYPE_UNSIGNED (etype))
+	    {
+	      etype = lang_hooks.types.signed_type (etype);
+	      exp = fold_convert (etype, exp);
+	    }
+	  return fold_build2 (GT_EXPR, type, exp,
+			      fold_convert (etype, integer_zero_node));
+	}
+    }
+
+  value = const_binop (MINUS_EXPR, high, low, 0);
+  if (value != 0 && (!flag_wrapv || TREE_OVERFLOW (value))
+      && ! TYPE_UNSIGNED (etype))
+    {
+      tree utype, minv, maxv;
+
+      /* Check if (unsigned) INT_MAX + 1 == (unsigned) INT_MIN
+	 for the type in question, as we rely on this here.  */
+      switch (TREE_CODE (etype))
+	{
+	case INTEGER_TYPE:
+	case ENUMERAL_TYPE:
+	case CHAR_TYPE:
+	  /* There is no requirement that LOW be within the range of ETYPE
+	     if the latter is a subtype.  It must, however, be within the base
+	     type of ETYPE.  So be sure we do the subtraction in that type.  */
+	  if (TREE_TYPE (etype))
+	    etype = TREE_TYPE (etype);
+	  utype = lang_hooks.types.unsigned_type (etype);
+	  maxv = fold_convert (utype, TYPE_MAX_VALUE (etype));
+	  maxv = range_binop (PLUS_EXPR, NULL_TREE, maxv, 1,
+			      integer_one_node, 1);
+	  minv = fold_convert (utype, TYPE_MIN_VALUE (etype));
+	  if (integer_zerop (range_binop (NE_EXPR, integer_type_node,
+					  minv, 1, maxv, 1)))
+	    {
+	      etype = utype;
+	      high = fold_convert (etype, high);
+	      low = fold_convert (etype, low);
+	      exp = fold_convert (etype, exp);
+	      value = const_binop (MINUS_EXPR, high, low, 0);
+	    }
+	  break;
+	default:
+	  break;
+	}
+    }
+
+  if (value != 0 && ! TREE_OVERFLOW (value))
+    {
+      /* There is no requirement that LOW be within the range of ETYPE
+	 if the latter is a subtype.  It must, however, be within the base
+	 type of ETYPE.  So be sure we do the subtraction in that type.  */
+      if (INTEGRAL_TYPE_P (etype) && TREE_TYPE (etype))
+	{
+	  etype = TREE_TYPE (etype);
+	  exp = fold_convert (etype, exp);
+	  low = fold_convert (etype, low);
+	  value = fold_convert (etype, value);
+	}
+
+      return build_range_check (type,
+				fold_build2 (MINUS_EXPR, etype, exp, low),
+				1, build_int_cst (etype, 0), value);
+    }
+
+  return 0;
+}
+
+/* Given two ranges, see if we can merge them into one.  Return 1 if we
+   can, 0 if we can't.  Set the output range into the specified parameters.  */
+
+static int
+merge_ranges (int *pin_p, tree *plow, tree *phigh, int in0_p, tree low0,
+	      tree high0, int in1_p, tree low1, tree high1)
+{
+  int no_overlap;
+  int subset;
+  int temp;
+  tree tem;
+  int in_p;
+  tree low, high;
+  int lowequal = ((low0 == 0 && low1 == 0)
+		  || integer_onep (range_binop (EQ_EXPR, integer_type_node,
+						low0, 0, low1, 0)));
+  int highequal = ((high0 == 0 && high1 == 0)
+		   || integer_onep (range_binop (EQ_EXPR, integer_type_node,
+						 high0, 1, high1, 1)));
+
+  /* Make range 0 be the range that starts first, or ends last if they
+     start at the same value.  Swap them if it isn't.  */
+  if (integer_onep (range_binop (GT_EXPR, integer_type_node,
+				 low0, 0, low1, 0))
+      || (lowequal
+	  && integer_onep (range_binop (GT_EXPR, integer_type_node,
+					high1, 1, high0, 1))))
+    {
+      temp = in0_p, in0_p = in1_p, in1_p = temp;
+      tem = low0, low0 = low1, low1 = tem;
+      tem = high0, high0 = high1, high1 = tem;
+    }
+
+  /* Now flag two cases, whether the ranges are disjoint or whether the
+     second range is totally subsumed in the first.  Note that the tests
+     below are simplified by the ones above.  */
+  no_overlap = integer_onep (range_binop (LT_EXPR, integer_type_node,
+					  high0, 1, low1, 0));
+  subset = integer_onep (range_binop (LE_EXPR, integer_type_node,
+				      high1, 1, high0, 1));
+
+  /* We now have four cases, depending on whether we are including or
+     excluding the two ranges.  */
+  if (in0_p && in1_p)
+    {
+      /* If they don't overlap, the result is false.  If the second range
+	 is a subset it is the result.  Otherwise, the range is from the start
+	 of the second to the end of the first.  */
+      if (no_overlap)
+	in_p = 0, low = high = 0;
+      else if (subset)
+	in_p = 1, low = low1, high = high1;
+      else
+	in_p = 1, low = low1, high = high0;
+    }
+
+  else if (in0_p && ! in1_p)
+    {
+      /* If they don't overlap, the result is the first range.  If they are
+	 equal, the result is false.  If the second range is a subset of the
+	 first, and the ranges begin at the same place, we go from just after
+	 the end of the first range to the end of the second.  If the second
+	 range is not a subset of the first, or if it is a subset and both
+	 ranges end at the same place, the range starts at the start of the
+	 first range and ends just before the second range.
+	 Otherwise, we can't describe this as a single range.  */
+      if (no_overlap)
+	in_p = 1, low = low0, high = high0;
+      else if (lowequal && highequal)
+	in_p = 0, low = high = 0;
+      else if (subset && lowequal)
+	{
+	  in_p = 1, high = high0;
+	  low = range_binop (PLUS_EXPR, NULL_TREE, high1, 0,
+			     integer_one_node, 0);
+	}
+      else if (! subset || highequal)
+	{
+	  in_p = 1, low = low0;
+	  high = range_binop (MINUS_EXPR, NULL_TREE, low1, 0,
+			      integer_one_node, 0);
+	}
+      else
+	return 0;
+    }
+
+  else if (! in0_p && in1_p)
+    {
+      /* If they don't overlap, the result is the second range.  If the second
+	 is a subset of the first, the result is false.  Otherwise,
+	 the range starts just after the first range and ends at the
+	 end of the second.  */
+      if (no_overlap)
+	in_p = 1, low = low1, high = high1;
+      else if (subset || highequal)
+	in_p = 0, low = high = 0;
+      else
+	{
+	  in_p = 1, high = high1;
+	  low = range_binop (PLUS_EXPR, NULL_TREE, high0, 1,
+			     integer_one_node, 0);
+	}
+    }
+
+  else
+    {
+      /* The case where we are excluding both ranges.  Here the complex case
+	 is if they don't overlap.  In that case, the only time we have a
+	 range is if they are adjacent.  If the second is a subset of the
+	 first, the result is the first.  Otherwise, the range to exclude
+	 starts at the beginning of the first range and ends at the end of the
+	 second.  */
+      if (no_overlap)
+	{
+	  if (integer_onep (range_binop (EQ_EXPR, integer_type_node,
+					 range_binop (PLUS_EXPR, NULL_TREE,
+						      high0, 1,
+						      integer_one_node, 1),
+					 1, low1, 0)))
+	    in_p = 0, low = low0, high = high1;
+	  else
+	    {
+	      /* Canonicalize - [min, x] into - [-, x].  */
+	      if (low0 && TREE_CODE (low0) == INTEGER_CST)
+		switch (TREE_CODE (TREE_TYPE (low0)))
+		  {
+		  case ENUMERAL_TYPE:
+		    if (TYPE_PRECISION (TREE_TYPE (low0))
+			!= GET_MODE_BITSIZE (TYPE_MODE (TREE_TYPE (low0))))
+		      break;
+		    /* FALLTHROUGH */
+		  case INTEGER_TYPE:
+		  case CHAR_TYPE:
+		    if (tree_int_cst_equal (low0,
+					    TYPE_MIN_VALUE (TREE_TYPE (low0))))
+		      low0 = 0;
+		    break;
+		  case POINTER_TYPE:
+		    if (TYPE_UNSIGNED (TREE_TYPE (low0))
+			&& integer_zerop (low0))
+		      low0 = 0;
+		    break;
+		  default:
+		    break;
+		  }
+
+	      /* Canonicalize - [x, max] into - [x, -].  */
+	      if (high1 && TREE_CODE (high1) == INTEGER_CST)
+		switch (TREE_CODE (TREE_TYPE (high1)))
+		  {
+		  case ENUMERAL_TYPE:
+		    if (TYPE_PRECISION (TREE_TYPE (high1))
+			!= GET_MODE_BITSIZE (TYPE_MODE (TREE_TYPE (high1))))
+		      break;
+		    /* FALLTHROUGH */
+		  case INTEGER_TYPE:
+		  case CHAR_TYPE:
+		    if (tree_int_cst_equal (high1,
+					    TYPE_MAX_VALUE (TREE_TYPE (high1))))
+		      high1 = 0;
+		    break;
+		  case POINTER_TYPE:
+		    if (TYPE_UNSIGNED (TREE_TYPE (high1))
+			&& integer_zerop (range_binop (PLUS_EXPR, NULL_TREE,
+						       high1, 1,
+						       integer_one_node, 1)))
+		      high1 = 0;
+		    break;
+		  default:
+		    break;
+		  }
+
+	      /* The ranges might be also adjacent between the maximum and
+	         minimum values of the given type.  For
+	         - [{min,-}, x] and - [y, {max,-}] ranges where x + 1 < y
+	         return + [x + 1, y - 1].  */
+	      if (low0 == 0 && high1 == 0)
+	        {
+		  low = range_binop (PLUS_EXPR, NULL_TREE, high0, 1,
+				     integer_one_node, 1);
+		  high = range_binop (MINUS_EXPR, NULL_TREE, low1, 0,
+				      integer_one_node, 0);
+		  if (low == 0 || high == 0)
+		    return 0;
+
+		  in_p = 1;
+		}
+	      else
+		return 0;
+	    }
+	}
+      else if (subset)
+	in_p = 0, low = low0, high = high0;
+      else
+	in_p = 0, low = low0, high = high1;
+    }
+
+  *pin_p = in_p, *plow = low, *phigh = high;
+  return 1;
+}
+
+
+/* Subroutine of fold, looking inside expressions of the form
+   A op B ? A : C, where ARG0, ARG1 and ARG2 are the three operands
+   of the COND_EXPR.  This function is being used also to optimize
+   A op B ? C : A, by reversing the comparison first.
+
+   Return a folded expression whose code is not a COND_EXPR
+   anymore, or NULL_TREE if no folding opportunity is found.  */
+
+static tree
+fold_cond_expr_with_comparison (tree type, tree arg0, tree arg1, tree arg2)
+{
+  enum tree_code comp_code = TREE_CODE (arg0);
+  tree arg00 = TREE_OPERAND (arg0, 0);
+  tree arg01 = TREE_OPERAND (arg0, 1);
+  tree arg1_type = TREE_TYPE (arg1);
+  tree tem;
+
+  STRIP_NOPS (arg1);
+  STRIP_NOPS (arg2);
+
+  /* If we have A op 0 ? A : -A, consider applying the following
+     transformations:
+
+     A == 0? A : -A    same as -A
+     A != 0? A : -A    same as A
+     A >= 0? A : -A    same as abs (A)
+     A > 0?  A : -A    same as abs (A)
+     A <= 0? A : -A    same as -abs (A)
+     A < 0?  A : -A    same as -abs (A)
+
+     None of these transformations work for modes with signed
+     zeros.  If A is +/-0, the first two transformations will
+     change the sign of the result (from +0 to -0, or vice
+     versa).  The last four will fix the sign of the result,
+     even though the original expressions could be positive or
+     negative, depending on the sign of A.
+
+     Note that all these transformations are correct if A is
+     NaN, since the two alternatives (A and -A) are also NaNs.  */
+  if ((FLOAT_TYPE_P (TREE_TYPE (arg01))
+       ? real_zerop (arg01)
+       : integer_zerop (arg01))
+      && ((TREE_CODE (arg2) == NEGATE_EXPR
+	   && operand_equal_p (TREE_OPERAND (arg2, 0), arg1, 0))
+	     /* In the case that A is of the form X-Y, '-A' (arg2) may
+	        have already been folded to Y-X, check for that. */
+	  || (TREE_CODE (arg1) == MINUS_EXPR
+	      && TREE_CODE (arg2) == MINUS_EXPR
+	      && operand_equal_p (TREE_OPERAND (arg1, 0),
+				  TREE_OPERAND (arg2, 1), 0)
+	      && operand_equal_p (TREE_OPERAND (arg1, 1),
+				  TREE_OPERAND (arg2, 0), 0))))
+    switch (comp_code)
+      {
+      case EQ_EXPR:
+      case UNEQ_EXPR:
+	tem = fold_convert (arg1_type, arg1);
+	return pedantic_non_lvalue (fold_convert (type, negate_expr (tem)));
+      case NE_EXPR:
+      case LTGT_EXPR:
+	return pedantic_non_lvalue (fold_convert (type, arg1));
+      case UNGE_EXPR:
+      case UNGT_EXPR:
+	if (flag_trapping_math)
+	  break;
+	/* Fall through.  */
+      case GE_EXPR:
+      case GT_EXPR:
+	if (TYPE_UNSIGNED (TREE_TYPE (arg1)))
+	  arg1 = fold_convert (lang_hooks.types.signed_type
+			       (TREE_TYPE (arg1)), arg1);
+	tem = fold_build1 (ABS_EXPR, TREE_TYPE (arg1), arg1);
+	return pedantic_non_lvalue (fold_convert (type, tem));
+      case UNLE_EXPR:
+      case UNLT_EXPR:
+	if (flag_trapping_math)
+	  break;
+      case LE_EXPR:
+      case LT_EXPR:
+	if (TYPE_UNSIGNED (TREE_TYPE (arg1)))
+	  arg1 = fold_convert (lang_hooks.types.signed_type
+			       (TREE_TYPE (arg1)), arg1);
+	tem = fold_build1 (ABS_EXPR, TREE_TYPE (arg1), arg1);
+	return negate_expr (fold_convert (type, tem));
+      default:
+	gcc_assert (TREE_CODE_CLASS (comp_code) == tcc_comparison);
+	break;
+      }
+
+  /* A != 0 ? A : 0 is simply A, unless A is -0.  Likewise
+     A == 0 ? A : 0 is always 0 unless A is -0.  Note that
+     both transformations are correct when A is NaN: A != 0
+     is then true, and A == 0 is false.  */
+
+  if (integer_zerop (arg01) && integer_zerop (arg2))
+    {
+      if (comp_code == NE_EXPR)
+	return pedantic_non_lvalue (fold_convert (type, arg1));
+      else if (comp_code == EQ_EXPR)
+	return fold_convert (type, integer_zero_node);
+    }
+
+  /* Try some transformations of A op B ? A : B.
+
+     A == B? A : B    same as B
+     A != B? A : B    same as A
+     A >= B? A : B    same as max (A, B)
+     A > B?  A : B    same as max (B, A)
+     A <= B? A : B    same as min (A, B)
+     A < B?  A : B    same as min (B, A)
+
+     As above, these transformations don't work in the presence
+     of signed zeros.  For example, if A and B are zeros of
+     opposite sign, the first two transformations will change
+     the sign of the result.  In the last four, the original
+     expressions give different results for (A=+0, B=-0) and
+     (A=-0, B=+0), but the transformed expressions do not.
+
+     The first two transformations are correct if either A or B
+     is a NaN.  In the first transformation, the condition will
+     be false, and B will indeed be chosen.  In the case of the
+     second transformation, the condition A != B will be true,
+     and A will be chosen.
+
+     The conversions to max() and min() are not correct if B is
+     a number and A is not.  The conditions in the original
+     expressions will be false, so all four give B.  The min()
+     and max() versions would give a NaN instead.  */
+  if (operand_equal_for_comparison_p (arg01, arg2, arg00)
+      /* Avoid these transformations if the COND_EXPR may be used
+	 as an lvalue in the C++ front-end.  PR c++/19199.  */
+      && (in_gimple_form
+	  || strcmp (lang_hooks.name, "GNU C++") != 0
+	  || ! maybe_lvalue_p (arg1)
+	  || ! maybe_lvalue_p (arg2)))
+    {
+      tree comp_op0 = arg00;
+      tree comp_op1 = arg01;
+      tree comp_type = TREE_TYPE (comp_op0);
+
+      /* Avoid adding NOP_EXPRs in case this is an lvalue.  */
+      if (TYPE_MAIN_VARIANT (comp_type) == TYPE_MAIN_VARIANT (type))
+	{
+	  comp_type = type;
+	  comp_op0 = arg1;
+	  comp_op1 = arg2;
+	}
+
+      switch (comp_code)
+	{
+	case EQ_EXPR:
+	  return pedantic_non_lvalue (fold_convert (type, arg2));
+	case NE_EXPR:
+	  return pedantic_non_lvalue (fold_convert (type, arg1));
+	case LE_EXPR:
+	case LT_EXPR:
+	case UNLE_EXPR:
+	case UNLT_EXPR:
+	  /* In C++ a ?: expression can be an lvalue, so put the
+	     operand which will be used if they are equal first
+	     so that we can convert this back to the
+	     corresponding COND_EXPR.  */
+	  if (!HONOR_NANS (TYPE_MODE (TREE_TYPE (arg1))))
+	    {
+	      comp_op0 = fold_convert (comp_type, comp_op0);
+	      comp_op1 = fold_convert (comp_type, comp_op1);
+	      tem = (comp_code == LE_EXPR || comp_code == UNLE_EXPR)
+		    ? fold_build2 (MIN_EXPR, comp_type, comp_op0, comp_op1)
+		    : fold_build2 (MIN_EXPR, comp_type, comp_op1, comp_op0);
+	      return pedantic_non_lvalue (fold_convert (type, tem));
+	    }
+	  break;
+	case GE_EXPR:
+	case GT_EXPR:
+	case UNGE_EXPR:
+	case UNGT_EXPR:
+	  if (!HONOR_NANS (TYPE_MODE (TREE_TYPE (arg1))))
+	    {
+	      comp_op0 = fold_convert (comp_type, comp_op0);
+	      comp_op1 = fold_convert (comp_type, comp_op1);
+	      tem = (comp_code == GE_EXPR || comp_code == UNGE_EXPR)
+		    ? fold_build2 (MAX_EXPR, comp_type, comp_op0, comp_op1)
+		    : fold_build2 (MAX_EXPR, comp_type, comp_op1, comp_op0);
+	      return pedantic_non_lvalue (fold_convert (type, tem));
+	    }
+	  break;
+	case UNEQ_EXPR:
+	  if (!HONOR_NANS (TYPE_MODE (TREE_TYPE (arg1))))
+	    return pedantic_non_lvalue (fold_convert (type, arg2));
+	  break;
+	case LTGT_EXPR:
+	  if (!HONOR_NANS (TYPE_MODE (TREE_TYPE (arg1))))
+	    return pedantic_non_lvalue (fold_convert (type, arg1));
+	  break;
+	default:
+	  gcc_assert (TREE_CODE_CLASS (comp_code) == tcc_comparison);
+	  break;
+	}
+    }
+
+  /* If this is A op C1 ? A : C2 with C1 and C2 constant integers,
+     we might still be able to simplify this.  For example,
+     if C1 is one less or one more than C2, this might have started
+     out as a MIN or MAX and been transformed by this function.
+     Only good for INTEGER_TYPEs, because we need TYPE_MAX_VALUE.  */
+
+  if (INTEGRAL_TYPE_P (type)
+      && TREE_CODE (arg01) == INTEGER_CST
+      && TREE_CODE (arg2) == INTEGER_CST)
+    switch (comp_code)
+      {
+      case EQ_EXPR:
+	/* We can replace A with C1 in this case.  */
+	arg1 = fold_convert (type, arg01);
+	return fold_build3 (COND_EXPR, type, arg0, arg1, arg2);
+
+      case LT_EXPR:
+	/* If C1 is C2 + 1, this is min(A, C2).  */
+	if (! operand_equal_p (arg2, TYPE_MAX_VALUE (type),
+			       OEP_ONLY_CONST)
+	    && operand_equal_p (arg01,
+				const_binop (PLUS_EXPR, arg2,
+					     integer_one_node, 0),
+				OEP_ONLY_CONST))
+	  return pedantic_non_lvalue (fold_build2 (MIN_EXPR,
+						   type, arg1, arg2));
+	break;
+
+      case LE_EXPR:
+	/* If C1 is C2 - 1, this is min(A, C2).  */
+	if (! operand_equal_p (arg2, TYPE_MIN_VALUE (type),
+			       OEP_ONLY_CONST)
+	    && operand_equal_p (arg01,
+				const_binop (MINUS_EXPR, arg2,
+					     integer_one_node, 0),
+				OEP_ONLY_CONST))
+	  return pedantic_non_lvalue (fold_build2 (MIN_EXPR,
+						   type, arg1, arg2));
+	break;
+
+      case GT_EXPR:
+	/* If C1 is C2 - 1, this is max(A, C2).  */
+	if (! operand_equal_p (arg2, TYPE_MIN_VALUE (type),
+			       OEP_ONLY_CONST)
+	    && operand_equal_p (arg01,
+				const_binop (MINUS_EXPR, arg2,
+					     integer_one_node, 0),
+				OEP_ONLY_CONST))
+	  return pedantic_non_lvalue (fold_build2 (MAX_EXPR,
+						   type, arg1, arg2));
+	break;
+
+      case GE_EXPR:
+	/* If C1 is C2 + 1, this is max(A, C2).  */
+	if (! operand_equal_p (arg2, TYPE_MAX_VALUE (type),
+			       OEP_ONLY_CONST)
+	    && operand_equal_p (arg01,
+				const_binop (PLUS_EXPR, arg2,
+					     integer_one_node, 0),
+				OEP_ONLY_CONST))
+	  return pedantic_non_lvalue (fold_build2 (MAX_EXPR,
+						   type, arg1, arg2));
+	break;
+      case NE_EXPR:
+	break;
+      default:
+	gcc_unreachable ();
+      }
+
+  return NULL_TREE;
+}
+
+
+
+#ifndef LOGICAL_OP_NON_SHORT_CIRCUIT
+#define LOGICAL_OP_NON_SHORT_CIRCUIT (BRANCH_COST >= 2)
+#endif
+
+/* EXP is some logical combination of boolean tests.  See if we can
+   merge it into some range test.  Return the new tree if so.  */
+
+static tree
+fold_range_test (enum tree_code code, tree type, tree op0, tree op1)
+{
+  int or_op = (code == TRUTH_ORIF_EXPR
+	       || code == TRUTH_OR_EXPR);
+  int in0_p, in1_p, in_p;
+  tree low0, low1, low, high0, high1, high;
+  tree lhs = make_range (op0, &in0_p, &low0, &high0);
+  tree rhs = make_range (op1, &in1_p, &low1, &high1);
+  tree tem;
+
+  /* If this is an OR operation, invert both sides; we will invert
+     again at the end.  */
+  if (or_op)
+    in0_p = ! in0_p, in1_p = ! in1_p;
+
+  /* If both expressions are the same, if we can merge the ranges, and we
+     can build the range test, return it or it inverted.  If one of the
+     ranges is always true or always false, consider it to be the same
+     expression as the other.  */
+  if ((lhs == 0 || rhs == 0 || operand_equal_p (lhs, rhs, 0))
+      && merge_ranges (&in_p, &low, &high, in0_p, low0, high0,
+		       in1_p, low1, high1)
+      && 0 != (tem = (build_range_check (type,
+					 lhs != 0 ? lhs
+					 : rhs != 0 ? rhs : integer_zero_node,
+					 in_p, low, high))))
+    return or_op ? invert_truthvalue (tem) : tem;
+
+  /* On machines where the branch cost is expensive, if this is a
+     short-circuited branch and the underlying object on both sides
+     is the same, make a non-short-circuit operation.  */
+  else if (LOGICAL_OP_NON_SHORT_CIRCUIT
+	   && lhs != 0 && rhs != 0
+	   && (code == TRUTH_ANDIF_EXPR
+	       || code == TRUTH_ORIF_EXPR)
+	   && operand_equal_p (lhs, rhs, 0))
+    {
+      /* If simple enough, just rewrite.  Otherwise, make a SAVE_EXPR
+	 unless we are at top level or LHS contains a PLACEHOLDER_EXPR, in
+	 which cases we can't do this.  */
+      if (simple_operand_p (lhs))
+	return build2 (code == TRUTH_ANDIF_EXPR
+		       ? TRUTH_AND_EXPR : TRUTH_OR_EXPR,
+		       type, op0, op1);
+
+      else if (lang_hooks.decls.global_bindings_p () == 0
+	       && ! CONTAINS_PLACEHOLDER_P (lhs))
+	{
+	  tree common = save_expr (lhs);
+
+	  if (0 != (lhs = build_range_check (type, common,
+					     or_op ? ! in0_p : in0_p,
+					     low0, high0))
+	      && (0 != (rhs = build_range_check (type, common,
+						 or_op ? ! in1_p : in1_p,
+						 low1, high1))))
+	    return build2 (code == TRUTH_ANDIF_EXPR
+			   ? TRUTH_AND_EXPR : TRUTH_OR_EXPR,
+			   type, lhs, rhs);
+	}
+    }
+
+  return 0;
+}
+
+/* Subroutine for fold_truthop: C is an INTEGER_CST interpreted as a P
+   bit value.  Arrange things so the extra bits will be set to zero if and
+   only if C is signed-extended to its full width.  If MASK is nonzero,
+   it is an INTEGER_CST that should be AND'ed with the extra bits.  */
+
+static tree
+unextend (tree c, int p, int unsignedp, tree mask)
+{
+  tree type = TREE_TYPE (c);
+  int modesize = GET_MODE_BITSIZE (TYPE_MODE (type));
+  tree temp;
+
+  if (p == modesize || unsignedp)
+    return c;
+
+  /* We work by getting just the sign bit into the low-order bit, then
+     into the high-order bit, then sign-extend.  We then XOR that value
+     with C.  */
+  temp = const_binop (RSHIFT_EXPR, c, size_int (p - 1), 0);
+  temp = const_binop (BIT_AND_EXPR, temp, size_int (1), 0);
+
+  /* We must use a signed type in order to get an arithmetic right shift.
+     However, we must also avoid introducing accidental overflows, so that
+     a subsequent call to integer_zerop will work.  Hence we must
+     do the type conversion here.  At this point, the constant is either
+     zero or one, and the conversion to a signed type can never overflow.
+     We could get an overflow if this conversion is done anywhere else.  */
+  if (TYPE_UNSIGNED (type))
+    temp = fold_convert (lang_hooks.types.signed_type (type), temp);
+
+  temp = const_binop (LSHIFT_EXPR, temp, size_int (modesize - 1), 0);
+  temp = const_binop (RSHIFT_EXPR, temp, size_int (modesize - p - 1), 0);
+  if (mask != 0)
+    temp = const_binop (BIT_AND_EXPR, temp,
+			fold_convert (TREE_TYPE (c), mask), 0);
+  /* If necessary, convert the type back to match the type of C.  */
+  if (TYPE_UNSIGNED (type))
+    temp = fold_convert (type, temp);
+
+  return fold_convert (type, const_binop (BIT_XOR_EXPR, c, temp, 0));
+}
+
+/* Find ways of folding logical expressions of LHS and RHS:
+   Try to merge two comparisons to the same innermost item.
+   Look for range tests like "ch >= '0' && ch <= '9'".
+   Look for combinations of simple terms on machines with expensive branches
+   and evaluate the RHS unconditionally.
+
+   For example, if we have p->a == 2 && p->b == 4 and we can make an
+   object large enough to span both A and B, we can do this with a comparison
+   against the object ANDed with the a mask.
+
+   If we have p->a == q->a && p->b == q->b, we may be able to use bit masking
+   operations to do this with one comparison.
+
+   We check for both normal comparisons and the BIT_AND_EXPRs made this by
+   function and the one above.
+
+   CODE is the logical operation being done.  It can be TRUTH_ANDIF_EXPR,
+   TRUTH_AND_EXPR, TRUTH_ORIF_EXPR, or TRUTH_OR_EXPR.
+
+   TRUTH_TYPE is the type of the logical operand and LHS and RHS are its
+   two operands.
+
+   We return the simplified tree or 0 if no optimization is possible.  */
+
+static tree
+fold_truthop (enum tree_code code, tree truth_type, tree lhs, tree rhs)
+{
+  /* If this is the "or" of two comparisons, we can do something if
+     the comparisons are NE_EXPR.  If this is the "and", we can do something
+     if the comparisons are EQ_EXPR.  I.e.,
+	(a->b == 2 && a->c == 4) can become (a->new == NEW).
+
+     WANTED_CODE is this operation code.  For single bit fields, we can
+     convert EQ_EXPR to NE_EXPR so we need not reject the "wrong"
+     comparison for one-bit fields.  */
+
+  enum tree_code wanted_code;
+  enum tree_code lcode, rcode;
+  tree ll_arg, lr_arg, rl_arg, rr_arg;
+  tree ll_inner, lr_inner, rl_inner, rr_inner;
+  HOST_WIDE_INT ll_bitsize, ll_bitpos, lr_bitsize, lr_bitpos;
+  HOST_WIDE_INT rl_bitsize, rl_bitpos, rr_bitsize, rr_bitpos;
+  HOST_WIDE_INT xll_bitpos, xlr_bitpos, xrl_bitpos, xrr_bitpos;
+  HOST_WIDE_INT lnbitsize, lnbitpos, rnbitsize, rnbitpos;
+  int ll_unsignedp, lr_unsignedp, rl_unsignedp, rr_unsignedp;
+  enum machine_mode ll_mode, lr_mode, rl_mode, rr_mode;
+  enum machine_mode lnmode, rnmode;
+  tree ll_mask, lr_mask, rl_mask, rr_mask;
+  tree ll_and_mask, lr_and_mask, rl_and_mask, rr_and_mask;
+  tree l_const, r_const;
+  tree lntype, rntype, result;
+  int first_bit, end_bit;
+  int volatilep;
+
+  /* Start by getting the comparison codes.  Fail if anything is volatile.
+     If one operand is a BIT_AND_EXPR with the constant one, treat it as if
+     it were surrounded with a NE_EXPR.  */
+
+  if (TREE_SIDE_EFFECTS (lhs) || TREE_SIDE_EFFECTS (rhs))
+    return 0;
+
+  lcode = TREE_CODE (lhs);
+  rcode = TREE_CODE (rhs);
+
+  if (lcode == BIT_AND_EXPR && integer_onep (TREE_OPERAND (lhs, 1)))
+    {
+      lhs = build2 (NE_EXPR, truth_type, lhs,
+		    fold_convert (TREE_TYPE (lhs), integer_zero_node));
+      lcode = NE_EXPR;
+    }
+
+  if (rcode == BIT_AND_EXPR && integer_onep (TREE_OPERAND (rhs, 1)))
+    {
+      rhs = build2 (NE_EXPR, truth_type, rhs,
+		    fold_convert (TREE_TYPE (rhs), integer_zero_node));
+      rcode = NE_EXPR;
+    }
+
+  if (TREE_CODE_CLASS (lcode) != tcc_comparison
+      || TREE_CODE_CLASS (rcode) != tcc_comparison)
+    return 0;
+
+  ll_arg = TREE_OPERAND (lhs, 0);
+  lr_arg = TREE_OPERAND (lhs, 1);
+  rl_arg = TREE_OPERAND (rhs, 0);
+  rr_arg = TREE_OPERAND (rhs, 1);
+
+  /* Simplify (x<y) && (x==y) into (x<=y) and related optimizations.  */
+  if (simple_operand_p (ll_arg)
+      && simple_operand_p (lr_arg))
+    {
+      tree result;
+      if (operand_equal_p (ll_arg, rl_arg, 0)
+          && operand_equal_p (lr_arg, rr_arg, 0))
+	{
+          result = combine_comparisons (code, lcode, rcode,
+					truth_type, ll_arg, lr_arg);
+	  if (result)
+	    return result;
+	}
+      else if (operand_equal_p (ll_arg, rr_arg, 0)
+               && operand_equal_p (lr_arg, rl_arg, 0))
+	{
+          result = combine_comparisons (code, lcode,
+					swap_tree_comparison (rcode),
+					truth_type, ll_arg, lr_arg);
+	  if (result)
+	    return result;
+	}
+    }
+
+  code = ((code == TRUTH_AND_EXPR || code == TRUTH_ANDIF_EXPR)
+	  ? TRUTH_AND_EXPR : TRUTH_OR_EXPR);
+
+  /* If the RHS can be evaluated unconditionally and its operands are
+     simple, it wins to evaluate the RHS unconditionally on machines
+     with expensive branches.  In this case, this isn't a comparison
+     that can be merged.  Avoid doing this if the RHS is a floating-point
+     comparison since those can trap.  */
+
+  if (BRANCH_COST >= 2
+      && ! FLOAT_TYPE_P (TREE_TYPE (rl_arg))
+      && simple_operand_p (rl_arg)
+      && simple_operand_p (rr_arg))
+    {
+      /* Convert (a != 0) || (b != 0) into (a | b) != 0.  */
+      if (code == TRUTH_OR_EXPR
+	  && lcode == NE_EXPR && integer_zerop (lr_arg)
+	  && rcode == NE_EXPR && integer_zerop (rr_arg)
+	  && TREE_TYPE (ll_arg) == TREE_TYPE (rl_arg))
+	return build2 (NE_EXPR, truth_type,
+		       build2 (BIT_IOR_EXPR, TREE_TYPE (ll_arg),
+			       ll_arg, rl_arg),
+		       fold_convert (TREE_TYPE (ll_arg), integer_zero_node));
+
+      /* Convert (a == 0) && (b == 0) into (a | b) == 0.  */
+      if (code == TRUTH_AND_EXPR
+	  && lcode == EQ_EXPR && integer_zerop (lr_arg)
+	  && rcode == EQ_EXPR && integer_zerop (rr_arg)
+	  && TREE_TYPE (ll_arg) == TREE_TYPE (rl_arg))
+	return build2 (EQ_EXPR, truth_type,
+		       build2 (BIT_IOR_EXPR, TREE_TYPE (ll_arg),
+			       ll_arg, rl_arg),
+		       fold_convert (TREE_TYPE (ll_arg), integer_zero_node));
+
+      if (LOGICAL_OP_NON_SHORT_CIRCUIT)
+	return build2 (code, truth_type, lhs, rhs);
+    }
+
+  /* See if the comparisons can be merged.  Then get all the parameters for
+     each side.  */
+
+  if ((lcode != EQ_EXPR && lcode != NE_EXPR)
+      || (rcode != EQ_EXPR && rcode != NE_EXPR))
+    return 0;
+
+  volatilep = 0;
+  ll_inner = decode_field_reference (ll_arg,
+				     &ll_bitsize, &ll_bitpos, &ll_mode,
+				     &ll_unsignedp, &volatilep, &ll_mask,
+				     &ll_and_mask);
+  lr_inner = decode_field_reference (lr_arg,
+				     &lr_bitsize, &lr_bitpos, &lr_mode,
+				     &lr_unsignedp, &volatilep, &lr_mask,
+				     &lr_and_mask);
+  rl_inner = decode_field_reference (rl_arg,
+				     &rl_bitsize, &rl_bitpos, &rl_mode,
+				     &rl_unsignedp, &volatilep, &rl_mask,
+				     &rl_and_mask);
+  rr_inner = decode_field_reference (rr_arg,
+				     &rr_bitsize, &rr_bitpos, &rr_mode,
+				     &rr_unsignedp, &volatilep, &rr_mask,
+				     &rr_and_mask);
+
+  /* It must be true that the inner operation on the lhs of each
+     comparison must be the same if we are to be able to do anything.
+     Then see if we have constants.  If not, the same must be true for
+     the rhs's.  */
+  if (volatilep || ll_inner == 0 || rl_inner == 0
+      || ! operand_equal_p (ll_inner, rl_inner, 0))
+    return 0;
+
+  if (TREE_CODE (lr_arg) == INTEGER_CST
+      && TREE_CODE (rr_arg) == INTEGER_CST)
+    l_const = lr_arg, r_const = rr_arg;
+  else if (lr_inner == 0 || rr_inner == 0
+	   || ! operand_equal_p (lr_inner, rr_inner, 0))
+    return 0;
+  else
+    l_const = r_const = 0;
+
+  /* If either comparison code is not correct for our logical operation,
+     fail.  However, we can convert a one-bit comparison against zero into
+     the opposite comparison against that bit being set in the field.  */
+
+  wanted_code = (code == TRUTH_AND_EXPR ? EQ_EXPR : NE_EXPR);
+  if (lcode != wanted_code)
+    {
+      if (l_const && integer_zerop (l_const) && integer_pow2p (ll_mask))
+	{
+	  /* Make the left operand unsigned, since we are only interested
+	     in the value of one bit.  Otherwise we are doing the wrong
+	     thing below.  */
+	  ll_unsignedp = 1;
+	  l_const = ll_mask;
+	}
+      else
+	return 0;
+    }
+
+  /* This is analogous to the code for l_const above.  */
+  if (rcode != wanted_code)
+    {
+      if (r_const && integer_zerop (r_const) && integer_pow2p (rl_mask))
+	{
+	  rl_unsignedp = 1;
+	  r_const = rl_mask;
+	}
+      else
+	return 0;
+    }
+
+  /* After this point all optimizations will generate bit-field
+     references, which we might not want.  */
+  if (! lang_hooks.can_use_bit_fields_p ())
+    return 0;
+
+  /* See if we can find a mode that contains both fields being compared on
+     the left.  If we can't, fail.  Otherwise, update all constants and masks
+     to be relative to a field of that size.  */
+  first_bit = MIN (ll_bitpos, rl_bitpos);
+  end_bit = MAX (ll_bitpos + ll_bitsize, rl_bitpos + rl_bitsize);
+  lnmode = get_best_mode (end_bit - first_bit, first_bit,
+			  TYPE_ALIGN (TREE_TYPE (ll_inner)), word_mode,
+			  volatilep);
+  if (lnmode == VOIDmode)
+    return 0;
+
+  lnbitsize = GET_MODE_BITSIZE (lnmode);
+  lnbitpos = first_bit & ~ (lnbitsize - 1);
+  lntype = lang_hooks.types.type_for_size (lnbitsize, 1);
+  xll_bitpos = ll_bitpos - lnbitpos, xrl_bitpos = rl_bitpos - lnbitpos;
+
+  if (BYTES_BIG_ENDIAN)
+    {
+      xll_bitpos = lnbitsize - xll_bitpos - ll_bitsize;
+      xrl_bitpos = lnbitsize - xrl_bitpos - rl_bitsize;
+    }
+
+  ll_mask = const_binop (LSHIFT_EXPR, fold_convert (lntype, ll_mask),
+			 size_int (xll_bitpos), 0);
+  rl_mask = const_binop (LSHIFT_EXPR, fold_convert (lntype, rl_mask),
+			 size_int (xrl_bitpos), 0);
+
+  if (l_const)
+    {
+      l_const = fold_convert (lntype, l_const);
+      l_const = unextend (l_const, ll_bitsize, ll_unsignedp, ll_and_mask);
+      l_const = const_binop (LSHIFT_EXPR, l_const, size_int (xll_bitpos), 0);
+      if (integer_nonzerop (const_binop (BIT_AND_EXPR, l_const,
+					 fold_build1 (BIT_NOT_EXPR,
+						      lntype, ll_mask),
+					 0)))
+	{
+	  warning (0, "comparison is always %d", wanted_code == NE_EXPR);
+
+	  return constant_boolean_node (wanted_code == NE_EXPR, truth_type);
+	}
+    }
+  if (r_const)
+    {
+      r_const = fold_convert (lntype, r_const);
+      r_const = unextend (r_const, rl_bitsize, rl_unsignedp, rl_and_mask);
+      r_const = const_binop (LSHIFT_EXPR, r_const, size_int (xrl_bitpos), 0);
+      if (integer_nonzerop (const_binop (BIT_AND_EXPR, r_const,
+					 fold_build1 (BIT_NOT_EXPR,
+						      lntype, rl_mask),
+					 0)))
+	{
+	  warning (0, "comparison is always %d", wanted_code == NE_EXPR);
+
+	  return constant_boolean_node (wanted_code == NE_EXPR, truth_type);
+	}
+    }
+
+  /* If the right sides are not constant, do the same for it.  Also,
+     disallow this optimization if a size or signedness mismatch occurs
+     between the left and right sides.  */
+  if (l_const == 0)
+    {
+      if (ll_bitsize != lr_bitsize || rl_bitsize != rr_bitsize
+	  || ll_unsignedp != lr_unsignedp || rl_unsignedp != rr_unsignedp
+	  /* Make sure the two fields on the right
+	     correspond to the left without being swapped.  */
+	  || ll_bitpos - rl_bitpos != lr_bitpos - rr_bitpos)
+	return 0;
+
+      first_bit = MIN (lr_bitpos, rr_bitpos);
+      end_bit = MAX (lr_bitpos + lr_bitsize, rr_bitpos + rr_bitsize);
+      rnmode = get_best_mode (end_bit - first_bit, first_bit,
+			      TYPE_ALIGN (TREE_TYPE (lr_inner)), word_mode,
+			      volatilep);
+      if (rnmode == VOIDmode)
+	return 0;
+
+      rnbitsize = GET_MODE_BITSIZE (rnmode);
+      rnbitpos = first_bit & ~ (rnbitsize - 1);
+      rntype = lang_hooks.types.type_for_size (rnbitsize, 1);
+      xlr_bitpos = lr_bitpos - rnbitpos, xrr_bitpos = rr_bitpos - rnbitpos;
+
+      if (BYTES_BIG_ENDIAN)
+	{
+	  xlr_bitpos = rnbitsize - xlr_bitpos - lr_bitsize;
+	  xrr_bitpos = rnbitsize - xrr_bitpos - rr_bitsize;
+	}
+
+      lr_mask = const_binop (LSHIFT_EXPR, fold_convert (rntype, lr_mask),
+			     size_int (xlr_bitpos), 0);
+      rr_mask = const_binop (LSHIFT_EXPR, fold_convert (rntype, rr_mask),
+			     size_int (xrr_bitpos), 0);
+
+      /* Make a mask that corresponds to both fields being compared.
+	 Do this for both items being compared.  If the operands are the
+	 same size and the bits being compared are in the same position
+	 then we can do this by masking both and comparing the masked
+	 results.  */
+      ll_mask = const_binop (BIT_IOR_EXPR, ll_mask, rl_mask, 0);
+      lr_mask = const_binop (BIT_IOR_EXPR, lr_mask, rr_mask, 0);
+      if (lnbitsize == rnbitsize && xll_bitpos == xlr_bitpos)
+	{
+	  lhs = make_bit_field_ref (ll_inner, lntype, lnbitsize, lnbitpos,
+				    ll_unsignedp || rl_unsignedp);
+	  if (! all_ones_mask_p (ll_mask, lnbitsize))
+	    lhs = build2 (BIT_AND_EXPR, lntype, lhs, ll_mask);
+
+	  rhs = make_bit_field_ref (lr_inner, rntype, rnbitsize, rnbitpos,
+				    lr_unsignedp || rr_unsignedp);
+	  if (! all_ones_mask_p (lr_mask, rnbitsize))
+	    rhs = build2 (BIT_AND_EXPR, rntype, rhs, lr_mask);
+
+	  return build2 (wanted_code, truth_type, lhs, rhs);
+	}
+
+      /* There is still another way we can do something:  If both pairs of
+	 fields being compared are adjacent, we may be able to make a wider
+	 field containing them both.
+
+	 Note that we still must mask the lhs/rhs expressions.  Furthermore,
+	 the mask must be shifted to account for the shift done by
+	 make_bit_field_ref.  */
+      if ((ll_bitsize + ll_bitpos == rl_bitpos
+	   && lr_bitsize + lr_bitpos == rr_bitpos)
+	  || (ll_bitpos == rl_bitpos + rl_bitsize
+	      && lr_bitpos == rr_bitpos + rr_bitsize))
+	{
+	  tree type;
+
+	  lhs = make_bit_field_ref (ll_inner, lntype, ll_bitsize + rl_bitsize,
+				    MIN (ll_bitpos, rl_bitpos), ll_unsignedp);
+	  rhs = make_bit_field_ref (lr_inner, rntype, lr_bitsize + rr_bitsize,
+				    MIN (lr_bitpos, rr_bitpos), lr_unsignedp);
+
+	  ll_mask = const_binop (RSHIFT_EXPR, ll_mask,
+				 size_int (MIN (xll_bitpos, xrl_bitpos)), 0);
+	  lr_mask = const_binop (RSHIFT_EXPR, lr_mask,
+				 size_int (MIN (xlr_bitpos, xrr_bitpos)), 0);
+
+	  /* Convert to the smaller type before masking out unwanted bits.  */
+	  type = lntype;
+	  if (lntype != rntype)
+	    {
+	      if (lnbitsize > rnbitsize)
+		{
+		  lhs = fold_convert (rntype, lhs);
+		  ll_mask = fold_convert (rntype, ll_mask);
+		  type = rntype;
+		}
+	      else if (lnbitsize < rnbitsize)
+		{
+		  rhs = fold_convert (lntype, rhs);
+		  lr_mask = fold_convert (lntype, lr_mask);
+		  type = lntype;
+		}
+	    }
+
+	  if (! all_ones_mask_p (ll_mask, ll_bitsize + rl_bitsize))
+	    lhs = build2 (BIT_AND_EXPR, type, lhs, ll_mask);
+
+	  if (! all_ones_mask_p (lr_mask, lr_bitsize + rr_bitsize))
+	    rhs = build2 (BIT_AND_EXPR, type, rhs, lr_mask);
+
+	  return build2 (wanted_code, truth_type, lhs, rhs);
+	}
+
+      return 0;
+    }
+
+  /* Handle the case of comparisons with constants.  If there is something in
+     common between the masks, those bits of the constants must be the same.
+     If not, the condition is always false.  Test for this to avoid generating
+     incorrect code below.  */
+  result = const_binop (BIT_AND_EXPR, ll_mask, rl_mask, 0);
+  if (! integer_zerop (result)
+      && simple_cst_equal (const_binop (BIT_AND_EXPR, result, l_const, 0),
+			   const_binop (BIT_AND_EXPR, result, r_const, 0)) != 1)
+    {
+      if (wanted_code == NE_EXPR)
+	{
+	  warning (0, "%<or%> of unmatched not-equal tests is always 1");
+	  return constant_boolean_node (true, truth_type);
+	}
+      else
+	{
+	  warning (0, "%<and%> of mutually exclusive equal-tests is always 0");
+	  return constant_boolean_node (false, truth_type);
+	}
+    }
+
+  /* Construct the expression we will return.  First get the component
+     reference we will make.  Unless the mask is all ones the width of
+     that field, perform the mask operation.  Then compare with the
+     merged constant.  */
+  result = make_bit_field_ref (ll_inner, lntype, lnbitsize, lnbitpos,
+			       ll_unsignedp || rl_unsignedp);
+
+  ll_mask = const_binop (BIT_IOR_EXPR, ll_mask, rl_mask, 0);
+  if (! all_ones_mask_p (ll_mask, lnbitsize))
+    result = build2 (BIT_AND_EXPR, lntype, result, ll_mask);
+
+  return build2 (wanted_code, truth_type, result,
+		 const_binop (BIT_IOR_EXPR, l_const, r_const, 0));
+}
+
+/* Optimize T, which is a comparison of a MIN_EXPR or MAX_EXPR with a
+   constant.  */
+
+static tree
+optimize_minmax_comparison (enum tree_code code, tree type, tree op0, tree op1)
+{
+  tree arg0 = op0;
+  enum tree_code op_code;
+  tree comp_const = op1;
+  tree minmax_const;
+  int consts_equal, consts_lt;
+  tree inner;
+
+  STRIP_SIGN_NOPS (arg0);
+
+  op_code = TREE_CODE (arg0);
+  minmax_const = TREE_OPERAND (arg0, 1);
+  consts_equal = tree_int_cst_equal (minmax_const, comp_const);
+  consts_lt = tree_int_cst_lt (minmax_const, comp_const);
+  inner = TREE_OPERAND (arg0, 0);
+
+  /* If something does not permit us to optimize, return the original tree.  */
+  if ((op_code != MIN_EXPR && op_code != MAX_EXPR)
+      || TREE_CODE (comp_const) != INTEGER_CST
+      || TREE_CONSTANT_OVERFLOW (comp_const)
+      || TREE_CODE (minmax_const) != INTEGER_CST
+      || TREE_CONSTANT_OVERFLOW (minmax_const))
+    return NULL_TREE;
+
+  /* Now handle all the various comparison codes.  We only handle EQ_EXPR
+     and GT_EXPR, doing the rest with recursive calls using logical
+     simplifications.  */
+  switch (code)
+    {
+    case NE_EXPR:  case LT_EXPR:  case LE_EXPR:
+      {
+	/* FIXME: We should be able to invert code without building a
+	   scratch tree node, but doing so would require us to
+	   duplicate a part of invert_truthvalue here.  */
+	tree tem = invert_truthvalue (build2 (code, type, op0, op1));
+	tem = optimize_minmax_comparison (TREE_CODE (tem),
+					  TREE_TYPE (tem),
+					  TREE_OPERAND (tem, 0),
+					  TREE_OPERAND (tem, 1));
+	return invert_truthvalue (tem);
+      }
+
+    case GE_EXPR:
+      return
+	fold_build2 (TRUTH_ORIF_EXPR, type,
+		     optimize_minmax_comparison
+		     (EQ_EXPR, type, arg0, comp_const),
+		     optimize_minmax_comparison
+		     (GT_EXPR, type, arg0, comp_const));
+
+    case EQ_EXPR:
+      if (op_code == MAX_EXPR && consts_equal)
+	/* MAX (X, 0) == 0  ->  X <= 0  */
+	return fold_build2 (LE_EXPR, type, inner, comp_const);
+
+      else if (op_code == MAX_EXPR && consts_lt)
+	/* MAX (X, 0) == 5  ->  X == 5   */
+	return fold_build2 (EQ_EXPR, type, inner, comp_const);
+
+      else if (op_code == MAX_EXPR)
+	/* MAX (X, 0) == -1  ->  false  */
+	return omit_one_operand (type, integer_zero_node, inner);
+
+      else if (consts_equal)
+	/* MIN (X, 0) == 0  ->  X >= 0  */
+	return fold_build2 (GE_EXPR, type, inner, comp_const);
+
+      else if (consts_lt)
+	/* MIN (X, 0) == 5  ->  false  */
+	return omit_one_operand (type, integer_zero_node, inner);
+
+      else
+	/* MIN (X, 0) == -1  ->  X == -1  */
+	return fold_build2 (EQ_EXPR, type, inner, comp_const);
+
+    case GT_EXPR:
+      if (op_code == MAX_EXPR && (consts_equal || consts_lt))
+	/* MAX (X, 0) > 0  ->  X > 0
+	   MAX (X, 0) > 5  ->  X > 5  */
+	return fold_build2 (GT_EXPR, type, inner, comp_const);
+
+      else if (op_code == MAX_EXPR)
+	/* MAX (X, 0) > -1  ->  true  */
+	return omit_one_operand (type, integer_one_node, inner);
+
+      else if (op_code == MIN_EXPR && (consts_equal || consts_lt))
+	/* MIN (X, 0) > 0  ->  false
+	   MIN (X, 0) > 5  ->  false  */
+	return omit_one_operand (type, integer_zero_node, inner);
+
+      else
+	/* MIN (X, 0) > -1  ->  X > -1  */
+	return fold_build2 (GT_EXPR, type, inner, comp_const);
+
+    default:
+      return NULL_TREE;
+    }
+}
+
+/* T is an integer expression that is being multiplied, divided, or taken a
+   modulus (CODE says which and what kind of divide or modulus) by a
+   constant C.  See if we can eliminate that operation by folding it with
+   other operations already in T.  WIDE_TYPE, if non-null, is a type that
+   should be used for the computation if wider than our type.
+
+   For example, if we are dividing (X * 8) + (Y * 16) by 4, we can return
+   (X * 2) + (Y * 4).  We must, however, be assured that either the original
+   expression would not overflow or that overflow is undefined for the type
+   in the language in question.
+
+   We also canonicalize (X + 7) * 4 into X * 4 + 28 in the hope that either
+   the machine has a multiply-accumulate insn or that this is part of an
+   addressing calculation.
+
+   If we return a non-null expression, it is an equivalent form of the
+   original computation, but need not be in the original type.  */
+
+static tree
+extract_muldiv (tree t, tree c, enum tree_code code, tree wide_type)
+{
+  /* To avoid exponential search depth, refuse to allow recursion past
+     three levels.  Beyond that (1) it's highly unlikely that we'll find
+     something interesting and (2) we've probably processed it before
+     when we built the inner expression.  */
+
+  static int depth;
+  tree ret;
+
+  if (depth > 3)
+    return NULL;
+
+  depth++;
+  ret = extract_muldiv_1 (t, c, code, wide_type);
+  depth--;
+
+  return ret;
+}
+
+static tree
+extract_muldiv_1 (tree t, tree c, enum tree_code code, tree wide_type)
+{
+  tree type = TREE_TYPE (t);
+  enum tree_code tcode = TREE_CODE (t);
+  tree ctype = (wide_type != 0 && (GET_MODE_SIZE (TYPE_MODE (wide_type))
+				   > GET_MODE_SIZE (TYPE_MODE (type)))
+		? wide_type : type);
+  tree t1, t2;
+  int same_p = tcode == code;
+  tree op0 = NULL_TREE, op1 = NULL_TREE;
+
+  /* Don't deal with constants of zero here; they confuse the code below.  */
+  if (integer_zerop (c))
+    return NULL_TREE;
+
+  if (TREE_CODE_CLASS (tcode) == tcc_unary)
+    op0 = TREE_OPERAND (t, 0);
+
+  if (TREE_CODE_CLASS (tcode) == tcc_binary)
+    op0 = TREE_OPERAND (t, 0), op1 = TREE_OPERAND (t, 1);
+
+  /* Note that we need not handle conditional operations here since fold
+     already handles those cases.  So just do arithmetic here.  */
+  switch (tcode)
+    {
+    case INTEGER_CST:
+      /* For a constant, we can always simplify if we are a multiply
+	 or (for divide and modulus) if it is a multiple of our constant.  */
+      if (code == MULT_EXPR
+	  || integer_zerop (const_binop (TRUNC_MOD_EXPR, t, c, 0)))
+	return const_binop (code, fold_convert (ctype, t),
+			    fold_convert (ctype, c), 0);
+      break;
+
+    case CONVERT_EXPR:  case NON_LVALUE_EXPR:  case NOP_EXPR:
+      /* If op0 is an expression ...  */
+      if ((COMPARISON_CLASS_P (op0)
+	   || UNARY_CLASS_P (op0)
+	   || BINARY_CLASS_P (op0)
+	   || EXPRESSION_CLASS_P (op0))
+	  /* ... and is unsigned, and its type is smaller than ctype,
+	     then we cannot pass through as widening.  */
+	  && ((TYPE_UNSIGNED (TREE_TYPE (op0))
+	       && ! (TREE_CODE (TREE_TYPE (op0)) == INTEGER_TYPE
+		     && TYPE_IS_SIZETYPE (TREE_TYPE (op0)))
+	       && (GET_MODE_SIZE (TYPE_MODE (ctype))
+	           > GET_MODE_SIZE (TYPE_MODE (TREE_TYPE (op0)))))
+	      /* ... or this is a truncation (t is narrower than op0),
+		 then we cannot pass through this narrowing.  */
+	      || (GET_MODE_SIZE (TYPE_MODE (type))
+		  < GET_MODE_SIZE (TYPE_MODE (TREE_TYPE (op0))))
+	      /* ... or signedness changes for division or modulus,
+		 then we cannot pass through this conversion.  */
+	      || (code != MULT_EXPR
+		  && (TYPE_UNSIGNED (ctype)
+		      != TYPE_UNSIGNED (TREE_TYPE (op0))))))
+	break;
+
+      /* Pass the constant down and see if we can make a simplification.  If
+	 we can, replace this expression with the inner simplification for
+	 possible later conversion to our or some other type.  */
+      if ((t2 = fold_convert (TREE_TYPE (op0), c)) != 0
+	  && TREE_CODE (t2) == INTEGER_CST
+	  && ! TREE_CONSTANT_OVERFLOW (t2)
+	  && (0 != (t1 = extract_muldiv (op0, t2, code,
+					 code == MULT_EXPR
+					 ? ctype : NULL_TREE))))
+	return t1;
+      break;
+
+    case ABS_EXPR:
+      /* If widening the type changes it from signed to unsigned, then we
+         must avoid building ABS_EXPR itself as unsigned.  */
+      if (TYPE_UNSIGNED (ctype) && !TYPE_UNSIGNED (type))
+        {
+          tree cstype = (*lang_hooks.types.signed_type) (ctype);
+          if ((t1 = extract_muldiv (op0, c, code, cstype)) != 0)
+            {
+              t1 = fold_build1 (tcode, cstype, fold_convert (cstype, t1));
+              return fold_convert (ctype, t1);
+            }
+          break;
+        }
+      /* FALLTHROUGH */
+    case NEGATE_EXPR:
+      if ((t1 = extract_muldiv (op0, c, code, wide_type)) != 0)
+	return fold_build1 (tcode, ctype, fold_convert (ctype, t1));
+      break;
+
+    case MIN_EXPR:  case MAX_EXPR:
+      /* If widening the type changes the signedness, then we can't perform
+	 this optimization as that changes the result.  */
+      if (TYPE_UNSIGNED (ctype) != TYPE_UNSIGNED (type))
+	break;
+
+      /* MIN (a, b) / 5 -> MIN (a / 5, b / 5)  */
+      if ((t1 = extract_muldiv (op0, c, code, wide_type)) != 0
+	  && (t2 = extract_muldiv (op1, c, code, wide_type)) != 0)
+	{
+	  if (tree_int_cst_sgn (c) < 0)
+	    tcode = (tcode == MIN_EXPR ? MAX_EXPR : MIN_EXPR);
+
+	  return fold_build2 (tcode, ctype, fold_convert (ctype, t1),
+			      fold_convert (ctype, t2));
+	}
+      break;
+
+    case LSHIFT_EXPR:  case RSHIFT_EXPR:
+      /* If the second operand is constant, this is a multiplication
+	 or floor division, by a power of two, so we can treat it that
+	 way unless the multiplier or divisor overflows.  Signed
+	 left-shift overflow is implementation-defined rather than
+	 undefined in C90, so do not convert signed left shift into
+	 multiplication.  */
+      if (TREE_CODE (op1) == INTEGER_CST
+	  && (tcode == RSHIFT_EXPR || TYPE_UNSIGNED (TREE_TYPE (op0)))
+	  /* const_binop may not detect overflow correctly,
+	     so check for it explicitly here.  */
+	  && TYPE_PRECISION (TREE_TYPE (size_one_node)) > TREE_INT_CST_LOW (op1)
+	  && TREE_INT_CST_HIGH (op1) == 0
+	  && 0 != (t1 = fold_convert (ctype,
+				      const_binop (LSHIFT_EXPR,
+						   size_one_node,
+						   op1, 0)))
+	  && ! TREE_OVERFLOW (t1))
+	return extract_muldiv (build2 (tcode == LSHIFT_EXPR
+				       ? MULT_EXPR : FLOOR_DIV_EXPR,
+				       ctype, fold_convert (ctype, op0), t1),
+			       c, code, wide_type);
+      break;
+
+    case PLUS_EXPR:  case MINUS_EXPR:
+      /* See if we can eliminate the operation on both sides.  If we can, we
+	 can return a new PLUS or MINUS.  If we can't, the only remaining
+	 cases where we can do anything are if the second operand is a
+	 constant.  */
+      t1 = extract_muldiv (op0, c, code, wide_type);
+      t2 = extract_muldiv (op1, c, code, wide_type);
+      if (t1 != 0 && t2 != 0
+	  && (code == MULT_EXPR
+	      /* If not multiplication, we can only do this if both operands
+		 are divisible by c.  */
+	      || (multiple_of_p (ctype, op0, c)
+	          && multiple_of_p (ctype, op1, c))))
+	return fold_build2 (tcode, ctype, fold_convert (ctype, t1),
+			    fold_convert (ctype, t2));
+
+      /* If this was a subtraction, negate OP1 and set it to be an addition.
+	 This simplifies the logic below.  */
+      if (tcode == MINUS_EXPR)
+	tcode = PLUS_EXPR, op1 = negate_expr (op1);
+
+      if (TREE_CODE (op1) != INTEGER_CST)
+	break;
+
+      /* If either OP1 or C are negative, this optimization is not safe for
+	 some of the division and remainder types while for others we need
+	 to change the code.  */
+      if (tree_int_cst_sgn (op1) < 0 || tree_int_cst_sgn (c) < 0)
+	{
+	  if (code == CEIL_DIV_EXPR)
+	    code = FLOOR_DIV_EXPR;
+	  else if (code == FLOOR_DIV_EXPR)
+	    code = CEIL_DIV_EXPR;
+	  else if (code != MULT_EXPR
+		   && code != CEIL_MOD_EXPR && code != FLOOR_MOD_EXPR)
+	    break;
+	}
+
+      /* If it's a multiply or a division/modulus operation of a multiple
+         of our constant, do the operation and verify it doesn't overflow.  */
+      if (code == MULT_EXPR
+	  || integer_zerop (const_binop (TRUNC_MOD_EXPR, op1, c, 0)))
+	{
+	  op1 = const_binop (code, fold_convert (ctype, op1),
+			     fold_convert (ctype, c), 0);
+	  /* We allow the constant to overflow with wrapping semantics.  */
+	  if (op1 == 0
+	      || (TREE_OVERFLOW (op1) && ! flag_wrapv))
+	    break;
+	}
+      else
+	break;
+
+      /* If we have an unsigned type is not a sizetype, we cannot widen
+	 the operation since it will change the result if the original
+	 computation overflowed.  */
+      if (TYPE_UNSIGNED (ctype)
+	  && ! (TREE_CODE (ctype) == INTEGER_TYPE && TYPE_IS_SIZETYPE (ctype))
+	  && ctype != type)
+	break;
+
+      /* If we were able to eliminate our operation from the first side,
+	 apply our operation to the second side and reform the PLUS.  */
+      if (t1 != 0 && (TREE_CODE (t1) != code || code == MULT_EXPR))
+	return fold_build2 (tcode, ctype, fold_convert (ctype, t1), op1);
+
+      /* The last case is if we are a multiply.  In that case, we can
+	 apply the distributive law to commute the multiply and addition
+	 if the multiplication of the constants doesn't overflow.  */
+      if (code == MULT_EXPR)
+	return fold_build2 (tcode, ctype,
+			    fold_build2 (code, ctype,
+					 fold_convert (ctype, op0),
+					 fold_convert (ctype, c)),
+			    op1);
+
+      break;
+
+    case MULT_EXPR:
+      /* We have a special case here if we are doing something like
+	 (C * 8) % 4 since we know that's zero.  */
+      if ((code == TRUNC_MOD_EXPR || code == CEIL_MOD_EXPR
+	   || code == FLOOR_MOD_EXPR || code == ROUND_MOD_EXPR)
+	  && TREE_CODE (TREE_OPERAND (t, 1)) == INTEGER_CST
+	  && integer_zerop (const_binop (TRUNC_MOD_EXPR, op1, c, 0)))
+	return omit_one_operand (type, integer_zero_node, op0);
+
+      /* ... fall through ...  */
+
+    case TRUNC_DIV_EXPR:  case CEIL_DIV_EXPR:  case FLOOR_DIV_EXPR:
+    case ROUND_DIV_EXPR:  case EXACT_DIV_EXPR:
+      /* If we can extract our operation from the LHS, do so and return a
+	 new operation.  Likewise for the RHS from a MULT_EXPR.  Otherwise,
+	 do something only if the second operand is a constant.  */
+      if (same_p
+	  && (t1 = extract_muldiv (op0, c, code, wide_type)) != 0)
+	return fold_build2 (tcode, ctype, fold_convert (ctype, t1),
+			    fold_convert (ctype, op1));
+      else if (tcode == MULT_EXPR && code == MULT_EXPR
+	       && (t1 = extract_muldiv (op1, c, code, wide_type)) != 0)
+	return fold_build2 (tcode, ctype, fold_convert (ctype, op0),
+			    fold_convert (ctype, t1));
+      else if (TREE_CODE (op1) != INTEGER_CST)
+	return 0;
+
+      /* If these are the same operation types, we can associate them
+	 assuming no overflow.  */
+      if (tcode == code
+	  && 0 != (t1 = const_binop (MULT_EXPR, fold_convert (ctype, op1),
+				     fold_convert (ctype, c), 0))
+	  && ! TREE_OVERFLOW (t1))
+	return fold_build2 (tcode, ctype, fold_convert (ctype, op0), t1);
+
+      /* If these operations "cancel" each other, we have the main
+	 optimizations of this pass, which occur when either constant is a
+	 multiple of the other, in which case we replace this with either an
+	 operation or CODE or TCODE.
+
+	 If we have an unsigned type that is not a sizetype, we cannot do
+	 this since it will change the result if the original computation
+	 overflowed.  */
+      if ((! TYPE_UNSIGNED (ctype)
+	   || (TREE_CODE (ctype) == INTEGER_TYPE && TYPE_IS_SIZETYPE (ctype)))
+	  && ! flag_wrapv
+	  && ((code == MULT_EXPR && tcode == EXACT_DIV_EXPR)
+	      || (tcode == MULT_EXPR
+		  && code != TRUNC_MOD_EXPR && code != CEIL_MOD_EXPR
+		  && code != FLOOR_MOD_EXPR && code != ROUND_MOD_EXPR)))
+	{
+	  if (integer_zerop (const_binop (TRUNC_MOD_EXPR, op1, c, 0)))
+	    return fold_build2 (tcode, ctype, fold_convert (ctype, op0),
+				fold_convert (ctype,
+					      const_binop (TRUNC_DIV_EXPR,
+							   op1, c, 0)));
+	  else if (integer_zerop (const_binop (TRUNC_MOD_EXPR, c, op1, 0)))
+	    return fold_build2 (code, ctype, fold_convert (ctype, op0),
+				fold_convert (ctype,
+					      const_binop (TRUNC_DIV_EXPR,
+							   c, op1, 0)));
+	}
+      break;
+
+    default:
+      break;
+    }
+
+  return 0;
+}
+
+/* Return a node which has the indicated constant VALUE (either 0 or
+   1), and is of the indicated TYPE.  */
+
+tree
+constant_boolean_node (int value, tree type)
+{
+  if (type == integer_type_node)
+    return value ? integer_one_node : integer_zero_node;
+  else if (type == boolean_type_node)
+    return value ? boolean_true_node : boolean_false_node;
+  else
+    return build_int_cst (type, value);
+}
+
+
+/* Return true if expr looks like an ARRAY_REF and set base and
+   offset to the appropriate trees.  If there is no offset,
+   offset is set to NULL_TREE.  Base will be canonicalized to
+   something you can get the element type from using
+   TREE_TYPE (TREE_TYPE (base)).  Offset will be the offset
+   in bytes to the base.  */
+
+static bool
+extract_array_ref (tree expr, tree *base, tree *offset)
+{
+  /* One canonical form is a PLUS_EXPR with the first
+     argument being an ADDR_EXPR with a possible NOP_EXPR
+     attached.  */
+  if (TREE_CODE (expr) == PLUS_EXPR)
+    {
+      tree op0 = TREE_OPERAND (expr, 0);
+      tree inner_base, dummy1;
+      /* Strip NOP_EXPRs here because the C frontends and/or
+	 folders present us (int *)&x.a + 4B possibly.  */
+      STRIP_NOPS (op0);
+      if (extract_array_ref (op0, &inner_base, &dummy1))
+	{
+	  *base = inner_base;
+	  if (dummy1 == NULL_TREE)
+	    *offset = TREE_OPERAND (expr, 1);
+	  else
+	    *offset = fold_build2 (PLUS_EXPR, TREE_TYPE (expr),
+				   dummy1, TREE_OPERAND (expr, 1));
+	  return true;
+	}
+    }
+  /* Other canonical form is an ADDR_EXPR of an ARRAY_REF,
+     which we transform into an ADDR_EXPR with appropriate
+     offset.  For other arguments to the ADDR_EXPR we assume
+     zero offset and as such do not care about the ADDR_EXPR
+     type and strip possible nops from it.  */
+  else if (TREE_CODE (expr) == ADDR_EXPR)
+    {
+      tree op0 = TREE_OPERAND (expr, 0);
+      if (TREE_CODE (op0) == ARRAY_REF)
+	{
+	  tree idx = TREE_OPERAND (op0, 1);
+	  *base = TREE_OPERAND (op0, 0);
+	  *offset = fold_build2 (MULT_EXPR, TREE_TYPE (idx), idx,
+				 array_ref_element_size (op0)); 
+	}
+      else
+	{
+	  /* Handle array-to-pointer decay as &a.  */
+	  if (TREE_CODE (TREE_TYPE (op0)) == ARRAY_TYPE)
+	    *base = TREE_OPERAND (expr, 0);
+	  else
+	    *base = expr;
+	  *offset = NULL_TREE;
+	}
+      return true;
+    }
+  /* The next canonical form is a VAR_DECL with POINTER_TYPE.  */
+  else if (SSA_VAR_P (expr)
+	   && TREE_CODE (TREE_TYPE (expr)) == POINTER_TYPE)
+    {
+      *base = expr;
+      *offset = NULL_TREE;
+      return true;
+    }
+
+  return false;
+}
+
+
+/* Transform `a + (b ? x : y)' into `b ? (a + x) : (a + y)'.
+   Transform, `a + (x < y)' into `(x < y) ? (a + 1) : (a + 0)'.  Here
+   CODE corresponds to the `+', COND to the `(b ? x : y)' or `(x < y)'
+   expression, and ARG to `a'.  If COND_FIRST_P is nonzero, then the
+   COND is the first argument to CODE; otherwise (as in the example
+   given here), it is the second argument.  TYPE is the type of the
+   original expression.  Return NULL_TREE if no simplification is
+   possible.  */
+
+static tree
+fold_binary_op_with_conditional_arg (enum tree_code code,
+				     tree type, tree op0, tree op1,
+				     tree cond, tree arg, int cond_first_p)
+{
+  tree cond_type = cond_first_p ? TREE_TYPE (op0) : TREE_TYPE (op1);
+  tree arg_type = cond_first_p ? TREE_TYPE (op1) : TREE_TYPE (op0);
+  tree test, true_value, false_value;
+  tree lhs = NULL_TREE;
+  tree rhs = NULL_TREE;
+
+  /* This transformation is only worthwhile if we don't have to wrap
+     arg in a SAVE_EXPR, and the operation can be simplified on at least
+     one of the branches once its pushed inside the COND_EXPR.  */
+  if (!TREE_CONSTANT (arg))
+    return NULL_TREE;
+
+  if (TREE_CODE (cond) == COND_EXPR)
+    {
+      test = TREE_OPERAND (cond, 0);
+      true_value = TREE_OPERAND (cond, 1);
+      false_value = TREE_OPERAND (cond, 2);
+      /* If this operand throws an expression, then it does not make
+	 sense to try to perform a logical or arithmetic operation
+	 involving it.  */
+      if (VOID_TYPE_P (TREE_TYPE (true_value)))
+	lhs = true_value;
+      if (VOID_TYPE_P (TREE_TYPE (false_value)))
+	rhs = false_value;
+    }
+  else
+    {
+      tree testtype = TREE_TYPE (cond);
+      test = cond;
+      true_value = constant_boolean_node (true, testtype);
+      false_value = constant_boolean_node (false, testtype);
+    }
+
+  arg = fold_convert (arg_type, arg);
+  if (lhs == 0)
+    {
+      true_value = fold_convert (cond_type, true_value);
+      if (cond_first_p)
+	lhs = fold_build2 (code, type, true_value, arg);
+      else
+	lhs = fold_build2 (code, type, arg, true_value);
+    }
+  if (rhs == 0)
+    {
+      false_value = fold_convert (cond_type, false_value);
+      if (cond_first_p)
+	rhs = fold_build2 (code, type, false_value, arg);
+      else
+	rhs = fold_build2 (code, type, arg, false_value);
+    }
+
+  test = fold_build3 (COND_EXPR, type, test, lhs, rhs);
+  return fold_convert (type, test);
+}
+
+
+/* Subroutine of fold() that checks for the addition of +/- 0.0.
+
+   If !NEGATE, return true if ADDEND is +/-0.0 and, for all X of type
+   TYPE, X + ADDEND is the same as X.  If NEGATE, return true if X -
+   ADDEND is the same as X.
+
+   X + 0 and X - 0 both give X when X is NaN, infinite, or nonzero
+   and finite.  The problematic cases are when X is zero, and its mode
+   has signed zeros.  In the case of rounding towards -infinity,
+   X - 0 is not the same as X because 0 - 0 is -0.  In other rounding
+   modes, X + 0 is not the same as X because -0 + 0 is 0.  */
+
+static bool
+fold_real_zero_addition_p (tree type, tree addend, int negate)
+{
+  if (!real_zerop (addend))
+    return false;
+
+  /* Don't allow the fold with -fsignaling-nans.  */
+  if (HONOR_SNANS (TYPE_MODE (type)))
+    return false;
+
+  /* Allow the fold if zeros aren't signed, or their sign isn't important.  */
+  if (!HONOR_SIGNED_ZEROS (TYPE_MODE (type)))
+    return true;
+
+  /* Treat x + -0 as x - 0 and x - -0 as x + 0.  */
+  if (TREE_CODE (addend) == REAL_CST
+      && REAL_VALUE_MINUS_ZERO (TREE_REAL_CST (addend)))
+    negate = !negate;
+
+  /* The mode has signed zeros, and we have to honor their sign.
+     In this situation, there is only one case we can return true for.
+     X - 0 is the same as X unless rounding towards -infinity is
+     supported.  */
+  return negate && !HONOR_SIGN_DEPENDENT_ROUNDING (TYPE_MODE (type));
+}
+
+/* Subroutine of fold() that checks comparisons of built-in math
+   functions against real constants.
+
+   FCODE is the DECL_FUNCTION_CODE of the built-in, CODE is the comparison
+   operator: EQ_EXPR, NE_EXPR, GT_EXPR, LT_EXPR, GE_EXPR or LE_EXPR.  TYPE
+   is the type of the result and ARG0 and ARG1 are the operands of the
+   comparison.  ARG1 must be a TREE_REAL_CST.
+
+   The function returns the constant folded tree if a simplification
+   can be made, and NULL_TREE otherwise.  */
+
+static tree
+fold_mathfn_compare (enum built_in_function fcode, enum tree_code code,
+		     tree type, tree arg0, tree arg1)
+{
+  REAL_VALUE_TYPE c;
+
+  if (BUILTIN_SQRT_P (fcode))
+    {
+      tree arg = TREE_VALUE (TREE_OPERAND (arg0, 1));
+      enum machine_mode mode = TYPE_MODE (TREE_TYPE (arg0));
+
+      c = TREE_REAL_CST (arg1);
+      if (REAL_VALUE_NEGATIVE (c))
+	{
+	  /* sqrt(x) < y is always false, if y is negative.  */
+	  if (code == EQ_EXPR || code == LT_EXPR || code == LE_EXPR)
+	    return omit_one_operand (type, integer_zero_node, arg);
+
+	  /* sqrt(x) > y is always true, if y is negative and we
+	     don't care about NaNs, i.e. negative values of x.  */
+	  if (code == NE_EXPR || !HONOR_NANS (mode))
+	    return omit_one_operand (type, integer_one_node, arg);
+
+	  /* sqrt(x) > y is the same as x >= 0, if y is negative.  */
+	  return fold_build2 (GE_EXPR, type, arg,
+			      build_real (TREE_TYPE (arg), dconst0));
+	}
+      else if (code == GT_EXPR || code == GE_EXPR)
+	{
+	  REAL_VALUE_TYPE c2;
+
+	  REAL_ARITHMETIC (c2, MULT_EXPR, c, c);
+	  real_convert (&c2, mode, &c2);
+
+	  if (REAL_VALUE_ISINF (c2))
+	    {
+	      /* sqrt(x) > y is x == +Inf, when y is very large.  */
+	      if (HONOR_INFINITIES (mode))
+		return fold_build2 (EQ_EXPR, type, arg,
+				    build_real (TREE_TYPE (arg), c2));
+
+	      /* sqrt(x) > y is always false, when y is very large
+		 and we don't care about infinities.  */
+	      return omit_one_operand (type, integer_zero_node, arg);
+	    }
+
+	  /* sqrt(x) > c is the same as x > c*c.  */
+	  return fold_build2 (code, type, arg,
+			      build_real (TREE_TYPE (arg), c2));
+	}
+      else if (code == LT_EXPR || code == LE_EXPR)
+	{
+	  REAL_VALUE_TYPE c2;
+
+	  REAL_ARITHMETIC (c2, MULT_EXPR, c, c);
+	  real_convert (&c2, mode, &c2);
+
+	  if (REAL_VALUE_ISINF (c2))
+	    {
+	      /* sqrt(x) < y is always true, when y is a very large
+		 value and we don't care about NaNs or Infinities.  */
+	      if (! HONOR_NANS (mode) && ! HONOR_INFINITIES (mode))
+		return omit_one_operand (type, integer_one_node, arg);
+
+	      /* sqrt(x) < y is x != +Inf when y is very large and we
+		 don't care about NaNs.  */
+	      if (! HONOR_NANS (mode))
+		return fold_build2 (NE_EXPR, type, arg,
+				    build_real (TREE_TYPE (arg), c2));
+
+	      /* sqrt(x) < y is x >= 0 when y is very large and we
+		 don't care about Infinities.  */
+	      if (! HONOR_INFINITIES (mode))
+		return fold_build2 (GE_EXPR, type, arg,
+				    build_real (TREE_TYPE (arg), dconst0));
+
+	      /* sqrt(x) < y is x >= 0 && x != +Inf, when y is large.  */
+	      if (lang_hooks.decls.global_bindings_p () != 0
+		  || CONTAINS_PLACEHOLDER_P (arg))
+		return NULL_TREE;
+
+	      arg = save_expr (arg);
+	      return fold_build2 (TRUTH_ANDIF_EXPR, type,
+				  fold_build2 (GE_EXPR, type, arg,
+					       build_real (TREE_TYPE (arg),
+							   dconst0)),
+				  fold_build2 (NE_EXPR, type, arg,
+					       build_real (TREE_TYPE (arg),
+							   c2)));
+	    }
+
+	  /* sqrt(x) < c is the same as x < c*c, if we ignore NaNs.  */
+	  if (! HONOR_NANS (mode))
+	    return fold_build2 (code, type, arg,
+				build_real (TREE_TYPE (arg), c2));
+
+	  /* sqrt(x) < c is the same as x >= 0 && x < c*c.  */
+	  if (lang_hooks.decls.global_bindings_p () == 0
+	      && ! CONTAINS_PLACEHOLDER_P (arg))
+	    {
+	      arg = save_expr (arg);
+	      return fold_build2 (TRUTH_ANDIF_EXPR, type,
+				  fold_build2 (GE_EXPR, type, arg,
+					       build_real (TREE_TYPE (arg),
+							   dconst0)),
+				  fold_build2 (code, type, arg,
+					       build_real (TREE_TYPE (arg),
+							   c2)));
+	    }
+	}
+    }
+
+  return NULL_TREE;
+}
+
+/* Subroutine of fold() that optimizes comparisons against Infinities,
+   either +Inf or -Inf.
+
+   CODE is the comparison operator: EQ_EXPR, NE_EXPR, GT_EXPR, LT_EXPR,
+   GE_EXPR or LE_EXPR.  TYPE is the type of the result and ARG0 and ARG1
+   are the operands of the comparison.  ARG1 must be a TREE_REAL_CST.
+
+   The function returns the constant folded tree if a simplification
+   can be made, and NULL_TREE otherwise.  */
+
+static tree
+fold_inf_compare (enum tree_code code, tree type, tree arg0, tree arg1)
+{
+  enum machine_mode mode;
+  REAL_VALUE_TYPE max;
+  tree temp;
+  bool neg;
+
+  mode = TYPE_MODE (TREE_TYPE (arg0));
+
+  /* For negative infinity swap the sense of the comparison.  */
+  neg = REAL_VALUE_NEGATIVE (TREE_REAL_CST (arg1));
+  if (neg)
+    code = swap_tree_comparison (code);
+
+  switch (code)
+    {
+    case GT_EXPR:
+      /* x > +Inf is always false, if with ignore sNANs.  */
+      if (HONOR_SNANS (mode))
+        return NULL_TREE;
+      return omit_one_operand (type, integer_zero_node, arg0);
+
+    case LE_EXPR:
+      /* x <= +Inf is always true, if we don't case about NaNs.  */
+      if (! HONOR_NANS (mode))
+	return omit_one_operand (type, integer_one_node, arg0);
+
+      /* x <= +Inf is the same as x == x, i.e. isfinite(x).  */
+      if (lang_hooks.decls.global_bindings_p () == 0
+	  && ! CONTAINS_PLACEHOLDER_P (arg0))
+	{
+	  arg0 = save_expr (arg0);
+	  return fold_build2 (EQ_EXPR, type, arg0, arg0);
+	}
+      break;
+
+    case EQ_EXPR:
+    case GE_EXPR:
+      /* x == +Inf and x >= +Inf are always equal to x > DBL_MAX.  */
+      real_maxval (&max, neg, mode);
+      return fold_build2 (neg ? LT_EXPR : GT_EXPR, type,
+			  arg0, build_real (TREE_TYPE (arg0), max));
+
+    case LT_EXPR:
+      /* x < +Inf is always equal to x <= DBL_MAX.  */
+      real_maxval (&max, neg, mode);
+      return fold_build2 (neg ? GE_EXPR : LE_EXPR, type,
+			  arg0, build_real (TREE_TYPE (arg0), max));
+
+    case NE_EXPR:
+      /* x != +Inf is always equal to !(x > DBL_MAX).  */
+      real_maxval (&max, neg, mode);
+      if (! HONOR_NANS (mode))
+	return fold_build2 (neg ? GE_EXPR : LE_EXPR, type,
+			    arg0, build_real (TREE_TYPE (arg0), max));
+
+      /* The transformation below creates non-gimple code and thus is
+	 not appropriate if we are in gimple form.  */
+      if (in_gimple_form)
+	return NULL_TREE;
+
+      temp = fold_build2 (neg ? LT_EXPR : GT_EXPR, type,
+			  arg0, build_real (TREE_TYPE (arg0), max));
+      return fold_build1 (TRUTH_NOT_EXPR, type, temp);
+
+    default:
+      break;
+    }
+
+  return NULL_TREE;
+}
+
+/* Subroutine of fold() that optimizes comparisons of a division by
+   a nonzero integer constant against an integer constant, i.e.
+   X/C1 op C2.
+
+   CODE is the comparison operator: EQ_EXPR, NE_EXPR, GT_EXPR, LT_EXPR,
+   GE_EXPR or LE_EXPR.  TYPE is the type of the result and ARG0 and ARG1
+   are the operands of the comparison.  ARG1 must be a TREE_REAL_CST.
+
+   The function returns the constant folded tree if a simplification
+   can be made, and NULL_TREE otherwise.  */
+
+static tree
+fold_div_compare (enum tree_code code, tree type, tree arg0, tree arg1)
+{
+  tree prod, tmp, hi, lo;
+  tree arg00 = TREE_OPERAND (arg0, 0);
+  tree arg01 = TREE_OPERAND (arg0, 1);
+  unsigned HOST_WIDE_INT lpart;
+  HOST_WIDE_INT hpart;
+  int overflow;
+
+  /* We have to do this the hard way to detect unsigned overflow.
+     prod = int_const_binop (MULT_EXPR, arg01, arg1, 0);  */
+  overflow = mul_double (TREE_INT_CST_LOW (arg01),
+			 TREE_INT_CST_HIGH (arg01),
+			 TREE_INT_CST_LOW (arg1),
+			 TREE_INT_CST_HIGH (arg1), &lpart, &hpart);
+  prod = build_int_cst_wide (TREE_TYPE (arg00), lpart, hpart);
+  prod = force_fit_type (prod, -1, overflow, false);
+
+  if (TYPE_UNSIGNED (TREE_TYPE (arg0)))
+    {
+      tmp = int_const_binop (MINUS_EXPR, arg01, integer_one_node, 0);
+      lo = prod;
+
+      /* Likewise hi = int_const_binop (PLUS_EXPR, prod, tmp, 0).  */
+      overflow = add_double (TREE_INT_CST_LOW (prod),
+			     TREE_INT_CST_HIGH (prod),
+			     TREE_INT_CST_LOW (tmp),
+			     TREE_INT_CST_HIGH (tmp),
+			     &lpart, &hpart);
+      hi = build_int_cst_wide (TREE_TYPE (arg00), lpart, hpart);
+      hi = force_fit_type (hi, -1, overflow | TREE_OVERFLOW (prod),
+			   TREE_CONSTANT_OVERFLOW (prod));
+    }
+  else if (tree_int_cst_sgn (arg01) >= 0)
+    {
+      tmp = int_const_binop (MINUS_EXPR, arg01, integer_one_node, 0);
+      switch (tree_int_cst_sgn (arg1))
+	{
+	case -1:
+	  lo = int_const_binop (MINUS_EXPR, prod, tmp, 0);
+	  hi = prod;
+	  break;
+
+	case  0:
+	  lo = fold_negate_const (tmp, TREE_TYPE (arg0));
+	  hi = tmp;
+	  break;
+
+	case  1:
+          hi = int_const_binop (PLUS_EXPR, prod, tmp, 0);
+	  lo = prod;
+	  break;
+
+	default:
+	  gcc_unreachable ();
+	}
+    }
+  else
+    {
+      /* A negative divisor reverses the relational operators.  */
+      code = swap_tree_comparison (code);
+
+      tmp = int_const_binop (PLUS_EXPR, arg01, integer_one_node, 0);
+      switch (tree_int_cst_sgn (arg1))
+	{
+	case -1:
+	  hi = int_const_binop (MINUS_EXPR, prod, tmp, 0);
+	  lo = prod;
+	  break;
+
+	case  0:
+	  hi = fold_negate_const (tmp, TREE_TYPE (arg0));
+	  lo = tmp;
+	  break;
+
+	case  1:
+          lo = int_const_binop (PLUS_EXPR, prod, tmp, 0);
+	  hi = prod;
+	  break;
+
+	default:
+	  gcc_unreachable ();
+	}
+    }
+
+  switch (code)
+    {
+    case EQ_EXPR:
+      if (TREE_OVERFLOW (lo) && TREE_OVERFLOW (hi))
+	return omit_one_operand (type, integer_zero_node, arg00);
+      if (TREE_OVERFLOW (hi))
+	return fold_build2 (GE_EXPR, type, arg00, lo);
+      if (TREE_OVERFLOW (lo))
+	return fold_build2 (LE_EXPR, type, arg00, hi);
+      return build_range_check (type, arg00, 1, lo, hi);
+
+    case NE_EXPR:
+      if (TREE_OVERFLOW (lo) && TREE_OVERFLOW (hi))
+	return omit_one_operand (type, integer_one_node, arg00);
+      if (TREE_OVERFLOW (hi))
+	return fold_build2 (LT_EXPR, type, arg00, lo);
+      if (TREE_OVERFLOW (lo))
+	return fold_build2 (GT_EXPR, type, arg00, hi);
+      return build_range_check (type, arg00, 0, lo, hi);
+
+    case LT_EXPR:
+      if (TREE_OVERFLOW (lo))
+	return omit_one_operand (type, integer_zero_node, arg00);
+      return fold_build2 (LT_EXPR, type, arg00, lo);
+
+    case LE_EXPR:
+      if (TREE_OVERFLOW (hi))
+	return omit_one_operand (type, integer_one_node, arg00);
+      return fold_build2 (LE_EXPR, type, arg00, hi);
+
+    case GT_EXPR:
+      if (TREE_OVERFLOW (hi))
+	return omit_one_operand (type, integer_zero_node, arg00);
+      return fold_build2 (GT_EXPR, type, arg00, hi);
+
+    case GE_EXPR:
+      if (TREE_OVERFLOW (lo))
+	return omit_one_operand (type, integer_one_node, arg00);
+      return fold_build2 (GE_EXPR, type, arg00, lo);
+
+    default:
+      break;
+    }
+
+  return NULL_TREE;
+}
+
+
+/* If CODE with arguments ARG0 and ARG1 represents a single bit
+   equality/inequality test, then return a simplified form of the test
+   using a sign testing.  Otherwise return NULL.  TYPE is the desired
+   result type.  */
+
+static tree
+fold_single_bit_test_into_sign_test (enum tree_code code, tree arg0, tree arg1,
+				     tree result_type)
+{
+  /* If this is testing a single bit, we can optimize the test.  */
+  if ((code == NE_EXPR || code == EQ_EXPR)
+      && TREE_CODE (arg0) == BIT_AND_EXPR && integer_zerop (arg1)
+      && integer_pow2p (TREE_OPERAND (arg0, 1)))
+    {
+      /* If we have (A & C) != 0 where C is the sign bit of A, convert
+	 this into A < 0.  Similarly for (A & C) == 0 into A >= 0.  */
+      tree arg00 = sign_bit_p (TREE_OPERAND (arg0, 0), TREE_OPERAND (arg0, 1));
+
+      if (arg00 != NULL_TREE
+	  /* This is only a win if casting to a signed type is cheap,
+	     i.e. when arg00's type is not a partial mode.  */
+	  && TYPE_PRECISION (TREE_TYPE (arg00))
+	     == GET_MODE_BITSIZE (TYPE_MODE (TREE_TYPE (arg00))))
+	{
+	  tree stype = lang_hooks.types.signed_type (TREE_TYPE (arg00));
+	  return fold_build2 (code == EQ_EXPR ? GE_EXPR : LT_EXPR,
+			      result_type, fold_convert (stype, arg00),
+			      fold_convert (stype, integer_zero_node));
+	}
+    }
+
+  return NULL_TREE;
+}
+
+/* If CODE with arguments ARG0 and ARG1 represents a single bit
+   equality/inequality test, then return a simplified form of
+   the test using shifts and logical operations.  Otherwise return
+   NULL.  TYPE is the desired result type.  */
+
+tree
+fold_single_bit_test (enum tree_code code, tree arg0, tree arg1,
+		      tree result_type)
+{
+  /* If this is testing a single bit, we can optimize the test.  */
+  if ((code == NE_EXPR || code == EQ_EXPR)
+      && TREE_CODE (arg0) == BIT_AND_EXPR && integer_zerop (arg1)
+      && integer_pow2p (TREE_OPERAND (arg0, 1)))
+    {
+      tree inner = TREE_OPERAND (arg0, 0);
+      tree type = TREE_TYPE (arg0);
+      int bitnum = tree_log2 (TREE_OPERAND (arg0, 1));
+      enum machine_mode operand_mode = TYPE_MODE (type);
+      int ops_unsigned;
+      tree signed_type, unsigned_type, intermediate_type;
+      tree tem;
+
+      /* First, see if we can fold the single bit test into a sign-bit
+	 test.  */
+      tem = fold_single_bit_test_into_sign_test (code, arg0, arg1,
+						 result_type);
+      if (tem)
+	return tem;
+
+      /* Otherwise we have (A & C) != 0 where C is a single bit,
+	 convert that into ((A >> C2) & 1).  Where C2 = log2(C).
+	 Similarly for (A & C) == 0.  */
+
+      /* If INNER is a right shift of a constant and it plus BITNUM does
+	 not overflow, adjust BITNUM and INNER.  */
+      if (TREE_CODE (inner) == RSHIFT_EXPR
+	  && TREE_CODE (TREE_OPERAND (inner, 1)) == INTEGER_CST
+	  && TREE_INT_CST_HIGH (TREE_OPERAND (inner, 1)) == 0
+	  && bitnum < TYPE_PRECISION (type)
+	  && 0 > compare_tree_int (TREE_OPERAND (inner, 1),
+				   bitnum - TYPE_PRECISION (type)))
+	{
+	  bitnum += TREE_INT_CST_LOW (TREE_OPERAND (inner, 1));
+	  inner = TREE_OPERAND (inner, 0);
+	}
+
+      /* If we are going to be able to omit the AND below, we must do our
+	 operations as unsigned.  If we must use the AND, we have a choice.
+	 Normally unsigned is faster, but for some machines signed is.  */
+#ifdef LOAD_EXTEND_OP
+      ops_unsigned = (LOAD_EXTEND_OP (operand_mode) == SIGN_EXTEND 
+		      && !flag_syntax_only) ? 0 : 1;
+#else
+      ops_unsigned = 1;
+#endif
+
+      signed_type = lang_hooks.types.type_for_mode (operand_mode, 0);
+      unsigned_type = lang_hooks.types.type_for_mode (operand_mode, 1);
+      intermediate_type = ops_unsigned ? unsigned_type : signed_type;
+      inner = fold_convert (intermediate_type, inner);
+
+      if (bitnum != 0)
+	inner = build2 (RSHIFT_EXPR, intermediate_type,
+			inner, size_int (bitnum));
+
+      if (code == EQ_EXPR)
+	inner = fold_build2 (BIT_XOR_EXPR, intermediate_type,
+			     inner, integer_one_node);
+
+      /* Put the AND last so it can combine with more things.  */
+      inner = build2 (BIT_AND_EXPR, intermediate_type,
+		      inner, integer_one_node);
+
+      /* Make sure to return the proper type.  */
+      inner = fold_convert (result_type, inner);
+
+      return inner;
+    }
+  return NULL_TREE;
+}
+
+/* Check whether we are allowed to reorder operands arg0 and arg1,
+   such that the evaluation of arg1 occurs before arg0.  */
+
+static bool
+reorder_operands_p (tree arg0, tree arg1)
+{
+  if (! flag_evaluation_order)
+      return true;
+  if (TREE_CONSTANT (arg0) || TREE_CONSTANT (arg1))
+    return true;
+  return ! TREE_SIDE_EFFECTS (arg0)
+	 && ! TREE_SIDE_EFFECTS (arg1);
+}
+
+/* Test whether it is preferable two swap two operands, ARG0 and
+   ARG1, for example because ARG0 is an integer constant and ARG1
+   isn't.  If REORDER is true, only recommend swapping if we can
+   evaluate the operands in reverse order.  */
+
+bool
+tree_swap_operands_p (tree arg0, tree arg1, bool reorder)
+{
+  STRIP_SIGN_NOPS (arg0);
+  STRIP_SIGN_NOPS (arg1);
+
+  if (TREE_CODE (arg1) == INTEGER_CST)
+    return 0;
+  if (TREE_CODE (arg0) == INTEGER_CST)
+    return 1;
+
+  if (TREE_CODE (arg1) == REAL_CST)
+    return 0;
+  if (TREE_CODE (arg0) == REAL_CST)
+    return 1;
+
+  if (TREE_CODE (arg1) == COMPLEX_CST)
+    return 0;
+  if (TREE_CODE (arg0) == COMPLEX_CST)
+    return 1;
+
+  if (TREE_CONSTANT (arg1))
+    return 0;
+  if (TREE_CONSTANT (arg0))
+    return 1;
+
+  if (optimize_size)
+    return 0;
+
+  if (reorder && flag_evaluation_order
+      && (TREE_SIDE_EFFECTS (arg0) || TREE_SIDE_EFFECTS (arg1)))
+    return 0;
+
+  if (DECL_P (arg1))
+    return 0;
+  if (DECL_P (arg0))
+    return 1;
+
+  /* It is preferable to swap two SSA_NAME to ensure a canonical form
+     for commutative and comparison operators.  Ensuring a canonical
+     form allows the optimizers to find additional redundancies without
+     having to explicitly check for both orderings.  */
+  if (TREE_CODE (arg0) == SSA_NAME
+      && TREE_CODE (arg1) == SSA_NAME
+      && SSA_NAME_VERSION (arg0) > SSA_NAME_VERSION (arg1))
+    return 1;
+
+  return 0;
+}
+
+/* Fold comparison ARG0 CODE ARG1 (with result in TYPE), where
+   ARG0 is extended to a wider type.  */
+
+static tree
+fold_widened_comparison (enum tree_code code, tree type, tree arg0, tree arg1)
+{
+  tree arg0_unw = get_unwidened (arg0, NULL_TREE);
+  tree arg1_unw;
+  tree shorter_type, outer_type;
+  tree min, max;
+  bool above, below;
+
+  if (arg0_unw == arg0)
+    return NULL_TREE;
+  shorter_type = TREE_TYPE (arg0_unw);
+
+#ifdef HAVE_canonicalize_funcptr_for_compare
+  /* Disable this optimization if we're casting a function pointer
+     type on targets that require function pointer canonicalization.  */
+  if (HAVE_canonicalize_funcptr_for_compare
+      && TREE_CODE (shorter_type) == POINTER_TYPE
+      && TREE_CODE (TREE_TYPE (shorter_type)) == FUNCTION_TYPE)
+    return NULL_TREE;
+#endif
+
+  if (TYPE_PRECISION (TREE_TYPE (arg0)) <= TYPE_PRECISION (shorter_type))
+    return NULL_TREE;
+
+  arg1_unw = get_unwidened (arg1, shorter_type);
+
+  /* If possible, express the comparison in the shorter mode.  */
+  if ((code == EQ_EXPR || code == NE_EXPR
+       || TYPE_UNSIGNED (TREE_TYPE (arg0)) == TYPE_UNSIGNED (shorter_type))
+      && (TREE_TYPE (arg1_unw) == shorter_type
+	  || (TREE_CODE (arg1_unw) == INTEGER_CST
+	      && (TREE_CODE (shorter_type) == INTEGER_TYPE
+		  || TREE_CODE (shorter_type) == BOOLEAN_TYPE)
+	      && int_fits_type_p (arg1_unw, shorter_type))))
+    return fold_build2 (code, type, arg0_unw,
+		       fold_convert (shorter_type, arg1_unw));
+
+  if (TREE_CODE (arg1_unw) != INTEGER_CST
+      || TREE_CODE (shorter_type) != INTEGER_TYPE
+      || !int_fits_type_p (arg1_unw, shorter_type))
+    return NULL_TREE;
+
+  /* If we are comparing with the integer that does not fit into the range
+     of the shorter type, the result is known.  */
+  outer_type = TREE_TYPE (arg1_unw);
+  min = lower_bound_in_type (outer_type, shorter_type);
+  max = upper_bound_in_type (outer_type, shorter_type);
+
+  above = integer_nonzerop (fold_relational_const (LT_EXPR, type,
+						   max, arg1_unw));
+  below = integer_nonzerop (fold_relational_const (LT_EXPR, type,
+						   arg1_unw, min));
+
+  switch (code)
+    {
+    case EQ_EXPR:
+      if (above || below)
+	return omit_one_operand (type, integer_zero_node, arg0);
+      break;
+
+    case NE_EXPR:
+      if (above || below)
+	return omit_one_operand (type, integer_one_node, arg0);
+      break;
+
+    case LT_EXPR:
+    case LE_EXPR:
+      if (above)
+	return omit_one_operand (type, integer_one_node, arg0);
+      else if (below)
+	return omit_one_operand (type, integer_zero_node, arg0);
+
+    case GT_EXPR:
+    case GE_EXPR:
+      if (above)
+	return omit_one_operand (type, integer_zero_node, arg0);
+      else if (below)
+	return omit_one_operand (type, integer_one_node, arg0);
+
+    default:
+      break;
+    }
+
+  return NULL_TREE;
+}
+
+/* Fold comparison ARG0 CODE ARG1 (with result in TYPE), where for
+   ARG0 just the signedness is changed.  */
+
+static tree
+fold_sign_changed_comparison (enum tree_code code, tree type,
+			      tree arg0, tree arg1)
+{
+  tree arg0_inner, tmp;
+  tree inner_type, outer_type;
+
+  if (TREE_CODE (arg0) != NOP_EXPR
+      && TREE_CODE (arg0) != CONVERT_EXPR)
+    return NULL_TREE;
+
+  outer_type = TREE_TYPE (arg0);
+  arg0_inner = TREE_OPERAND (arg0, 0);
+  inner_type = TREE_TYPE (arg0_inner);
+
+#ifdef HAVE_canonicalize_funcptr_for_compare
+  /* Disable this optimization if we're casting a function pointer
+     type on targets that require function pointer canonicalization.  */
+  if (HAVE_canonicalize_funcptr_for_compare
+      && TREE_CODE (inner_type) == POINTER_TYPE
+      && TREE_CODE (TREE_TYPE (inner_type)) == FUNCTION_TYPE)
+    return NULL_TREE;
+#endif
+
+  if (TYPE_PRECISION (inner_type) != TYPE_PRECISION (outer_type))
+    return NULL_TREE;
+
+  if (TREE_CODE (arg1) != INTEGER_CST
+      && !((TREE_CODE (arg1) == NOP_EXPR
+	    || TREE_CODE (arg1) == CONVERT_EXPR)
+	   && TREE_TYPE (TREE_OPERAND (arg1, 0)) == inner_type))
+    return NULL_TREE;
+
+  if (TYPE_UNSIGNED (inner_type) != TYPE_UNSIGNED (outer_type)
+      && code != NE_EXPR
+      && code != EQ_EXPR)
+    return NULL_TREE;
+
+  if (TREE_CODE (arg1) == INTEGER_CST)
+    {
+      tmp = build_int_cst_wide (inner_type,
+				TREE_INT_CST_LOW (arg1),
+				TREE_INT_CST_HIGH (arg1));
+      arg1 = force_fit_type (tmp, 0,
+			     TREE_OVERFLOW (arg1),
+			     TREE_CONSTANT_OVERFLOW (arg1));
+    }
+  else
+    arg1 = fold_convert (inner_type, arg1);
+
+  return fold_build2 (code, type, arg0_inner, arg1);
+}
+
+/* Tries to replace &a[idx] CODE s * delta with &a[idx CODE delta], if s is
+   step of the array.  Reconstructs s and delta in the case of s * delta
+   being an integer constant (and thus already folded).
+   ADDR is the address. MULT is the multiplicative expression.
+   If the function succeeds, the new address expression is returned.  Otherwise
+   NULL_TREE is returned.  */
+
+static tree
+try_move_mult_to_index (enum tree_code code, tree addr, tree op1)
+{
+  tree s, delta, step;
+  tree ref = TREE_OPERAND (addr, 0), pref;
+  tree ret, pos;
+  tree itype;
+
+  /* Canonicalize op1 into a possibly non-constant delta
+     and an INTEGER_CST s.  */
+  if (TREE_CODE (op1) == MULT_EXPR)
+    {
+      tree arg0 = TREE_OPERAND (op1, 0), arg1 = TREE_OPERAND (op1, 1);
+
+      STRIP_NOPS (arg0);
+      STRIP_NOPS (arg1);
+  
+      if (TREE_CODE (arg0) == INTEGER_CST)
+        {
+          s = arg0;
+          delta = arg1;
+        }
+      else if (TREE_CODE (arg1) == INTEGER_CST)
+        {
+          s = arg1;
+          delta = arg0;
+        }
+      else
+        return NULL_TREE;
+    }
+  else if (TREE_CODE (op1) == INTEGER_CST)
+    {
+      delta = op1;
+      s = NULL_TREE;
+    }
+  else
+    {
+      /* Simulate we are delta * 1.  */
+      delta = op1;
+      s = integer_one_node;
+    }
+
+  for (;; ref = TREE_OPERAND (ref, 0))
+    {
+      if (TREE_CODE (ref) == ARRAY_REF)
+	{
+	  itype = TYPE_DOMAIN (TREE_TYPE (TREE_OPERAND (ref, 0)));
+	  if (! itype)
+	    continue;
+
+	  step = array_ref_element_size (ref);
+	  if (TREE_CODE (step) != INTEGER_CST)
+	    continue;
+
+	  if (s)
+	    {
+	      if (! tree_int_cst_equal (step, s))
+                continue;
+	    }
+	  else
+	    {
+	      /* Try if delta is a multiple of step.  */
+	      tree tmp = div_if_zero_remainder (EXACT_DIV_EXPR, delta, step);
+	      if (! tmp)
+		continue;
+	      delta = tmp;
+	    }
+
+	  break;
+	}
+
+      if (!handled_component_p (ref))
+	return NULL_TREE;
+    }
+
+  /* We found the suitable array reference.  So copy everything up to it,
+     and replace the index.  */
+
+  pref = TREE_OPERAND (addr, 0);
+  ret = copy_node (pref);
+  pos = ret;
+
+  while (pref != ref)
+    {
+      pref = TREE_OPERAND (pref, 0);
+      TREE_OPERAND (pos, 0) = copy_node (pref);
+      pos = TREE_OPERAND (pos, 0);
+    }
+
+  TREE_OPERAND (pos, 1) = fold_build2 (code, itype,
+				       fold_convert (itype,
+						     TREE_OPERAND (pos, 1)),
+				       fold_convert (itype, delta));
+
+  return fold_build1 (ADDR_EXPR, TREE_TYPE (addr), ret);
+}
+
+
+/* Fold A < X && A + 1 > Y to A < X && A >= Y.  Normally A + 1 > Y
+   means A >= Y && A != MAX, but in this case we know that
+   A < X <= MAX.  INEQ is A + 1 > Y, BOUND is A < X.  */
+
+static tree
+fold_to_nonsharp_ineq_using_bound (tree ineq, tree bound)
+{
+  tree a, typea, type = TREE_TYPE (ineq), a1, diff, y;
+
+  if (TREE_CODE (bound) == LT_EXPR)
+    a = TREE_OPERAND (bound, 0);
+  else if (TREE_CODE (bound) == GT_EXPR)
+    a = TREE_OPERAND (bound, 1);
+  else
+    return NULL_TREE;
+
+  typea = TREE_TYPE (a);
+  if (!INTEGRAL_TYPE_P (typea)
+      && !POINTER_TYPE_P (typea))
+    return NULL_TREE;
+
+  if (TREE_CODE (ineq) == LT_EXPR)
+    {
+      a1 = TREE_OPERAND (ineq, 1);
+      y = TREE_OPERAND (ineq, 0);
+    }
+  else if (TREE_CODE (ineq) == GT_EXPR)
+    {
+      a1 = TREE_OPERAND (ineq, 0);
+      y = TREE_OPERAND (ineq, 1);
+    }
+  else
+    return NULL_TREE;
+
+  if (TREE_TYPE (a1) != typea)
+    return NULL_TREE;
+
+  diff = fold_build2 (MINUS_EXPR, typea, a1, a);
+  if (!integer_onep (diff))
+    return NULL_TREE;
+
+  return fold_build2 (GE_EXPR, type, a, y);
+}
+
+/* Fold a unary expression of code CODE and type TYPE with operand
+   OP0.  Return the folded expression if folding is successful.
+   Otherwise, return NULL_TREE.  */
+
+tree
+fold_unary (enum tree_code code, tree type, tree op0)
+{
+  tree tem;
+  tree arg0;
+  enum tree_code_class kind = TREE_CODE_CLASS (code);
+
+  gcc_assert (IS_EXPR_CODE_CLASS (kind)
+	      && TREE_CODE_LENGTH (code) == 1);
+
+  arg0 = op0;
+  if (arg0)
+    {
+      if (code == NOP_EXPR || code == CONVERT_EXPR
+	  || code == FLOAT_EXPR || code == ABS_EXPR)
+	{
+	  /* Don't use STRIP_NOPS, because signedness of argument type
+	     matters.  */
+	  STRIP_SIGN_NOPS (arg0);
+	}
+      else
+	{
+	  /* Strip any conversions that don't change the mode.  This
+	     is safe for every expression, except for a comparison
+	     expression because its signedness is derived from its
+	     operands.
+
+	     Note that this is done as an internal manipulation within
+	     the constant folder, in order to find the simplest
+	     representation of the arguments so that their form can be
+	     studied.  In any cases, the appropriate type conversions
+	     should be put back in the tree that will get out of the
+	     constant folder.  */
+	  STRIP_NOPS (arg0);
+	}
+    }
+
+  if (TREE_CODE_CLASS (code) == tcc_unary)
+    {
+      if (TREE_CODE (arg0) == COMPOUND_EXPR)
+	return build2 (COMPOUND_EXPR, type, TREE_OPERAND (arg0, 0),
+		       fold_build1 (code, type, TREE_OPERAND (arg0, 1)));
+      else if (TREE_CODE (arg0) == COND_EXPR)
+	{
+	  tree arg01 = TREE_OPERAND (arg0, 1);
+	  tree arg02 = TREE_OPERAND (arg0, 2);
+	  if (! VOID_TYPE_P (TREE_TYPE (arg01)))
+	    arg01 = fold_build1 (code, type, arg01);
+	  if (! VOID_TYPE_P (TREE_TYPE (arg02)))
+	    arg02 = fold_build1 (code, type, arg02);
+	  tem = fold_build3 (COND_EXPR, type, TREE_OPERAND (arg0, 0),
+			     arg01, arg02);
+
+	  /* If this was a conversion, and all we did was to move into
+	     inside the COND_EXPR, bring it back out.  But leave it if
+	     it is a conversion from integer to integer and the
+	     result precision is no wider than a word since such a
+	     conversion is cheap and may be optimized away by combine,
+	     while it couldn't if it were outside the COND_EXPR.  Then return
+	     so we don't get into an infinite recursion loop taking the
+	     conversion out and then back in.  */
+
+	  if ((code == NOP_EXPR || code == CONVERT_EXPR
+	       || code == NON_LVALUE_EXPR)
+	      && TREE_CODE (tem) == COND_EXPR
+	      && TREE_CODE (TREE_OPERAND (tem, 1)) == code
+	      && TREE_CODE (TREE_OPERAND (tem, 2)) == code
+	      && ! VOID_TYPE_P (TREE_OPERAND (tem, 1))
+	      && ! VOID_TYPE_P (TREE_OPERAND (tem, 2))
+	      && (TREE_TYPE (TREE_OPERAND (TREE_OPERAND (tem, 1), 0))
+		  == TREE_TYPE (TREE_OPERAND (TREE_OPERAND (tem, 2), 0)))
+	      && (! (INTEGRAL_TYPE_P (TREE_TYPE (tem))
+		     && (INTEGRAL_TYPE_P
+			 (TREE_TYPE (TREE_OPERAND (TREE_OPERAND (tem, 1), 0))))
+		     && TYPE_PRECISION (TREE_TYPE (tem)) <= BITS_PER_WORD)
+		  || flag_syntax_only))
+	    tem = build1 (code, type,
+			  build3 (COND_EXPR,
+				  TREE_TYPE (TREE_OPERAND
+					     (TREE_OPERAND (tem, 1), 0)),
+				  TREE_OPERAND (tem, 0),
+				  TREE_OPERAND (TREE_OPERAND (tem, 1), 0),
+				  TREE_OPERAND (TREE_OPERAND (tem, 2), 0)));
+	  return tem;
+	}
+      else if (COMPARISON_CLASS_P (arg0))
+	{
+	  if (TREE_CODE (type) == BOOLEAN_TYPE)
+	    {
+	      arg0 = copy_node (arg0);
+	      TREE_TYPE (arg0) = type;
+	      return arg0;
+	    }
+	  else if (TREE_CODE (type) != INTEGER_TYPE)
+	    return fold_build3 (COND_EXPR, type, arg0,
+				fold_build1 (code, type,
+					     integer_one_node),
+				fold_build1 (code, type,
+					     integer_zero_node));
+	}
+   }
+
+  switch (code)
+    {
+    case NOP_EXPR:
+    case FLOAT_EXPR:
+    case CONVERT_EXPR:
+    case FIX_TRUNC_EXPR:
+    case FIX_CEIL_EXPR:
+    case FIX_FLOOR_EXPR:
+    case FIX_ROUND_EXPR:
+      if (TREE_TYPE (op0) == type)
+	return op0;
+
+      /* Handle cases of two conversions in a row.  */
+      if (TREE_CODE (op0) == NOP_EXPR
+	  || TREE_CODE (op0) == CONVERT_EXPR)
+	{
+	  tree inside_type = TREE_TYPE (TREE_OPERAND (op0, 0));
+	  tree inter_type = TREE_TYPE (op0);
+	  int inside_int = INTEGRAL_TYPE_P (inside_type);
+	  int inside_ptr = POINTER_TYPE_P (inside_type);
+	  int inside_float = FLOAT_TYPE_P (inside_type);
+	  int inside_vec = TREE_CODE (inside_type) == VECTOR_TYPE;
+	  unsigned int inside_prec = TYPE_PRECISION (inside_type);
+	  int inside_unsignedp = TYPE_UNSIGNED (inside_type);
+	  int inter_int = INTEGRAL_TYPE_P (inter_type);
+	  int inter_ptr = POINTER_TYPE_P (inter_type);
+	  int inter_float = FLOAT_TYPE_P (inter_type);
+	  int inter_vec = TREE_CODE (inter_type) == VECTOR_TYPE;
+	  unsigned int inter_prec = TYPE_PRECISION (inter_type);
+	  int inter_unsignedp = TYPE_UNSIGNED (inter_type);
+	  int final_int = INTEGRAL_TYPE_P (type);
+	  int final_ptr = POINTER_TYPE_P (type);
+	  int final_float = FLOAT_TYPE_P (type);
+	  int final_vec = TREE_CODE (type) == VECTOR_TYPE;
+	  unsigned int final_prec = TYPE_PRECISION (type);
+	  int final_unsignedp = TYPE_UNSIGNED (type);
+
+	  /* In addition to the cases of two conversions in a row
+	     handled below, if we are converting something to its own
+	     type via an object of identical or wider precision, neither
+	     conversion is needed.  */
+	  if (TYPE_MAIN_VARIANT (inside_type) == TYPE_MAIN_VARIANT (type)
+	      && ((inter_int && final_int) || (inter_float && final_float))
+	      && inter_prec >= final_prec)
+	    return fold_build1 (code, type, TREE_OPERAND (op0, 0));
+
+	  /* Likewise, if the intermediate and final types are either both
+	     float or both integer, we don't need the middle conversion if
+	     it is wider than the final type and doesn't change the signedness
+	     (for integers).  Avoid this if the final type is a pointer
+	     since then we sometimes need the inner conversion.  Likewise if
+	     the outer has a precision not equal to the size of its mode.  */
+	  if ((((inter_int || inter_ptr) && (inside_int || inside_ptr))
+	       || (inter_float && inside_float)
+	       || (inter_vec && inside_vec))
+	      && inter_prec >= inside_prec
+	      && (inter_float || inter_vec
+		  || inter_unsignedp == inside_unsignedp)
+	      && ! (final_prec != GET_MODE_BITSIZE (TYPE_MODE (type))
+		    && TYPE_MODE (type) == TYPE_MODE (inter_type))
+	      && ! final_ptr
+	      && (! final_vec || inter_prec == inside_prec))
+	    return fold_build1 (code, type, TREE_OPERAND (op0, 0));
+
+	  /* If we have a sign-extension of a zero-extended value, we can
+	     replace that by a single zero-extension.  */
+	  if (inside_int && inter_int && final_int
+	      && inside_prec < inter_prec && inter_prec < final_prec
+	      && inside_unsignedp && !inter_unsignedp)
+	    return fold_build1 (code, type, TREE_OPERAND (op0, 0));
+
+	  /* Two conversions in a row are not needed unless:
+	     - some conversion is floating-point (overstrict for now), or
+	     - some conversion is a vector (overstrict for now), or
+	     - the intermediate type is narrower than both initial and
+	       final, or
+	     - the intermediate type and innermost type differ in signedness,
+	       and the outermost type is wider than the intermediate, or
+	     - the initial type is a pointer type and the precisions of the
+	       intermediate and final types differ, or
+	     - the final type is a pointer type and the precisions of the
+	       initial and intermediate types differ.  */
+	  if (! inside_float && ! inter_float && ! final_float
+	      && ! inside_vec && ! inter_vec && ! final_vec
+	      && (inter_prec > inside_prec || inter_prec > final_prec)
+	      && ! (inside_int && inter_int
+		    && inter_unsignedp != inside_unsignedp
+		    && inter_prec < final_prec)
+	      && ((inter_unsignedp && inter_prec > inside_prec)
+		  == (final_unsignedp && final_prec > inter_prec))
+	      && ! (inside_ptr && inter_prec != final_prec)
+	      && ! (final_ptr && inside_prec != inter_prec)
+	      && ! (final_prec != GET_MODE_BITSIZE (TYPE_MODE (type))
+		    && TYPE_MODE (type) == TYPE_MODE (inter_type))
+	      && ! final_ptr)
+	    return fold_build1 (code, type, TREE_OPERAND (op0, 0));
+	}
+
+      /* Handle (T *)&A.B.C for A being of type T and B and C
+	 living at offset zero.  This occurs frequently in
+	 C++ upcasting and then accessing the base.  */
+      if (TREE_CODE (op0) == ADDR_EXPR
+	  && POINTER_TYPE_P (type)
+	  && handled_component_p (TREE_OPERAND (op0, 0)))
+        {
+	  HOST_WIDE_INT bitsize, bitpos;
+	  tree offset;
+	  enum machine_mode mode;
+	  int unsignedp, volatilep;
+          tree base = TREE_OPERAND (op0, 0);
+	  base = get_inner_reference (base, &bitsize, &bitpos, &offset,
+				      &mode, &unsignedp, &volatilep, false);
+	  /* If the reference was to a (constant) zero offset, we can use
+	     the address of the base if it has the same base type
+	     as the result type.  */
+	  if (! offset && bitpos == 0
+	      && TYPE_MAIN_VARIANT (TREE_TYPE (type))
+		  == TYPE_MAIN_VARIANT (TREE_TYPE (base)))
+	    return fold_convert (type, build_fold_addr_expr (base));
+        }
+
+      if (TREE_CODE (op0) == MODIFY_EXPR
+	  && TREE_CONSTANT (TREE_OPERAND (op0, 1))
+	  /* Detect assigning a bitfield.  */
+	  && !(TREE_CODE (TREE_OPERAND (op0, 0)) == COMPONENT_REF
+	       && DECL_BIT_FIELD (TREE_OPERAND (TREE_OPERAND (op0, 0), 1))))
+	{
+	  /* Don't leave an assignment inside a conversion
+	     unless assigning a bitfield.  */
+	  tem = fold_build1 (code, type, TREE_OPERAND (op0, 1));
+	  /* First do the assignment, then return converted constant.  */
+	  tem = build2 (COMPOUND_EXPR, TREE_TYPE (tem), op0, tem);
+	  TREE_NO_WARNING (tem) = 1;
+	  TREE_USED (tem) = 1;
+	  return tem;
+	}
+
+      /* Convert (T)(x & c) into (T)x & (T)c, if c is an integer
+	 constants (if x has signed type, the sign bit cannot be set
+	 in c).  This folds extension into the BIT_AND_EXPR.  */
+      if (INTEGRAL_TYPE_P (type)
+	  && TREE_CODE (type) != BOOLEAN_TYPE
+	  && TREE_CODE (op0) == BIT_AND_EXPR
+	  && TREE_CODE (TREE_OPERAND (op0, 1)) == INTEGER_CST)
+	{
+	  tree and = op0;
+	  tree and0 = TREE_OPERAND (and, 0), and1 = TREE_OPERAND (and, 1);
+	  int change = 0;
+
+	  if (TYPE_UNSIGNED (TREE_TYPE (and))
+	      || (TYPE_PRECISION (type)
+		  <= TYPE_PRECISION (TREE_TYPE (and))))
+	    change = 1;
+	  else if (TYPE_PRECISION (TREE_TYPE (and1))
+		   <= HOST_BITS_PER_WIDE_INT
+		   && host_integerp (and1, 1))
+	    {
+	      unsigned HOST_WIDE_INT cst;
+
+	      cst = tree_low_cst (and1, 1);
+	      cst &= (HOST_WIDE_INT) -1
+		     << (TYPE_PRECISION (TREE_TYPE (and1)) - 1);
+	      change = (cst == 0);
+#ifdef LOAD_EXTEND_OP
+	      if (change
+		  && !flag_syntax_only
+		  && (LOAD_EXTEND_OP (TYPE_MODE (TREE_TYPE (and0)))
+		      == ZERO_EXTEND))
+		{
+		  tree uns = lang_hooks.types.unsigned_type (TREE_TYPE (and0));
+		  and0 = fold_convert (uns, and0);
+		  and1 = fold_convert (uns, and1);
+		}
+#endif
+	    }
+	  if (change)
+	    {
+	      tem = build_int_cst_wide (type, TREE_INT_CST_LOW (and1),
+					TREE_INT_CST_HIGH (and1));
+	      tem = force_fit_type (tem, 0, TREE_OVERFLOW (and1),
+				    TREE_CONSTANT_OVERFLOW (and1));
+	      return fold_build2 (BIT_AND_EXPR, type,
+				  fold_convert (type, and0), tem);
+	    }
+	}
+
+      /* Convert (T1)((T2)X op Y) into (T1)X op Y, for pointer types T1 and
+	 T2 being pointers to types of the same size.  */
+      if (POINTER_TYPE_P (type)
+	  && BINARY_CLASS_P (arg0)
+	  && TREE_CODE (TREE_OPERAND (arg0, 0)) == NOP_EXPR
+	  && POINTER_TYPE_P (TREE_TYPE (TREE_OPERAND (arg0, 0))))
+	{
+	  tree arg00 = TREE_OPERAND (arg0, 0);
+	  tree t0 = type;
+	  tree t1 = TREE_TYPE (arg00);
+	  tree tt0 = TREE_TYPE (t0);
+	  tree tt1 = TREE_TYPE (t1);
+	  tree s0 = TYPE_SIZE (tt0);
+	  tree s1 = TYPE_SIZE (tt1);
+
+	  if (s0 && s1 && operand_equal_p (s0, s1, OEP_ONLY_CONST))
+	    return build2 (TREE_CODE (arg0), t0, fold_convert (t0, arg00),
+			   TREE_OPERAND (arg0, 1));
+	}
+
+      tem = fold_convert_const (code, type, arg0);
+      return tem ? tem : NULL_TREE;
+
+    case VIEW_CONVERT_EXPR:
+      if (TREE_CODE (op0) == VIEW_CONVERT_EXPR)
+	return build1 (VIEW_CONVERT_EXPR, type, TREE_OPERAND (op0, 0));
+      return NULL_TREE;
+
+    case NEGATE_EXPR:
+      if (negate_expr_p (arg0))
+	return fold_convert (type, negate_expr (arg0));
+      /* Convert - (~A) to A + 1.  */
+      if (INTEGRAL_TYPE_P (type) && TREE_CODE (arg0) == BIT_NOT_EXPR)
+	return fold_build2 (PLUS_EXPR, type, TREE_OPERAND (arg0, 0),
+			    build_int_cst (type, 1));
+      return NULL_TREE;
+
+    case ABS_EXPR:
+      if (TREE_CODE (arg0) == INTEGER_CST || TREE_CODE (arg0) == REAL_CST)
+	return fold_abs_const (arg0, type);
+      else if (TREE_CODE (arg0) == NEGATE_EXPR)
+	return fold_build1 (ABS_EXPR, type, TREE_OPERAND (arg0, 0));
+      /* Convert fabs((double)float) into (double)fabsf(float).  */
+      else if (TREE_CODE (arg0) == NOP_EXPR
+	       && TREE_CODE (type) == REAL_TYPE)
+	{
+	  tree targ0 = strip_float_extensions (arg0);
+	  if (targ0 != arg0)
+	    return fold_convert (type, fold_build1 (ABS_EXPR,
+						    TREE_TYPE (targ0),
+						    targ0));
+	}
+      /* ABS_EXPR<ABS_EXPR<x>> = ABS_EXPR<x> even if flag_wrapv is on.  */
+      else if (tree_expr_nonnegative_p (arg0) || TREE_CODE (arg0) == ABS_EXPR)
+	return arg0;
+
+      /* Strip sign ops from argument.  */
+      if (TREE_CODE (type) == REAL_TYPE)
+	{
+	  tem = fold_strip_sign_ops (arg0);
+	  if (tem)
+	    return fold_build1 (ABS_EXPR, type, fold_convert (type, tem));
+	}
+      return NULL_TREE;
+
+    case CONJ_EXPR:
+      if (TREE_CODE (TREE_TYPE (arg0)) != COMPLEX_TYPE)
+	return fold_convert (type, arg0);
+      else if (TREE_CODE (arg0) == COMPLEX_EXPR)
+	return build2 (COMPLEX_EXPR, type,
+		       TREE_OPERAND (arg0, 0),
+		       negate_expr (TREE_OPERAND (arg0, 1)));
+      else if (TREE_CODE (arg0) == COMPLEX_CST)
+	return build_complex (type, TREE_REALPART (arg0),
+			      negate_expr (TREE_IMAGPART (arg0)));
+      else if (TREE_CODE (arg0) == PLUS_EXPR || TREE_CODE (arg0) == MINUS_EXPR)
+	return fold_build2 (TREE_CODE (arg0), type,
+			    fold_build1 (CONJ_EXPR, type,
+					 TREE_OPERAND (arg0, 0)),
+			    fold_build1 (CONJ_EXPR, type,
+					 TREE_OPERAND (arg0, 1)));
+      else if (TREE_CODE (arg0) == CONJ_EXPR)
+	return TREE_OPERAND (arg0, 0);
+      return NULL_TREE;
+
+    case BIT_NOT_EXPR:
+      if (TREE_CODE (arg0) == INTEGER_CST)
+        return fold_not_const (arg0, type);
+      else if (TREE_CODE (arg0) == BIT_NOT_EXPR)
+	return TREE_OPERAND (arg0, 0);
+      /* Convert ~ (-A) to A - 1.  */
+      else if (INTEGRAL_TYPE_P (type) && TREE_CODE (arg0) == NEGATE_EXPR)
+	return fold_build2 (MINUS_EXPR, type, TREE_OPERAND (arg0, 0),
+			    build_int_cst (type, 1));
+      /* Convert ~ (A - 1) or ~ (A + -1) to -A.  */
+      else if (INTEGRAL_TYPE_P (type)
+	       && ((TREE_CODE (arg0) == MINUS_EXPR
+		    && integer_onep (TREE_OPERAND (arg0, 1)))
+		   || (TREE_CODE (arg0) == PLUS_EXPR
+		       && integer_all_onesp (TREE_OPERAND (arg0, 1)))))
+	return fold_build1 (NEGATE_EXPR, type, TREE_OPERAND (arg0, 0));
+      /* Convert ~(X ^ Y) to ~X ^ Y or X ^ ~Y if ~X or ~Y simplify.  */
+      else if (TREE_CODE (arg0) == BIT_XOR_EXPR
+	       && (tem = fold_unary (BIT_NOT_EXPR, type,
+			       	     fold_convert (type,
+					     	   TREE_OPERAND (arg0, 0)))))
+	return fold_build2 (BIT_XOR_EXPR, type, tem,
+			    fold_convert (type, TREE_OPERAND (arg0, 1)));
+      else if (TREE_CODE (arg0) == BIT_XOR_EXPR
+	       && (tem = fold_unary (BIT_NOT_EXPR, type,
+			       	     fold_convert (type,
+					     	   TREE_OPERAND (arg0, 1)))))
+	return fold_build2 (BIT_XOR_EXPR, type,
+			    fold_convert (type, TREE_OPERAND (arg0, 0)), tem);
+
+      return NULL_TREE;
+
+    case TRUTH_NOT_EXPR:
+      /* The argument to invert_truthvalue must have Boolean type.  */
+      if (TREE_CODE (TREE_TYPE (arg0)) != BOOLEAN_TYPE)
+          arg0 = fold_convert (boolean_type_node, arg0);
+
+      /* Note that the operand of this must be an int
+	 and its values must be 0 or 1.
+	 ("true" is a fixed value perhaps depending on the language,
+	 but we don't handle values other than 1 correctly yet.)  */
+      tem = invert_truthvalue (arg0);
+      /* Avoid infinite recursion.  */
+      if (TREE_CODE (tem) == TRUTH_NOT_EXPR)
+	return NULL_TREE;
+      return fold_convert (type, tem);
+
+    case REALPART_EXPR:
+      if (TREE_CODE (TREE_TYPE (arg0)) != COMPLEX_TYPE)
+	return NULL_TREE;
+      else if (TREE_CODE (arg0) == COMPLEX_EXPR)
+	return omit_one_operand (type, TREE_OPERAND (arg0, 0),
+				 TREE_OPERAND (arg0, 1));
+      else if (TREE_CODE (arg0) == COMPLEX_CST)
+	return TREE_REALPART (arg0);
+      else if (TREE_CODE (arg0) == PLUS_EXPR || TREE_CODE (arg0) == MINUS_EXPR)
+	return fold_build2 (TREE_CODE (arg0), type,
+			    fold_build1 (REALPART_EXPR, type,
+					 TREE_OPERAND (arg0, 0)),
+			    fold_build1 (REALPART_EXPR, type,
+					 TREE_OPERAND (arg0, 1)));
+      return NULL_TREE;
+
+    case IMAGPART_EXPR:
+      if (TREE_CODE (TREE_TYPE (arg0)) != COMPLEX_TYPE)
+	return fold_convert (type, integer_zero_node);
+      else if (TREE_CODE (arg0) == COMPLEX_EXPR)
+	return omit_one_operand (type, TREE_OPERAND (arg0, 1),
+				 TREE_OPERAND (arg0, 0));
+      else if (TREE_CODE (arg0) == COMPLEX_CST)
+	return TREE_IMAGPART (arg0);
+      else if (TREE_CODE (arg0) == PLUS_EXPR || TREE_CODE (arg0) == MINUS_EXPR)
+	return fold_build2 (TREE_CODE (arg0), type,
+			    fold_build1 (IMAGPART_EXPR, type,
+					 TREE_OPERAND (arg0, 0)),
+			    fold_build1 (IMAGPART_EXPR, type,
+					 TREE_OPERAND (arg0, 1)));
+      return NULL_TREE;
+
+    default:
+      return NULL_TREE;
+    } /* switch (code) */
+}
+
+/* Fold a binary expression of code CODE and type TYPE with operands
+   OP0 and OP1.  Return the folded expression if folding is
+   successful.  Otherwise, return NULL_TREE.  */
+
+tree
+fold_binary (enum tree_code code, tree type, tree op0, tree op1)
+{
+  tree t1 = NULL_TREE;
+  tree tem;
+  tree arg0 = NULL_TREE, arg1 = NULL_TREE;
+  enum tree_code_class kind = TREE_CODE_CLASS (code);
+
+  /* WINS will be nonzero when the switch is done
+     if all operands are constant.  */
+  int wins = 1;
+
+  gcc_assert (IS_EXPR_CODE_CLASS (kind)
+	      && TREE_CODE_LENGTH (code) == 2);
+
+  arg0 = op0;
+  arg1 = op1;
+
+  if (arg0)
+    {
+      tree subop;
+
+      /* Strip any conversions that don't change the mode.  This is
+	 safe for every expression, except for a comparison expression
+	 because its signedness is derived from its operands.  So, in
+	 the latter case, only strip conversions that don't change the
+	 signedness.
+
+	 Note that this is done as an internal manipulation within the
+	 constant folder, in order to find the simplest representation
+	 of the arguments so that their form can be studied.  In any
+	 cases, the appropriate type conversions should be put back in
+	 the tree that will get out of the constant folder.  */
+      if (kind == tcc_comparison)
+	STRIP_SIGN_NOPS (arg0);
+      else
+	STRIP_NOPS (arg0);
+
+      if (TREE_CODE (arg0) == COMPLEX_CST)
+	subop = TREE_REALPART (arg0);
+      else
+	subop = arg0;
+
+      if (TREE_CODE (subop) != INTEGER_CST
+	  && TREE_CODE (subop) != REAL_CST)
+	/* Note that TREE_CONSTANT isn't enough:
+	   static var addresses are constant but we can't
+	   do arithmetic on them.  */
+	wins = 0;
+    }
+
+  if (arg1)
+    {
+      tree subop;
+
+      /* Strip any conversions that don't change the mode.  This is
+	 safe for every expression, except for a comparison expression
+	 because its signedness is derived from its operands.  So, in
+	 the latter case, only strip conversions that don't change the
+	 signedness.
+
+	 Note that this is done as an internal manipulation within the
+	 constant folder, in order to find the simplest representation
+	 of the arguments so that their form can be studied.  In any
+	 cases, the appropriate type conversions should be put back in
+	 the tree that will get out of the constant folder.  */
+      if (kind == tcc_comparison)
+	STRIP_SIGN_NOPS (arg1);
+      else
+	STRIP_NOPS (arg1);
+
+      if (TREE_CODE (arg1) == COMPLEX_CST)
+	subop = TREE_REALPART (arg1);
+      else
+	subop = arg1;
+
+      if (TREE_CODE (subop) != INTEGER_CST
+	  && TREE_CODE (subop) != REAL_CST)
+	/* Note that TREE_CONSTANT isn't enough:
+	   static var addresses are constant but we can't
+	   do arithmetic on them.  */
+	wins = 0;
+    }
+
+  /* If this is a commutative operation, and ARG0 is a constant, move it
+     to ARG1 to reduce the number of tests below.  */
+  if (commutative_tree_code (code)
+      && tree_swap_operands_p (arg0, arg1, true))
+    return fold_build2 (code, type, op1, op0);
+
+  /* Now WINS is set as described above,
+     ARG0 is the first operand of EXPR,
+     and ARG1 is the second operand (if it has more than one operand).
+
+     First check for cases where an arithmetic operation is applied to a
+     compound, conditional, or comparison operation.  Push the arithmetic
+     operation inside the compound or conditional to see if any folding
+     can then be done.  Convert comparison to conditional for this purpose.
+     The also optimizes non-constant cases that used to be done in
+     expand_expr.
+
+     Before we do that, see if this is a BIT_AND_EXPR or a BIT_IOR_EXPR,
+     one of the operands is a comparison and the other is a comparison, a
+     BIT_AND_EXPR with the constant 1, or a truth value.  In that case, the
+     code below would make the expression more complex.  Change it to a
+     TRUTH_{AND,OR}_EXPR.  Likewise, convert a similar NE_EXPR to
+     TRUTH_XOR_EXPR and an EQ_EXPR to the inversion of a TRUTH_XOR_EXPR.  */
+
+  if ((code == BIT_AND_EXPR || code == BIT_IOR_EXPR
+       || code == EQ_EXPR || code == NE_EXPR)
+      && ((truth_value_p (TREE_CODE (arg0))
+	   && (truth_value_p (TREE_CODE (arg1))
+	       || (TREE_CODE (arg1) == BIT_AND_EXPR
+		   && integer_onep (TREE_OPERAND (arg1, 1)))))
+	  || (truth_value_p (TREE_CODE (arg1))
+	      && (truth_value_p (TREE_CODE (arg0))
+		  || (TREE_CODE (arg0) == BIT_AND_EXPR
+		      && integer_onep (TREE_OPERAND (arg0, 1)))))))
+    {
+      tem = fold_build2 (code == BIT_AND_EXPR ? TRUTH_AND_EXPR
+			 : code == BIT_IOR_EXPR ? TRUTH_OR_EXPR
+			 : TRUTH_XOR_EXPR,
+			 boolean_type_node,
+			 fold_convert (boolean_type_node, arg0),
+			 fold_convert (boolean_type_node, arg1));
+
+      if (code == EQ_EXPR)
+	tem = invert_truthvalue (tem);
+
+      return fold_convert (type, tem);
+    }
+
+  if (TREE_CODE_CLASS (code) == tcc_binary
+      || TREE_CODE_CLASS (code) == tcc_comparison)
+    {
+      if (TREE_CODE (arg0) == COMPOUND_EXPR)
+	return build2 (COMPOUND_EXPR, type, TREE_OPERAND (arg0, 0),
+		       fold_build2 (code, type,
+				    TREE_OPERAND (arg0, 1), op1));
+      if (TREE_CODE (arg1) == COMPOUND_EXPR
+	  && reorder_operands_p (arg0, TREE_OPERAND (arg1, 0)))
+	return build2 (COMPOUND_EXPR, type, TREE_OPERAND (arg1, 0),
+		       fold_build2 (code, type,
+				    op0, TREE_OPERAND (arg1, 1)));
+
+      if (TREE_CODE (arg0) == COND_EXPR || COMPARISON_CLASS_P (arg0))
+	{
+	  tem = fold_binary_op_with_conditional_arg (code, type, op0, op1,
+						     arg0, arg1, 
+						     /*cond_first_p=*/1);
+	  if (tem != NULL_TREE)
+	    return tem;
+	}
+
+      if (TREE_CODE (arg1) == COND_EXPR || COMPARISON_CLASS_P (arg1))
+	{
+	  tem = fold_binary_op_with_conditional_arg (code, type, op0, op1,
+						     arg1, arg0, 
+					             /*cond_first_p=*/0);
+	  if (tem != NULL_TREE)
+	    return tem;
+	}
+    }
+
+  switch (code)
+    {
+    case PLUS_EXPR:
+      /* A + (-B) -> A - B */
+      if (TREE_CODE (arg1) == NEGATE_EXPR)
+	return fold_build2 (MINUS_EXPR, type,
+			    fold_convert (type, arg0),
+			    fold_convert (type, TREE_OPERAND (arg1, 0)));
+      /* (-A) + B -> B - A */
+      if (TREE_CODE (arg0) == NEGATE_EXPR
+	  && reorder_operands_p (TREE_OPERAND (arg0, 0), arg1))
+	return fold_build2 (MINUS_EXPR, type,
+			    fold_convert (type, arg1),
+			    fold_convert (type, TREE_OPERAND (arg0, 0)));
+      /* Convert ~A + 1 to -A.  */
+      if (INTEGRAL_TYPE_P (type)
+	  && TREE_CODE (arg0) == BIT_NOT_EXPR
+	  && integer_onep (arg1))
+	return fold_build1 (NEGATE_EXPR, type, TREE_OPERAND (arg0, 0));
+
+      if (! FLOAT_TYPE_P (type))
+	{
+	  if (integer_zerop (arg1))
+	    return non_lvalue (fold_convert (type, arg0));
+
+	  /* If we are adding two BIT_AND_EXPR's, both of which are and'ing
+	     with a constant, and the two constants have no bits in common,
+	     we should treat this as a BIT_IOR_EXPR since this may produce more
+	     simplifications.  */
+	  if (TREE_CODE (arg0) == BIT_AND_EXPR
+	      && TREE_CODE (arg1) == BIT_AND_EXPR
+	      && TREE_CODE (TREE_OPERAND (arg0, 1)) == INTEGER_CST
+	      && TREE_CODE (TREE_OPERAND (arg1, 1)) == INTEGER_CST
+	      && integer_zerop (const_binop (BIT_AND_EXPR,
+					     TREE_OPERAND (arg0, 1),
+					     TREE_OPERAND (arg1, 1), 0)))
+	    {
+	      code = BIT_IOR_EXPR;
+	      goto bit_ior;
+	    }
+
+	  /* Reassociate (plus (plus (mult) (foo)) (mult)) as
+	     (plus (plus (mult) (mult)) (foo)) so that we can
+	     take advantage of the factoring cases below.  */
+	  if (((TREE_CODE (arg0) == PLUS_EXPR
+		|| TREE_CODE (arg0) == MINUS_EXPR)
+	       && TREE_CODE (arg1) == MULT_EXPR)
+	      || ((TREE_CODE (arg1) == PLUS_EXPR
+		   || TREE_CODE (arg1) == MINUS_EXPR)
+		  && TREE_CODE (arg0) == MULT_EXPR))
+	    {
+	      tree parg0, parg1, parg, marg;
+	      enum tree_code pcode;
+
+	      if (TREE_CODE (arg1) == MULT_EXPR)
+		parg = arg0, marg = arg1;
+	      else
+		parg = arg1, marg = arg0;
+	      pcode = TREE_CODE (parg);
+	      parg0 = TREE_OPERAND (parg, 0);
+	      parg1 = TREE_OPERAND (parg, 1);
+	      STRIP_NOPS (parg0);
+	      STRIP_NOPS (parg1);
+
+	      if (TREE_CODE (parg0) == MULT_EXPR
+		  && TREE_CODE (parg1) != MULT_EXPR)
+		return fold_build2 (pcode, type,
+				    fold_build2 (PLUS_EXPR, type,
+						 fold_convert (type, parg0),
+						 fold_convert (type, marg)),
+				    fold_convert (type, parg1));
+	      if (TREE_CODE (parg0) != MULT_EXPR
+		  && TREE_CODE (parg1) == MULT_EXPR)
+		return fold_build2 (PLUS_EXPR, type,
+				    fold_convert (type, parg0),
+				    fold_build2 (pcode, type,
+						 fold_convert (type, marg),
+						 fold_convert (type,
+							       parg1)));
+	    }
+
+	  if (TREE_CODE (arg0) == MULT_EXPR && TREE_CODE (arg1) == MULT_EXPR)
+	    {
+	      tree arg00, arg01, arg10, arg11;
+	      tree alt0 = NULL_TREE, alt1 = NULL_TREE, same;
+
+	      /* (A * C) + (B * C) -> (A+B) * C.
+		 We are most concerned about the case where C is a constant,
+		 but other combinations show up during loop reduction.  Since
+		 it is not difficult, try all four possibilities.  */
+
+	      arg00 = TREE_OPERAND (arg0, 0);
+	      arg01 = TREE_OPERAND (arg0, 1);
+	      arg10 = TREE_OPERAND (arg1, 0);
+	      arg11 = TREE_OPERAND (arg1, 1);
+	      same = NULL_TREE;
+
+	      if (operand_equal_p (arg01, arg11, 0))
+		same = arg01, alt0 = arg00, alt1 = arg10;
+	      else if (operand_equal_p (arg00, arg10, 0))
+		same = arg00, alt0 = arg01, alt1 = arg11;
+	      else if (operand_equal_p (arg00, arg11, 0))
+		same = arg00, alt0 = arg01, alt1 = arg10;
+	      else if (operand_equal_p (arg01, arg10, 0))
+		same = arg01, alt0 = arg00, alt1 = arg11;
+
+	      /* No identical multiplicands; see if we can find a common
+		 power-of-two factor in non-power-of-two multiplies.  This
+		 can help in multi-dimensional array access.  */
+	      else if (TREE_CODE (arg01) == INTEGER_CST
+		       && TREE_CODE (arg11) == INTEGER_CST
+		       && TREE_INT_CST_HIGH (arg01) == 0
+		       && TREE_INT_CST_HIGH (arg11) == 0)
+		{
+		  HOST_WIDE_INT int01, int11, tmp;
+		  int01 = TREE_INT_CST_LOW (arg01);
+		  int11 = TREE_INT_CST_LOW (arg11);
+
+		  /* Move min of absolute values to int11.  */
+		  if ((int01 >= 0 ? int01 : -int01)
+		      < (int11 >= 0 ? int11 : -int11))
+		    {
+		      tmp = int01, int01 = int11, int11 = tmp;
+		      alt0 = arg00, arg00 = arg10, arg10 = alt0;
+		      alt0 = arg01, arg01 = arg11, arg11 = alt0;
+		    }
+
+		  if (exact_log2 (int11) > 0 && int01 % int11 == 0)
+		    {
+		      alt0 = fold_build2 (MULT_EXPR, type, arg00,
+					  build_int_cst (NULL_TREE,
+							 int01 / int11));
+		      alt1 = arg10;
+		      same = arg11;
+		    }
+		}
+
+	      if (same)
+		return fold_build2 (MULT_EXPR, type,
+				    fold_build2 (PLUS_EXPR, type,
+						 fold_convert (type, alt0),
+						 fold_convert (type, alt1)),
+				    fold_convert (type, same));
+	    }
+
+	  /* Try replacing &a[i1] + c * i2 with &a[i1 + i2], if c is step
+	     of the array.  Loop optimizer sometimes produce this type of
+	     expressions.  */
+	  if (TREE_CODE (arg0) == ADDR_EXPR)
+	    {
+	      tem = try_move_mult_to_index (PLUS_EXPR, arg0, arg1);
+	      if (tem)
+		return fold_convert (type, tem);
+	    }
+	  else if (TREE_CODE (arg1) == ADDR_EXPR)
+	    {
+	      tem = try_move_mult_to_index (PLUS_EXPR, arg1, arg0);
+	      if (tem)
+		return fold_convert (type, tem);
+	    }
+	}
+      else
+	{
+	  /* See if ARG1 is zero and X + ARG1 reduces to X.  */
+	  if (fold_real_zero_addition_p (TREE_TYPE (arg0), arg1, 0))
+	    return non_lvalue (fold_convert (type, arg0));
+
+	  /* Likewise if the operands are reversed.  */
+	  if (fold_real_zero_addition_p (TREE_TYPE (arg1), arg0, 0))
+	    return non_lvalue (fold_convert (type, arg1));
+
+	  /* Convert X + -C into X - C.  */
+	  if (TREE_CODE (arg1) == REAL_CST
+	      && REAL_VALUE_NEGATIVE (TREE_REAL_CST (arg1)))
+	    {
+	      tem = fold_negate_const (arg1, type);
+	      if (!TREE_OVERFLOW (arg1) || !flag_trapping_math)
+		return fold_build2 (MINUS_EXPR, type,
+				    fold_convert (type, arg0),
+				    fold_convert (type, tem));
+	    }
+
+          if (flag_unsafe_math_optimizations
+	      && (TREE_CODE (arg0) == RDIV_EXPR || TREE_CODE (arg0) == MULT_EXPR)
+	      && (TREE_CODE (arg1) == RDIV_EXPR || TREE_CODE (arg1) == MULT_EXPR)
+	      && (tem = distribute_real_division (code, type, arg0, arg1)))
+	    return tem;
+
+	  /* Convert x+x into x*2.0.  */
+	  if (operand_equal_p (arg0, arg1, 0)
+	      && SCALAR_FLOAT_TYPE_P (type))
+	    return fold_build2 (MULT_EXPR, type, arg0,
+				build_real (type, dconst2));
+
+	  /* Convert x*c+x into x*(c+1).  */
+	  if (flag_unsafe_math_optimizations
+	      && TREE_CODE (arg0) == MULT_EXPR
+	      && TREE_CODE (TREE_OPERAND (arg0, 1)) == REAL_CST
+	      && ! TREE_CONSTANT_OVERFLOW (TREE_OPERAND (arg0, 1))
+	      && operand_equal_p (TREE_OPERAND (arg0, 0), arg1, 0))
+	    {
+	      REAL_VALUE_TYPE c;
+
+	      c = TREE_REAL_CST (TREE_OPERAND (arg0, 1));
+	      real_arithmetic (&c, PLUS_EXPR, &c, &dconst1);
+	      return fold_build2 (MULT_EXPR, type, arg1,
+				  build_real (type, c));
+	    }
+
+	  /* Convert x+x*c into x*(c+1).  */
+	  if (flag_unsafe_math_optimizations
+	      && TREE_CODE (arg1) == MULT_EXPR
+	      && TREE_CODE (TREE_OPERAND (arg1, 1)) == REAL_CST
+	      && ! TREE_CONSTANT_OVERFLOW (TREE_OPERAND (arg1, 1))
+	      && operand_equal_p (TREE_OPERAND (arg1, 0), arg0, 0))
+	    {
+	      REAL_VALUE_TYPE c;
+
+	      c = TREE_REAL_CST (TREE_OPERAND (arg1, 1));
+	      real_arithmetic (&c, PLUS_EXPR, &c, &dconst1);
+	      return fold_build2 (MULT_EXPR, type, arg0,
+				  build_real (type, c));
+	    }
+
+	  /* Convert x*c1+x*c2 into x*(c1+c2).  */
+	  if (flag_unsafe_math_optimizations
+	      && TREE_CODE (arg0) == MULT_EXPR
+	      && TREE_CODE (arg1) == MULT_EXPR
+	      && TREE_CODE (TREE_OPERAND (arg0, 1)) == REAL_CST
+	      && ! TREE_CONSTANT_OVERFLOW (TREE_OPERAND (arg0, 1))
+	      && TREE_CODE (TREE_OPERAND (arg1, 1)) == REAL_CST
+	      && ! TREE_CONSTANT_OVERFLOW (TREE_OPERAND (arg1, 1))
+	      && operand_equal_p (TREE_OPERAND (arg0, 0),
+				  TREE_OPERAND (arg1, 0), 0))
+	    {
+	      REAL_VALUE_TYPE c1, c2;
+
+	      c1 = TREE_REAL_CST (TREE_OPERAND (arg0, 1));
+	      c2 = TREE_REAL_CST (TREE_OPERAND (arg1, 1));
+	      real_arithmetic (&c1, PLUS_EXPR, &c1, &c2);
+	      return fold_build2 (MULT_EXPR, type,
+				  TREE_OPERAND (arg0, 0),
+				  build_real (type, c1));
+	    }
+          /* Convert a + (b*c + d*e) into (a + b*c) + d*e.  */
+          if (flag_unsafe_math_optimizations
+              && TREE_CODE (arg1) == PLUS_EXPR
+              && TREE_CODE (arg0) != MULT_EXPR)
+            {
+              tree tree10 = TREE_OPERAND (arg1, 0);
+              tree tree11 = TREE_OPERAND (arg1, 1);
+              if (TREE_CODE (tree11) == MULT_EXPR
+		  && TREE_CODE (tree10) == MULT_EXPR)
+                {
+                  tree tree0;
+                  tree0 = fold_build2 (PLUS_EXPR, type, arg0, tree10);
+                  return fold_build2 (PLUS_EXPR, type, tree0, tree11);
+                }
+            }
+          /* Convert (b*c + d*e) + a into b*c + (d*e +a).  */
+          if (flag_unsafe_math_optimizations
+              && TREE_CODE (arg0) == PLUS_EXPR
+              && TREE_CODE (arg1) != MULT_EXPR)
+            {
+              tree tree00 = TREE_OPERAND (arg0, 0);
+              tree tree01 = TREE_OPERAND (arg0, 1);
+              if (TREE_CODE (tree01) == MULT_EXPR
+		  && TREE_CODE (tree00) == MULT_EXPR)
+                {
+                  tree tree0;
+                  tree0 = fold_build2 (PLUS_EXPR, type, tree01, arg1);
+                  return fold_build2 (PLUS_EXPR, type, tree00, tree0);
+                }
+            }
+	}
+
+     bit_rotate:
+      /* (A << C1) + (A >> C2) if A is unsigned and C1+C2 is the size of A
+	 is a rotate of A by C1 bits.  */
+      /* (A << B) + (A >> (Z - B)) if A is unsigned and Z is the size of A
+	 is a rotate of A by B bits.  */
+      {
+	enum tree_code code0, code1;
+	code0 = TREE_CODE (arg0);
+	code1 = TREE_CODE (arg1);
+	if (((code0 == RSHIFT_EXPR && code1 == LSHIFT_EXPR)
+	     || (code1 == RSHIFT_EXPR && code0 == LSHIFT_EXPR))
+	    && operand_equal_p (TREE_OPERAND (arg0, 0),
+			        TREE_OPERAND (arg1, 0), 0)
+	    && TYPE_UNSIGNED (TREE_TYPE (TREE_OPERAND (arg0, 0))))
+	  {
+	    tree tree01, tree11;
+	    enum tree_code code01, code11;
+
+	    tree01 = TREE_OPERAND (arg0, 1);
+	    tree11 = TREE_OPERAND (arg1, 1);
+	    STRIP_NOPS (tree01);
+	    STRIP_NOPS (tree11);
+	    code01 = TREE_CODE (tree01);
+	    code11 = TREE_CODE (tree11);
+	    if (code01 == INTEGER_CST
+		&& code11 == INTEGER_CST
+		&& TREE_INT_CST_HIGH (tree01) == 0
+		&& TREE_INT_CST_HIGH (tree11) == 0
+		&& ((TREE_INT_CST_LOW (tree01) + TREE_INT_CST_LOW (tree11))
+		    == TYPE_PRECISION (TREE_TYPE (TREE_OPERAND (arg0, 0)))))
+	      return build2 (LROTATE_EXPR, type, TREE_OPERAND (arg0, 0),
+			     code0 == LSHIFT_EXPR ? tree01 : tree11);
+	    else if (code11 == MINUS_EXPR)
+	      {
+		tree tree110, tree111;
+		tree110 = TREE_OPERAND (tree11, 0);
+		tree111 = TREE_OPERAND (tree11, 1);
+		STRIP_NOPS (tree110);
+		STRIP_NOPS (tree111);
+		if (TREE_CODE (tree110) == INTEGER_CST
+		    && 0 == compare_tree_int (tree110,
+					      TYPE_PRECISION
+					      (TREE_TYPE (TREE_OPERAND
+							  (arg0, 0))))
+		    && operand_equal_p (tree01, tree111, 0))
+		  return build2 ((code0 == LSHIFT_EXPR
+				  ? LROTATE_EXPR
+				  : RROTATE_EXPR),
+				 type, TREE_OPERAND (arg0, 0), tree01);
+	      }
+	    else if (code01 == MINUS_EXPR)
+	      {
+		tree tree010, tree011;
+		tree010 = TREE_OPERAND (tree01, 0);
+		tree011 = TREE_OPERAND (tree01, 1);
+		STRIP_NOPS (tree010);
+		STRIP_NOPS (tree011);
+		if (TREE_CODE (tree010) == INTEGER_CST
+		    && 0 == compare_tree_int (tree010,
+					      TYPE_PRECISION
+					      (TREE_TYPE (TREE_OPERAND
+							  (arg0, 0))))
+		    && operand_equal_p (tree11, tree011, 0))
+		  return build2 ((code0 != LSHIFT_EXPR
+				  ? LROTATE_EXPR
+				  : RROTATE_EXPR),
+				 type, TREE_OPERAND (arg0, 0), tree11);
+	      }
+	  }
+      }
+
+    associate:
+      /* In most languages, can't associate operations on floats through
+	 parentheses.  Rather than remember where the parentheses were, we
+	 don't associate floats at all, unless the user has specified
+	 -funsafe-math-optimizations.  */
+
+      if (! wins
+	  && (! FLOAT_TYPE_P (type) || flag_unsafe_math_optimizations))
+	{
+	  tree var0, con0, lit0, minus_lit0;
+	  tree var1, con1, lit1, minus_lit1;
+
+	  /* Split both trees into variables, constants, and literals.  Then
+	     associate each group together, the constants with literals,
+	     then the result with variables.  This increases the chances of
+	     literals being recombined later and of generating relocatable
+	     expressions for the sum of a constant and literal.  */
+	  var0 = split_tree (arg0, code, &con0, &lit0, &minus_lit0, 0);
+	  var1 = split_tree (arg1, code, &con1, &lit1, &minus_lit1,
+			     code == MINUS_EXPR);
+
+	  /* Only do something if we found more than two objects.  Otherwise,
+	     nothing has changed and we risk infinite recursion.  */
+	  if (2 < ((var0 != 0) + (var1 != 0)
+		   + (con0 != 0) + (con1 != 0)
+		   + (lit0 != 0) + (lit1 != 0)
+		   + (minus_lit0 != 0) + (minus_lit1 != 0)))
+	    {
+	      /* Recombine MINUS_EXPR operands by using PLUS_EXPR.  */
+	      if (code == MINUS_EXPR)
+		code = PLUS_EXPR;
+
+	      var0 = associate_trees (var0, var1, code, type);
+	      con0 = associate_trees (con0, con1, code, type);
+	      lit0 = associate_trees (lit0, lit1, code, type);
+	      minus_lit0 = associate_trees (minus_lit0, minus_lit1, code, type);
+
+	      /* Preserve the MINUS_EXPR if the negative part of the literal is
+		 greater than the positive part.  Otherwise, the multiplicative
+		 folding code (i.e extract_muldiv) may be fooled in case
+		 unsigned constants are subtracted, like in the following
+		 example: ((X*2 + 4) - 8U)/2.  */
+	      if (minus_lit0 && lit0)
+		{
+		  if (TREE_CODE (lit0) == INTEGER_CST
+		      && TREE_CODE (minus_lit0) == INTEGER_CST
+		      && tree_int_cst_lt (lit0, minus_lit0))
+		    {
+		      minus_lit0 = associate_trees (minus_lit0, lit0,
+						    MINUS_EXPR, type);
+		      lit0 = 0;
+		    }
+		  else
+		    {
+		      lit0 = associate_trees (lit0, minus_lit0,
+					      MINUS_EXPR, type);
+		      minus_lit0 = 0;
+		    }
+		}
+	      if (minus_lit0)
+		{
+		  if (con0 == 0)
+		    return fold_convert (type,
+					 associate_trees (var0, minus_lit0,
+							  MINUS_EXPR, type));
+		  else
+		    {
+		      con0 = associate_trees (con0, minus_lit0,
+					      MINUS_EXPR, type);
+		      return fold_convert (type,
+					   associate_trees (var0, con0,
+							    PLUS_EXPR, type));
+		    }
+		}
+
+	      con0 = associate_trees (con0, lit0, code, type);
+	      return fold_convert (type, associate_trees (var0, con0,
+							  code, type));
+	    }
+	}
+
+    binary:
+      if (wins)
+	t1 = const_binop (code, arg0, arg1, 0);
+      if (t1 != NULL_TREE)
+	{
+	  /* The return value should always have
+	     the same type as the original expression.  */
+	  if (TREE_TYPE (t1) != type)
+	    t1 = fold_convert (type, t1);
+
+	  return t1;
+	}
+      return NULL_TREE;
+
+    case MINUS_EXPR:
+      /* A - (-B) -> A + B */
+      if (TREE_CODE (arg1) == NEGATE_EXPR)
+	return fold_build2 (PLUS_EXPR, type, arg0, TREE_OPERAND (arg1, 0));
+      /* (-A) - B -> (-B) - A  where B is easily negated and we can swap.  */
+      if (TREE_CODE (arg0) == NEGATE_EXPR
+	  && (FLOAT_TYPE_P (type)
+	      || (INTEGRAL_TYPE_P (type) && flag_wrapv && !flag_trapv))
+	  && negate_expr_p (arg1)
+	  && reorder_operands_p (arg0, arg1))
+	return fold_build2 (MINUS_EXPR, type, negate_expr (arg1),
+			    TREE_OPERAND (arg0, 0));
+      /* Convert -A - 1 to ~A.  */
+      if (INTEGRAL_TYPE_P (type)
+	  && TREE_CODE (arg0) == NEGATE_EXPR
+	  && integer_onep (arg1))
+	return fold_build1 (BIT_NOT_EXPR, type, TREE_OPERAND (arg0, 0));
+
+      /* Convert -1 - A to ~A.  */
+      if (INTEGRAL_TYPE_P (type)
+	  && integer_all_onesp (arg0))
+	return fold_build1 (BIT_NOT_EXPR, type, arg1);
+
+      if (! FLOAT_TYPE_P (type))
+	{
+	  if (! wins && integer_zerop (arg0))
+	    return negate_expr (fold_convert (type, arg1));
+	  if (integer_zerop (arg1))
+	    return non_lvalue (fold_convert (type, arg0));
+
+	  /* Fold A - (A & B) into ~B & A.  */
+	  if (!TREE_SIDE_EFFECTS (arg0)
+	      && TREE_CODE (arg1) == BIT_AND_EXPR)
+	    {
+	      if (operand_equal_p (arg0, TREE_OPERAND (arg1, 1), 0))
+		return fold_build2 (BIT_AND_EXPR, type,
+				    fold_build1 (BIT_NOT_EXPR, type,
+						 TREE_OPERAND (arg1, 0)),
+				    arg0);
+	      if (operand_equal_p (arg0, TREE_OPERAND (arg1, 0), 0))
+		return fold_build2 (BIT_AND_EXPR, type,
+				    fold_build1 (BIT_NOT_EXPR, type,
+						 TREE_OPERAND (arg1, 1)),
+				    arg0);
+	    }
+
+	  /* Fold (A & ~B) - (A & B) into (A ^ B) - B, where B is
+	     any power of 2 minus 1.  */
+	  if (TREE_CODE (arg0) == BIT_AND_EXPR
+	      && TREE_CODE (arg1) == BIT_AND_EXPR
+	      && operand_equal_p (TREE_OPERAND (arg0, 0),
+				  TREE_OPERAND (arg1, 0), 0))
+	    {
+	      tree mask0 = TREE_OPERAND (arg0, 1);
+	      tree mask1 = TREE_OPERAND (arg1, 1);
+	      tree tem = fold_build1 (BIT_NOT_EXPR, type, mask0);
+
+	      if (operand_equal_p (tem, mask1, 0))
+		{
+		  tem = fold_build2 (BIT_XOR_EXPR, type,
+				     TREE_OPERAND (arg0, 0), mask1);
+		  return fold_build2 (MINUS_EXPR, type, tem, mask1);
+		}
+	    }
+	}
+
+      /* See if ARG1 is zero and X - ARG1 reduces to X.  */
+      else if (fold_real_zero_addition_p (TREE_TYPE (arg0), arg1, 1))
+	return non_lvalue (fold_convert (type, arg0));
+
+      /* (ARG0 - ARG1) is the same as (-ARG1 + ARG0).  So check whether
+	 ARG0 is zero and X + ARG0 reduces to X, since that would mean
+	 (-ARG1 + ARG0) reduces to -ARG1.  */
+      else if (!wins && fold_real_zero_addition_p (TREE_TYPE (arg1), arg0, 0))
+	return negate_expr (fold_convert (type, arg1));
+
+      /* Fold &x - &x.  This can happen from &x.foo - &x.
+	 This is unsafe for certain floats even in non-IEEE formats.
+	 In IEEE, it is unsafe because it does wrong for NaNs.
+	 Also note that operand_equal_p is always false if an operand
+	 is volatile.  */
+
+      if ((! FLOAT_TYPE_P (type) || flag_unsafe_math_optimizations)
+	  && operand_equal_p (arg0, arg1, 0))
+	return fold_convert (type, integer_zero_node);
+
+      /* A - B -> A + (-B) if B is easily negatable.  */
+      if (!wins && negate_expr_p (arg1)
+	  && ((FLOAT_TYPE_P (type)
+               /* Avoid this transformation if B is a positive REAL_CST.  */
+	       && (TREE_CODE (arg1) != REAL_CST
+		   ||  REAL_VALUE_NEGATIVE (TREE_REAL_CST (arg1))))
+	      || (INTEGRAL_TYPE_P (type) && flag_wrapv && !flag_trapv)))
+	return fold_build2 (PLUS_EXPR, type,
+			    fold_convert (type, arg0),
+			    fold_convert (type, negate_expr (arg1)));
+
+      /* Try folding difference of addresses.  */
+      {
+	HOST_WIDE_INT diff;
+
+	if ((TREE_CODE (arg0) == ADDR_EXPR
+	     || TREE_CODE (arg1) == ADDR_EXPR)
+	    && ptr_difference_const (arg0, arg1, &diff))
+	  return build_int_cst_type (type, diff);
+      }
+
+      /* Fold &a[i] - &a[j] to i-j.  */
+      if (TREE_CODE (arg0) == ADDR_EXPR
+	  && TREE_CODE (TREE_OPERAND (arg0, 0)) == ARRAY_REF
+	  && TREE_CODE (arg1) == ADDR_EXPR
+	  && TREE_CODE (TREE_OPERAND (arg1, 0)) == ARRAY_REF)
+        {
+	  tree aref0 = TREE_OPERAND (arg0, 0);
+	  tree aref1 = TREE_OPERAND (arg1, 0);
+	  if (operand_equal_p (TREE_OPERAND (aref0, 0),
+			       TREE_OPERAND (aref1, 0), 0))
+	    {
+	      tree op0 = fold_convert (type, TREE_OPERAND (aref0, 1));
+	      tree op1 = fold_convert (type, TREE_OPERAND (aref1, 1));
+	      tree esz = array_ref_element_size (aref0);
+	      tree diff = build2 (MINUS_EXPR, type, op0, op1);
+	      return fold_build2 (MULT_EXPR, type, diff,
+			          fold_convert (type, esz));
+			          
+	    }
+	}
+
+      /* Try replacing &a[i1] - c * i2 with &a[i1 - i2], if c is step
+	 of the array.  Loop optimizer sometimes produce this type of
+	 expressions.  */
+      if (TREE_CODE (arg0) == ADDR_EXPR)
+	{
+	  tem = try_move_mult_to_index (MINUS_EXPR, arg0, arg1);
+	  if (tem)
+	    return fold_convert (type, tem);
+	}
+
+      if (flag_unsafe_math_optimizations
+	  && (TREE_CODE (arg0) == RDIV_EXPR || TREE_CODE (arg0) == MULT_EXPR)
+	  && (TREE_CODE (arg1) == RDIV_EXPR || TREE_CODE (arg1) == MULT_EXPR)
+	  && (tem = distribute_real_division (code, type, arg0, arg1)))
+	return tem;
+
+      if (TREE_CODE (arg0) == MULT_EXPR
+	  && TREE_CODE (arg1) == MULT_EXPR
+	  && (!FLOAT_TYPE_P (type) || flag_unsafe_math_optimizations))
+	{
+          /* (A * C) - (B * C) -> (A-B) * C.  */
+	  if (operand_equal_p (TREE_OPERAND (arg0, 1),
+			       TREE_OPERAND (arg1, 1), 0))
+	    return fold_build2 (MULT_EXPR, type,
+				fold_build2 (MINUS_EXPR, type,
+					     TREE_OPERAND (arg0, 0),
+					     TREE_OPERAND (arg1, 0)),
+				TREE_OPERAND (arg0, 1));
+          /* (A * C1) - (A * C2) -> A * (C1-C2).  */
+	  if (operand_equal_p (TREE_OPERAND (arg0, 0),
+			       TREE_OPERAND (arg1, 0), 0))
+	    return fold_build2 (MULT_EXPR, type,
+				TREE_OPERAND (arg0, 0),
+				fold_build2 (MINUS_EXPR, type,
+					     TREE_OPERAND (arg0, 1),
+					     TREE_OPERAND (arg1, 1)));
+	}
+
+      goto associate;
+
+    case MULT_EXPR:
+      /* (-A) * (-B) -> A * B  */
+      if (TREE_CODE (arg0) == NEGATE_EXPR && negate_expr_p (arg1))
+	return fold_build2 (MULT_EXPR, type,
+			    TREE_OPERAND (arg0, 0),
+			    negate_expr (arg1));
+      if (TREE_CODE (arg1) == NEGATE_EXPR && negate_expr_p (arg0))
+	return fold_build2 (MULT_EXPR, type,
+			    negate_expr (arg0),
+			    TREE_OPERAND (arg1, 0));
+
+      if (! FLOAT_TYPE_P (type))
+	{
+	  if (integer_zerop (arg1))
+	    return omit_one_operand (type, arg1, arg0);
+	  if (integer_onep (arg1))
+	    return non_lvalue (fold_convert (type, arg0));
+	  /* Transform x * -1 into -x.  */
+	  if (integer_all_onesp (arg1))
+	    return fold_convert (type, negate_expr (arg0));
+
+	  /* (a * (1 << b)) is (a << b)  */
+	  if (TREE_CODE (arg1) == LSHIFT_EXPR
+	      && integer_onep (TREE_OPERAND (arg1, 0)))
+	    return fold_build2 (LSHIFT_EXPR, type, arg0,
+				TREE_OPERAND (arg1, 1));
+	  if (TREE_CODE (arg0) == LSHIFT_EXPR
+	      && integer_onep (TREE_OPERAND (arg0, 0)))
+	    return fold_build2 (LSHIFT_EXPR, type, arg1,
+				TREE_OPERAND (arg0, 1));
+
+	  if (TREE_CODE (arg1) == INTEGER_CST
+	      && 0 != (tem = extract_muldiv (op0,
+					     fold_convert (type, arg1),
+					     code, NULL_TREE)))
+	    return fold_convert (type, tem);
+
+	}
+      else
+	{
+	  /* Maybe fold x * 0 to 0.  The expressions aren't the same
+	     when x is NaN, since x * 0 is also NaN.  Nor are they the
+	     same in modes with signed zeros, since multiplying a
+	     negative value by 0 gives -0, not +0.  */
+	  if (!HONOR_NANS (TYPE_MODE (TREE_TYPE (arg0)))
+	      && !HONOR_SIGNED_ZEROS (TYPE_MODE (TREE_TYPE (arg0)))
+	      && real_zerop (arg1))
+	    return omit_one_operand (type, arg1, arg0);
+	  /* In IEEE floating point, x*1 is not equivalent to x for snans.  */
+	  if (!HONOR_SNANS (TYPE_MODE (TREE_TYPE (arg0)))
+	      && real_onep (arg1))
+	    return non_lvalue (fold_convert (type, arg0));
+
+	  /* Transform x * -1.0 into -x.  */
+	  if (!HONOR_SNANS (TYPE_MODE (TREE_TYPE (arg0)))
+	      && real_minus_onep (arg1))
+	    return fold_convert (type, negate_expr (arg0));
+
+	  /* Convert (C1/X)*C2 into (C1*C2)/X.  */
+	  if (flag_unsafe_math_optimizations
+	      && TREE_CODE (arg0) == RDIV_EXPR
+	      && TREE_CODE (arg1) == REAL_CST
+	      && TREE_CODE (TREE_OPERAND (arg0, 0)) == REAL_CST)
+	    {
+	      tree tem = const_binop (MULT_EXPR, TREE_OPERAND (arg0, 0),
+				      arg1, 0);
+	      if (tem)
+		return fold_build2 (RDIV_EXPR, type, tem,
+				    TREE_OPERAND (arg0, 1));
+	    }
+
+          /* Strip sign operations from X in X*X, i.e. -Y*-Y -> Y*Y.  */
+	  if (operand_equal_p (arg0, arg1, 0))
+	    {
+	      tree tem = fold_strip_sign_ops (arg0);
+	      if (tem != NULL_TREE)
+		{
+		  tem = fold_convert (type, tem);
+		  return fold_build2 (MULT_EXPR, type, tem, tem);
+		}
+	    }
+
+	  if (flag_unsafe_math_optimizations)
+	    {
+	      enum built_in_function fcode0 = builtin_mathfn_code (arg0);
+	      enum built_in_function fcode1 = builtin_mathfn_code (arg1);
+
+	      /* Optimizations of root(...)*root(...).  */
+	      if (fcode0 == fcode1 && BUILTIN_ROOT_P (fcode0))
+		{
+		  tree rootfn, arg, arglist;
+		  tree arg00 = TREE_VALUE (TREE_OPERAND (arg0, 1));
+		  tree arg10 = TREE_VALUE (TREE_OPERAND (arg1, 1));
+
+		  /* Optimize sqrt(x)*sqrt(x) as x.  */
+		  if (BUILTIN_SQRT_P (fcode0)
+		      && operand_equal_p (arg00, arg10, 0)
+		      && ! HONOR_SNANS (TYPE_MODE (type)))
+		    return arg00;
+
+	          /* Optimize root(x)*root(y) as root(x*y).  */
+		  rootfn = TREE_OPERAND (TREE_OPERAND (arg0, 0), 0);
+		  arg = fold_build2 (MULT_EXPR, type, arg00, arg10);
+		  arglist = build_tree_list (NULL_TREE, arg);
+		  return build_function_call_expr (rootfn, arglist);
+		}
+
+	      /* Optimize expN(x)*expN(y) as expN(x+y).  */
+	      if (fcode0 == fcode1 && BUILTIN_EXPONENT_P (fcode0))
+		{
+		  tree expfn = TREE_OPERAND (TREE_OPERAND (arg0, 0), 0);
+		  tree arg = fold_build2 (PLUS_EXPR, type,
+					  TREE_VALUE (TREE_OPERAND (arg0, 1)),
+					  TREE_VALUE (TREE_OPERAND (arg1, 1)));
+		  tree arglist = build_tree_list (NULL_TREE, arg);
+		  return build_function_call_expr (expfn, arglist);
+		}
+
+	      /* Optimizations of pow(...)*pow(...).  */
+	      if ((fcode0 == BUILT_IN_POW && fcode1 == BUILT_IN_POW)
+		  || (fcode0 == BUILT_IN_POWF && fcode1 == BUILT_IN_POWF)
+		  || (fcode0 == BUILT_IN_POWL && fcode1 == BUILT_IN_POWL))
+		{
+		  tree arg00 = TREE_VALUE (TREE_OPERAND (arg0, 1));
+		  tree arg01 = TREE_VALUE (TREE_CHAIN (TREE_OPERAND (arg0,
+								     1)));
+		  tree arg10 = TREE_VALUE (TREE_OPERAND (arg1, 1));
+		  tree arg11 = TREE_VALUE (TREE_CHAIN (TREE_OPERAND (arg1,
+								     1)));
+
+		  /* Optimize pow(x,y)*pow(z,y) as pow(x*z,y).  */
+		  if (operand_equal_p (arg01, arg11, 0))
+		    {
+		      tree powfn = TREE_OPERAND (TREE_OPERAND (arg0, 0), 0);
+		      tree arg = fold_build2 (MULT_EXPR, type, arg00, arg10);
+		      tree arglist = tree_cons (NULL_TREE, arg,
+						build_tree_list (NULL_TREE,
+								 arg01));
+		      return build_function_call_expr (powfn, arglist);
+		    }
+
+		  /* Optimize pow(x,y)*pow(x,z) as pow(x,y+z).  */
+		  if (operand_equal_p (arg00, arg10, 0))
+		    {
+		      tree powfn = TREE_OPERAND (TREE_OPERAND (arg0, 0), 0);
+		      tree arg = fold_build2 (PLUS_EXPR, type, arg01, arg11);
+		      tree arglist = tree_cons (NULL_TREE, arg00,
+						build_tree_list (NULL_TREE,
+								 arg));
+		      return build_function_call_expr (powfn, arglist);
+		    }
+		}
+
+	      /* Optimize tan(x)*cos(x) as sin(x).  */
+	      if (((fcode0 == BUILT_IN_TAN && fcode1 == BUILT_IN_COS)
+		   || (fcode0 == BUILT_IN_TANF && fcode1 == BUILT_IN_COSF)
+		   || (fcode0 == BUILT_IN_TANL && fcode1 == BUILT_IN_COSL)
+		   || (fcode0 == BUILT_IN_COS && fcode1 == BUILT_IN_TAN)
+		   || (fcode0 == BUILT_IN_COSF && fcode1 == BUILT_IN_TANF)
+		   || (fcode0 == BUILT_IN_COSL && fcode1 == BUILT_IN_TANL))
+		  && operand_equal_p (TREE_VALUE (TREE_OPERAND (arg0, 1)),
+				      TREE_VALUE (TREE_OPERAND (arg1, 1)), 0))
+		{
+		  tree sinfn = mathfn_built_in (type, BUILT_IN_SIN);
+
+		  if (sinfn != NULL_TREE)
+		    return build_function_call_expr (sinfn,
+						     TREE_OPERAND (arg0, 1));
+		}
+
+	      /* Optimize x*pow(x,c) as pow(x,c+1).  */
+	      if (fcode1 == BUILT_IN_POW
+		  || fcode1 == BUILT_IN_POWF
+		  || fcode1 == BUILT_IN_POWL)
+		{
+		  tree arg10 = TREE_VALUE (TREE_OPERAND (arg1, 1));
+		  tree arg11 = TREE_VALUE (TREE_CHAIN (TREE_OPERAND (arg1,
+								     1)));
+		  if (TREE_CODE (arg11) == REAL_CST
+		      && ! TREE_CONSTANT_OVERFLOW (arg11)
+		      && operand_equal_p (arg0, arg10, 0))
+		    {
+		      tree powfn = TREE_OPERAND (TREE_OPERAND (arg1, 0), 0);
+		      REAL_VALUE_TYPE c;
+		      tree arg, arglist;
+
+		      c = TREE_REAL_CST (arg11);
+		      real_arithmetic (&c, PLUS_EXPR, &c, &dconst1);
+		      arg = build_real (type, c);
+		      arglist = build_tree_list (NULL_TREE, arg);
+		      arglist = tree_cons (NULL_TREE, arg0, arglist);
+		      return build_function_call_expr (powfn, arglist);
+		    }
+		}
+
+	      /* Optimize pow(x,c)*x as pow(x,c+1).  */
+	      if (fcode0 == BUILT_IN_POW
+		  || fcode0 == BUILT_IN_POWF
+		  || fcode0 == BUILT_IN_POWL)
+		{
+		  tree arg00 = TREE_VALUE (TREE_OPERAND (arg0, 1));
+		  tree arg01 = TREE_VALUE (TREE_CHAIN (TREE_OPERAND (arg0,
+								     1)));
+		  if (TREE_CODE (arg01) == REAL_CST
+		      && ! TREE_CONSTANT_OVERFLOW (arg01)
+		      && operand_equal_p (arg1, arg00, 0))
+		    {
+		      tree powfn = TREE_OPERAND (TREE_OPERAND (arg0, 0), 0);
+		      REAL_VALUE_TYPE c;
+		      tree arg, arglist;
+
+		      c = TREE_REAL_CST (arg01);
+		      real_arithmetic (&c, PLUS_EXPR, &c, &dconst1);
+		      arg = build_real (type, c);
+		      arglist = build_tree_list (NULL_TREE, arg);
+		      arglist = tree_cons (NULL_TREE, arg1, arglist);
+		      return build_function_call_expr (powfn, arglist);
+		    }
+		}
+
+	      /* Optimize x*x as pow(x,2.0), which is expanded as x*x.  */
+	      if (! optimize_size
+		  && operand_equal_p (arg0, arg1, 0))
+		{
+		  tree powfn = mathfn_built_in (type, BUILT_IN_POW);
+
+		  if (powfn)
+		    {
+		      tree arg = build_real (type, dconst2);
+		      tree arglist = build_tree_list (NULL_TREE, arg);
+		      arglist = tree_cons (NULL_TREE, arg0, arglist);
+		      return build_function_call_expr (powfn, arglist);
+		    }
+		}
+	    }
+	}
+      goto associate;
+
+    case BIT_IOR_EXPR:
+    bit_ior:
+      if (integer_all_onesp (arg1))
+	return omit_one_operand (type, arg1, arg0);
+      if (integer_zerop (arg1))
+	return non_lvalue (fold_convert (type, arg0));
+      if (operand_equal_p (arg0, arg1, 0))
+	return non_lvalue (fold_convert (type, arg0));
+
+      /* ~X | X is -1.  */
+      if (TREE_CODE (arg0) == BIT_NOT_EXPR
+	  && operand_equal_p (TREE_OPERAND (arg0, 0), arg1, 0))
+	{
+	  t1 = build_int_cst (type, -1);
+	  t1 = force_fit_type (t1, 0, false, false);
+	  return omit_one_operand (type, t1, arg1);
+	}
+
+      /* X | ~X is -1.  */
+      if (TREE_CODE (arg1) == BIT_NOT_EXPR
+	  && operand_equal_p (arg0, TREE_OPERAND (arg1, 0), 0))
+	{
+	  t1 = build_int_cst (type, -1);
+	  t1 = force_fit_type (t1, 0, false, false);
+	  return omit_one_operand (type, t1, arg0);
+	}
+
+      t1 = distribute_bit_expr (code, type, arg0, arg1);
+      if (t1 != NULL_TREE)
+	return t1;
+
+      /* Convert (or (not arg0) (not arg1)) to (not (and (arg0) (arg1))).
+
+	 This results in more efficient code for machines without a NAND
+	 instruction.  Combine will canonicalize to the first form
+	 which will allow use of NAND instructions provided by the
+	 backend if they exist.  */
+      if (TREE_CODE (arg0) == BIT_NOT_EXPR
+	  && TREE_CODE (arg1) == BIT_NOT_EXPR)
+	{
+	  return fold_build1 (BIT_NOT_EXPR, type,
+			      build2 (BIT_AND_EXPR, type,
+				      TREE_OPERAND (arg0, 0),
+				      TREE_OPERAND (arg1, 0)));
+	}
+
+      /* See if this can be simplified into a rotate first.  If that
+	 is unsuccessful continue in the association code.  */
+      goto bit_rotate;
+
+    case BIT_XOR_EXPR:
+      if (integer_zerop (arg1))
+	return non_lvalue (fold_convert (type, arg0));
+      if (integer_all_onesp (arg1))
+	return fold_build1 (BIT_NOT_EXPR, type, arg0);
+      if (operand_equal_p (arg0, arg1, 0))
+	return omit_one_operand (type, integer_zero_node, arg0);
+
+      /* ~X ^ X is -1.  */
+      if (TREE_CODE (arg0) == BIT_NOT_EXPR
+	  && operand_equal_p (TREE_OPERAND (arg0, 0), arg1, 0))
+	{
+	  t1 = build_int_cst (type, -1);
+	  t1 = force_fit_type (t1, 0, false, false);
+	  return omit_one_operand (type, t1, arg1);
+	}
+
+      /* X ^ ~X is -1.  */
+      if (TREE_CODE (arg1) == BIT_NOT_EXPR
+	  && operand_equal_p (arg0, TREE_OPERAND (arg1, 0), 0))
+	{
+	  t1 = build_int_cst (type, -1);
+	  t1 = force_fit_type (t1, 0, false, false);
+	  return omit_one_operand (type, t1, arg0);
+	}
+
+      /* If we are XORing two BIT_AND_EXPR's, both of which are and'ing
+         with a constant, and the two constants have no bits in common,
+	 we should treat this as a BIT_IOR_EXPR since this may produce more
+	 simplifications.  */
+      if (TREE_CODE (arg0) == BIT_AND_EXPR
+	  && TREE_CODE (arg1) == BIT_AND_EXPR
+	  && TREE_CODE (TREE_OPERAND (arg0, 1)) == INTEGER_CST
+	  && TREE_CODE (TREE_OPERAND (arg1, 1)) == INTEGER_CST
+	  && integer_zerop (const_binop (BIT_AND_EXPR,
+					 TREE_OPERAND (arg0, 1),
+					 TREE_OPERAND (arg1, 1), 0)))
+	{
+	  code = BIT_IOR_EXPR;
+	  goto bit_ior;
+	}
+
+      /* (X | Y) ^ X -> Y & ~ X*/
+      if (TREE_CODE (arg0) == BIT_IOR_EXPR
+          && operand_equal_p (TREE_OPERAND (arg0, 0), arg1, 0))
+        {
+	  tree t2 = TREE_OPERAND (arg0, 1);
+	  t1 = fold_build1 (BIT_NOT_EXPR, TREE_TYPE (arg1),
+			    arg1);
+	  t1 = fold_build2 (BIT_AND_EXPR, type, fold_convert (type, t2),
+			    fold_convert (type, t1));
+	  return t1;
+	}
+
+      /* (Y | X) ^ X -> Y & ~ X*/
+      if (TREE_CODE (arg0) == BIT_IOR_EXPR
+          && operand_equal_p (TREE_OPERAND (arg0, 1), arg1, 0))
+        {
+	  tree t2 = TREE_OPERAND (arg0, 0);
+	  t1 = fold_build1 (BIT_NOT_EXPR, TREE_TYPE (arg1),
+			    arg1);
+	  t1 = fold_build2 (BIT_AND_EXPR, type, fold_convert (type, t2),
+			    fold_convert (type, t1));
+	  return t1;
+	}
+
+      /* X ^ (X | Y) -> Y & ~ X*/
+      if (TREE_CODE (arg1) == BIT_IOR_EXPR
+          && operand_equal_p (TREE_OPERAND (arg1, 0), arg0, 0))
+        {
+	  tree t2 = TREE_OPERAND (arg1, 1);
+	  t1 = fold_build1 (BIT_NOT_EXPR, TREE_TYPE (arg0),
+			    arg0);
+	  t1 = fold_build2 (BIT_AND_EXPR, type, fold_convert (type, t2),
+			    fold_convert (type, t1));
+	  return t1;
+	}
+
+      /* X ^ (Y | X) -> Y & ~ X*/
+      if (TREE_CODE (arg1) == BIT_IOR_EXPR
+          && operand_equal_p (TREE_OPERAND (arg1, 1), arg0, 0))
+        {
+	  tree t2 = TREE_OPERAND (arg1, 0);
+	  t1 = fold_build1 (BIT_NOT_EXPR, TREE_TYPE (arg0),
+			    arg0);
+	  t1 = fold_build2 (BIT_AND_EXPR, type, fold_convert (type, t2),
+			    fold_convert (type, t1));
+	  return t1;
+	}
+	
+      /* Convert ~X ^ ~Y to X ^ Y.  */
+      if (TREE_CODE (arg0) == BIT_NOT_EXPR
+	  && TREE_CODE (arg1) == BIT_NOT_EXPR)
+	return fold_build2 (code, type,
+			    fold_convert (type, TREE_OPERAND (arg0, 0)),
+			    fold_convert (type, TREE_OPERAND (arg1, 0)));
+
+      /* See if this can be simplified into a rotate first.  If that
+	 is unsuccessful continue in the association code.  */
+      goto bit_rotate;
+
+    case BIT_AND_EXPR:
+      if (integer_all_onesp (arg1))
+	return non_lvalue (fold_convert (type, arg0));
+      if (integer_zerop (arg1))
+	return omit_one_operand (type, arg1, arg0);
+      if (operand_equal_p (arg0, arg1, 0))
+	return non_lvalue (fold_convert (type, arg0));
+
+      /* ~X & X is always zero.  */
+      if (TREE_CODE (arg0) == BIT_NOT_EXPR
+	  && operand_equal_p (TREE_OPERAND (arg0, 0), arg1, 0))
+	return omit_one_operand (type, integer_zero_node, arg1);
+
+      /* X & ~X is always zero.  */
+      if (TREE_CODE (arg1) == BIT_NOT_EXPR
+	  && operand_equal_p (arg0, TREE_OPERAND (arg1, 0), 0))
+	return omit_one_operand (type, integer_zero_node, arg0);
+
+      t1 = distribute_bit_expr (code, type, arg0, arg1);
+      if (t1 != NULL_TREE)
+	return t1;
+      /* Simplify ((int)c & 0377) into (int)c, if c is unsigned char.  */
+      if (TREE_CODE (arg1) == INTEGER_CST && TREE_CODE (arg0) == NOP_EXPR
+	  && TYPE_UNSIGNED (TREE_TYPE (TREE_OPERAND (arg0, 0))))
+	{
+	  unsigned int prec
+	    = TYPE_PRECISION (TREE_TYPE (TREE_OPERAND (arg0, 0)));
+
+	  if (prec < BITS_PER_WORD && prec < HOST_BITS_PER_WIDE_INT
+	      && (~TREE_INT_CST_LOW (arg1)
+		  & (((HOST_WIDE_INT) 1 << prec) - 1)) == 0)
+	    return fold_convert (type, TREE_OPERAND (arg0, 0));
+	}
+
+      /* Convert (and (not arg0) (not arg1)) to (not (or (arg0) (arg1))).
+
+	 This results in more efficient code for machines without a NOR
+	 instruction.  Combine will canonicalize to the first form
+	 which will allow use of NOR instructions provided by the
+	 backend if they exist.  */
+      if (TREE_CODE (arg0) == BIT_NOT_EXPR
+	  && TREE_CODE (arg1) == BIT_NOT_EXPR)
+	{
+	  return fold_build1 (BIT_NOT_EXPR, type,
+			      build2 (BIT_IOR_EXPR, type,
+				      TREE_OPERAND (arg0, 0),
+				      TREE_OPERAND (arg1, 0)));
+	}
+
+      goto associate;
+
+    case RDIV_EXPR:
+      /* Don't touch a floating-point divide by zero unless the mode
+	 of the constant can represent infinity.  */
+      if (TREE_CODE (arg1) == REAL_CST
+	  && !MODE_HAS_INFINITIES (TYPE_MODE (TREE_TYPE (arg1)))
+	  && real_zerop (arg1))
+	return NULL_TREE;
+
+      /* (-A) / (-B) -> A / B  */
+      if (TREE_CODE (arg0) == NEGATE_EXPR && negate_expr_p (arg1))
+	return fold_build2 (RDIV_EXPR, type,
+			    TREE_OPERAND (arg0, 0),
+			    negate_expr (arg1));
+      if (TREE_CODE (arg1) == NEGATE_EXPR && negate_expr_p (arg0))
+	return fold_build2 (RDIV_EXPR, type,
+			    negate_expr (arg0),
+			    TREE_OPERAND (arg1, 0));
+
+      /* In IEEE floating point, x/1 is not equivalent to x for snans.  */
+      if (!HONOR_SNANS (TYPE_MODE (TREE_TYPE (arg0)))
+	  && real_onep (arg1))
+	return non_lvalue (fold_convert (type, arg0));
+
+      /* In IEEE floating point, x/-1 is not equivalent to -x for snans.  */
+      if (!HONOR_SNANS (TYPE_MODE (TREE_TYPE (arg0)))
+	  && real_minus_onep (arg1))
+	return non_lvalue (fold_convert (type, negate_expr (arg0)));
+
+      /* If ARG1 is a constant, we can convert this to a multiply by the
+	 reciprocal.  This does not have the same rounding properties,
+	 so only do this if -funsafe-math-optimizations.  We can actually
+	 always safely do it if ARG1 is a power of two, but it's hard to
+	 tell if it is or not in a portable manner.  */
+      if (TREE_CODE (arg1) == REAL_CST)
+	{
+	  if (flag_unsafe_math_optimizations
+	      && 0 != (tem = const_binop (code, build_real (type, dconst1),
+					  arg1, 0)))
+	    return fold_build2 (MULT_EXPR, type, arg0, tem);
+	  /* Find the reciprocal if optimizing and the result is exact.  */
+	  if (optimize)
+	    {
+	      REAL_VALUE_TYPE r;
+	      r = TREE_REAL_CST (arg1);
+	      if (exact_real_inverse (TYPE_MODE(TREE_TYPE(arg0)), &r))
+		{
+		  tem = build_real (type, r);
+		  return fold_build2 (MULT_EXPR, type,
+				      fold_convert (type, arg0), tem);
+		}
+	    }
+	}
+      /* Convert A/B/C to A/(B*C).  */
+      if (flag_unsafe_math_optimizations
+	  && TREE_CODE (arg0) == RDIV_EXPR)
+	return fold_build2 (RDIV_EXPR, type, TREE_OPERAND (arg0, 0),
+			    fold_build2 (MULT_EXPR, type,
+					 TREE_OPERAND (arg0, 1), arg1));
+
+      /* Convert A/(B/C) to (A/B)*C.  */
+      if (flag_unsafe_math_optimizations
+	  && TREE_CODE (arg1) == RDIV_EXPR)
+	return fold_build2 (MULT_EXPR, type,
+			    fold_build2 (RDIV_EXPR, type, arg0,
+					 TREE_OPERAND (arg1, 0)),
+			    TREE_OPERAND (arg1, 1));
+
+      /* Convert C1/(X*C2) into (C1/C2)/X.  */
+      if (flag_unsafe_math_optimizations
+	  && TREE_CODE (arg1) == MULT_EXPR
+	  && TREE_CODE (arg0) == REAL_CST
+	  && TREE_CODE (TREE_OPERAND (arg1, 1)) == REAL_CST)
+	{
+	  tree tem = const_binop (RDIV_EXPR, arg0,
+				  TREE_OPERAND (arg1, 1), 0);
+	  if (tem)
+	    return fold_build2 (RDIV_EXPR, type, tem,
+				TREE_OPERAND (arg1, 0));
+	}
+
+      if (flag_unsafe_math_optimizations)
+	{
+	  enum built_in_function fcode = builtin_mathfn_code (arg1);
+	  /* Optimize x/expN(y) into x*expN(-y).  */
+	  if (BUILTIN_EXPONENT_P (fcode))
+	    {
+	      tree expfn = TREE_OPERAND (TREE_OPERAND (arg1, 0), 0);
+	      tree arg = negate_expr (TREE_VALUE (TREE_OPERAND (arg1, 1)));
+	      tree arglist = build_tree_list (NULL_TREE,
+					      fold_convert (type, arg));
+	      arg1 = build_function_call_expr (expfn, arglist);
+	      return fold_build2 (MULT_EXPR, type, arg0, arg1);
+	    }
+
+	  /* Optimize x/pow(y,z) into x*pow(y,-z).  */
+	  if (fcode == BUILT_IN_POW
+	      || fcode == BUILT_IN_POWF
+	      || fcode == BUILT_IN_POWL)
+	    {
+	      tree powfn = TREE_OPERAND (TREE_OPERAND (arg1, 0), 0);
+	      tree arg10 = TREE_VALUE (TREE_OPERAND (arg1, 1));
+	      tree arg11 = TREE_VALUE (TREE_CHAIN (TREE_OPERAND (arg1, 1)));
+	      tree neg11 = fold_convert (type, negate_expr (arg11));
+	      tree arglist = tree_cons(NULL_TREE, arg10,
+				       build_tree_list (NULL_TREE, neg11));
+	      arg1 = build_function_call_expr (powfn, arglist);
+	      return fold_build2 (MULT_EXPR, type, arg0, arg1);
+	    }
+	}
+
+      if (flag_unsafe_math_optimizations)
+	{
+	  enum built_in_function fcode0 = builtin_mathfn_code (arg0);
+	  enum built_in_function fcode1 = builtin_mathfn_code (arg1);
+
+	  /* Optimize sin(x)/cos(x) as tan(x).  */
+	  if (((fcode0 == BUILT_IN_SIN && fcode1 == BUILT_IN_COS)
+	       || (fcode0 == BUILT_IN_SINF && fcode1 == BUILT_IN_COSF)
+	       || (fcode0 == BUILT_IN_SINL && fcode1 == BUILT_IN_COSL))
+	      && operand_equal_p (TREE_VALUE (TREE_OPERAND (arg0, 1)),
+				  TREE_VALUE (TREE_OPERAND (arg1, 1)), 0))
+	    {
+	      tree tanfn = mathfn_built_in (type, BUILT_IN_TAN);
+
+	      if (tanfn != NULL_TREE)
+		return build_function_call_expr (tanfn,
+						 TREE_OPERAND (arg0, 1));
+	    }
+
+	  /* Optimize cos(x)/sin(x) as 1.0/tan(x).  */
+	  if (((fcode0 == BUILT_IN_COS && fcode1 == BUILT_IN_SIN)
+	       || (fcode0 == BUILT_IN_COSF && fcode1 == BUILT_IN_SINF)
+	       || (fcode0 == BUILT_IN_COSL && fcode1 == BUILT_IN_SINL))
+	      && operand_equal_p (TREE_VALUE (TREE_OPERAND (arg0, 1)),
+				  TREE_VALUE (TREE_OPERAND (arg1, 1)), 0))
+	    {
+	      tree tanfn = mathfn_built_in (type, BUILT_IN_TAN);
+
+	      if (tanfn != NULL_TREE)
+		{
+		  tree tmp = TREE_OPERAND (arg0, 1);
+		  tmp = build_function_call_expr (tanfn, tmp);
+		  return fold_build2 (RDIV_EXPR, type,
+				      build_real (type, dconst1), tmp);
+		}
+	    }
+
+	  /* Optimize pow(x,c)/x as pow(x,c-1).  */
+	  if (fcode0 == BUILT_IN_POW
+	      || fcode0 == BUILT_IN_POWF
+	      || fcode0 == BUILT_IN_POWL)
+	    {
+	      tree arg00 = TREE_VALUE (TREE_OPERAND (arg0, 1));
+	      tree arg01 = TREE_VALUE (TREE_CHAIN (TREE_OPERAND (arg0, 1)));
+	      if (TREE_CODE (arg01) == REAL_CST
+		  && ! TREE_CONSTANT_OVERFLOW (arg01)
+		  && operand_equal_p (arg1, arg00, 0))
+		{
+		  tree powfn = TREE_OPERAND (TREE_OPERAND (arg0, 0), 0);
+		  REAL_VALUE_TYPE c;
+		  tree arg, arglist;
+
+		  c = TREE_REAL_CST (arg01);
+		  real_arithmetic (&c, MINUS_EXPR, &c, &dconst1);
+		  arg = build_real (type, c);
+		  arglist = build_tree_list (NULL_TREE, arg);
+		  arglist = tree_cons (NULL_TREE, arg1, arglist);
+		  return build_function_call_expr (powfn, arglist);
+		}
+	    }
+	}
+      goto binary;
+
+    case TRUNC_DIV_EXPR:
+    case ROUND_DIV_EXPR:
+    case FLOOR_DIV_EXPR:
+    case CEIL_DIV_EXPR:
+    case EXACT_DIV_EXPR:
+      if (integer_onep (arg1))
+	return non_lvalue (fold_convert (type, arg0));
+      if (integer_zerop (arg1))
+	return NULL_TREE;
+      /* X / -1 is -X.  */
+      if (!TYPE_UNSIGNED (type)
+	  && TREE_CODE (arg1) == INTEGER_CST
+	  && TREE_INT_CST_LOW (arg1) == (unsigned HOST_WIDE_INT) -1
+	  && TREE_INT_CST_HIGH (arg1) == -1)
+	return fold_convert (type, negate_expr (arg0));
+
+      /* If arg0 is a multiple of arg1, then rewrite to the fastest div
+	 operation, EXACT_DIV_EXPR.
+
+	 Note that only CEIL_DIV_EXPR and FLOOR_DIV_EXPR are rewritten now.
+	 At one time others generated faster code, it's not clear if they do
+	 after the last round to changes to the DIV code in expmed.c.  */
+      if ((code == CEIL_DIV_EXPR || code == FLOOR_DIV_EXPR)
+	  && multiple_of_p (type, arg0, arg1))
+	return fold_build2 (EXACT_DIV_EXPR, type, arg0, arg1);
+
+      if (TREE_CODE (arg1) == INTEGER_CST
+	  && 0 != (tem = extract_muldiv (op0, arg1, code, NULL_TREE)))
+	return fold_convert (type, tem);
+
+      goto binary;
+
+    case CEIL_MOD_EXPR:
+    case FLOOR_MOD_EXPR:
+    case ROUND_MOD_EXPR:
+    case TRUNC_MOD_EXPR:
+      /* X % 1 is always zero, but be sure to preserve any side
+	 effects in X.  */
+      if (integer_onep (arg1))
+	return omit_one_operand (type, integer_zero_node, arg0);
+
+      /* X % 0, return X % 0 unchanged so that we can get the
+	 proper warnings and errors.  */
+      if (integer_zerop (arg1))
+	return NULL_TREE;
+
+      /* 0 % X is always zero, but be sure to preserve any side
+	 effects in X.  Place this after checking for X == 0.  */
+      if (integer_zerop (arg0))
+	return omit_one_operand (type, integer_zero_node, arg1);
+
+      /* X % -1 is zero.  */
+      if (!TYPE_UNSIGNED (type)
+	  && TREE_CODE (arg1) == INTEGER_CST
+	  && TREE_INT_CST_LOW (arg1) == (unsigned HOST_WIDE_INT) -1
+	  && TREE_INT_CST_HIGH (arg1) == -1)
+	return omit_one_operand (type, integer_zero_node, arg0);
+
+      /* Optimize TRUNC_MOD_EXPR by a power of two into a BIT_AND_EXPR,
+         i.e. "X % C" into "X & C2", if X and C are positive.  */
+      if ((code == TRUNC_MOD_EXPR || code == FLOOR_MOD_EXPR)
+	  && (TYPE_UNSIGNED (type) || tree_expr_nonnegative_p (arg0))
+	  && integer_pow2p (arg1) && tree_int_cst_sgn (arg1) >= 0)
+	{
+	  unsigned HOST_WIDE_INT high, low;
+	  tree mask;
+	  int l;
+
+	  l = tree_log2 (arg1);
+	  if (l >= HOST_BITS_PER_WIDE_INT)
+	    {
+	      high = ((unsigned HOST_WIDE_INT) 1
+		      << (l - HOST_BITS_PER_WIDE_INT)) - 1;
+	      low = -1;
+	    }
+	  else
+	    {
+	      high = 0;
+	      low = ((unsigned HOST_WIDE_INT) 1 << l) - 1;
+	    }
+
+	  mask = build_int_cst_wide (type, low, high);
+	  return fold_build2 (BIT_AND_EXPR, type,
+			      fold_convert (type, arg0), mask);
+	}
+
+      /* X % -C is the same as X % C.  */
+      if (code == TRUNC_MOD_EXPR
+	  && !TYPE_UNSIGNED (type)
+	  && TREE_CODE (arg1) == INTEGER_CST
+	  && !TREE_CONSTANT_OVERFLOW (arg1)
+	  && TREE_INT_CST_HIGH (arg1) < 0
+	  && !flag_trapv
+	  /* Avoid this transformation if C is INT_MIN, i.e. C == -C.  */
+	  && !sign_bit_p (arg1, arg1))
+	return fold_build2 (code, type, fold_convert (type, arg0),
+			    fold_convert (type, negate_expr (arg1)));
+
+      /* X % -Y is the same as X % Y.  */
+      if (code == TRUNC_MOD_EXPR
+	  && !TYPE_UNSIGNED (type)
+	  && TREE_CODE (arg1) == NEGATE_EXPR
+	  && !flag_trapv)
+	return fold_build2 (code, type, fold_convert (type, arg0),
+			    fold_convert (type, TREE_OPERAND (arg1, 0)));
+
+      if (TREE_CODE (arg1) == INTEGER_CST
+	  && 0 != (tem = extract_muldiv (op0, arg1, code, NULL_TREE)))
+	return fold_convert (type, tem);
+
+      goto binary;
+
+    case LROTATE_EXPR:
+    case RROTATE_EXPR:
+      if (integer_all_onesp (arg0))
+	return omit_one_operand (type, arg0, arg1);
+      goto shift;
+
+    case RSHIFT_EXPR:
+      /* Optimize -1 >> x for arithmetic right shifts.  */
+      if (integer_all_onesp (arg0) && !TYPE_UNSIGNED (type))
+	return omit_one_operand (type, arg0, arg1);
+      /* ... fall through ...  */
+
+    case LSHIFT_EXPR:
+    shift:
+      if (integer_zerop (arg1))
+	return non_lvalue (fold_convert (type, arg0));
+      if (integer_zerop (arg0))
+	return omit_one_operand (type, arg0, arg1);
+
+      /* Since negative shift count is not well-defined,
+	 don't try to compute it in the compiler.  */
+      if (TREE_CODE (arg1) == INTEGER_CST && tree_int_cst_sgn (arg1) < 0)
+	return NULL_TREE;
+
+      /* Turn (a OP c1) OP c2 into a OP (c1+c2).  */
+      if (TREE_CODE (arg0) == code && host_integerp (arg1, false)
+	  && TREE_INT_CST_LOW (arg1) < TYPE_PRECISION (type)
+	  && host_integerp (TREE_OPERAND (arg0, 1), false)
+	  && TREE_INT_CST_LOW (TREE_OPERAND (arg0, 1)) < TYPE_PRECISION (type))
+	{
+	  HOST_WIDE_INT low = (TREE_INT_CST_LOW (TREE_OPERAND (arg0, 1))
+			       + TREE_INT_CST_LOW (arg1));
+
+	  /* Deal with a OP (c1 + c2) being undefined but (a OP c1) OP c2
+	     being well defined.  */
+	  if (low >= TYPE_PRECISION (type))
+	    {
+	      if (code == LROTATE_EXPR || code == RROTATE_EXPR)
+	        low = low % TYPE_PRECISION (type);
+	      else if (TYPE_UNSIGNED (type) || code == LSHIFT_EXPR)
+	        return build_int_cst (type, 0);
+	      else
+		low = TYPE_PRECISION (type) - 1;
+	    }
+
+	  return fold_build2 (code, type, TREE_OPERAND (arg0, 0),
+			      build_int_cst (type, low));
+	}
+
+      /* Transform (x >> c) << c into x & (-1<<c), or transform (x << c) >> c
+         into x & ((unsigned)-1 >> c) for unsigned types.  */
+      if (((code == LSHIFT_EXPR && TREE_CODE (arg0) == RSHIFT_EXPR)
+           || (TYPE_UNSIGNED (type)
+	       && code == RSHIFT_EXPR && TREE_CODE (arg0) == LSHIFT_EXPR))
+	  && host_integerp (arg1, false)
+	  && TREE_INT_CST_LOW (arg1) < TYPE_PRECISION (type)
+	  && host_integerp (TREE_OPERAND (arg0, 1), false)
+	  && TREE_INT_CST_LOW (TREE_OPERAND (arg0, 1)) < TYPE_PRECISION (type))
+	{
+	  HOST_WIDE_INT low0 = TREE_INT_CST_LOW (TREE_OPERAND (arg0, 1));
+	  HOST_WIDE_INT low1 = TREE_INT_CST_LOW (arg1);
+	  tree lshift;
+	  tree arg00;
+
+	  if (low0 == low1)
+	    {
+	      arg00 = fold_convert (type, TREE_OPERAND (arg0, 0));
+
+	      lshift = build_int_cst (type, -1);
+	      lshift = int_const_binop (code, lshift, arg1, 0);
+
+	      return fold_build2 (BIT_AND_EXPR, type, arg00, lshift);
+	    }
+	}
+
+      /* Rewrite an LROTATE_EXPR by a constant into an
+	 RROTATE_EXPR by a new constant.  */
+      if (code == LROTATE_EXPR && TREE_CODE (arg1) == INTEGER_CST)
+	{
+	  tree tem = build_int_cst (NULL_TREE,
+				    GET_MODE_BITSIZE (TYPE_MODE (type)));
+	  tem = fold_convert (TREE_TYPE (arg1), tem);
+	  tem = const_binop (MINUS_EXPR, tem, arg1, 0);
+	  return fold_build2 (RROTATE_EXPR, type, arg0, tem);
+	}
+
+      /* If we have a rotate of a bit operation with the rotate count and
+	 the second operand of the bit operation both constant,
+	 permute the two operations.  */
+      if (code == RROTATE_EXPR && TREE_CODE (arg1) == INTEGER_CST
+	  && (TREE_CODE (arg0) == BIT_AND_EXPR
+	      || TREE_CODE (arg0) == BIT_IOR_EXPR
+	      || TREE_CODE (arg0) == BIT_XOR_EXPR)
+	  && TREE_CODE (TREE_OPERAND (arg0, 1)) == INTEGER_CST)
+	return fold_build2 (TREE_CODE (arg0), type,
+			    fold_build2 (code, type,
+					 TREE_OPERAND (arg0, 0), arg1),
+			    fold_build2 (code, type,
+					 TREE_OPERAND (arg0, 1), arg1));
+
+      /* Two consecutive rotates adding up to the width of the mode can
+	 be ignored.  */
+      if (code == RROTATE_EXPR && TREE_CODE (arg1) == INTEGER_CST
+	  && TREE_CODE (arg0) == RROTATE_EXPR
+	  && TREE_CODE (TREE_OPERAND (arg0, 1)) == INTEGER_CST
+	  && TREE_INT_CST_HIGH (arg1) == 0
+	  && TREE_INT_CST_HIGH (TREE_OPERAND (arg0, 1)) == 0
+	  && ((TREE_INT_CST_LOW (arg1)
+	       + TREE_INT_CST_LOW (TREE_OPERAND (arg0, 1)))
+	      == (unsigned int) GET_MODE_BITSIZE (TYPE_MODE (type))))
+	return TREE_OPERAND (arg0, 0);
+
+      goto binary;
+
+    case MIN_EXPR:
+      if (operand_equal_p (arg0, arg1, 0))
+	return omit_one_operand (type, arg0, arg1);
+      if (INTEGRAL_TYPE_P (type)
+	  && operand_equal_p (arg1, TYPE_MIN_VALUE (type), OEP_ONLY_CONST))
+	return omit_one_operand (type, arg1, arg0);
+      goto associate;
+
+    case MAX_EXPR:
+      if (operand_equal_p (arg0, arg1, 0))
+	return omit_one_operand (type, arg0, arg1);
+      if (INTEGRAL_TYPE_P (type)
+	  && TYPE_MAX_VALUE (type)
+	  && operand_equal_p (arg1, TYPE_MAX_VALUE (type), OEP_ONLY_CONST))
+	return omit_one_operand (type, arg1, arg0);
+      goto associate;
+
+    case TRUTH_ANDIF_EXPR:
+      /* Note that the operands of this must be ints
+	 and their values must be 0 or 1.
+	 ("true" is a fixed value perhaps depending on the language.)  */
+      /* If first arg is constant zero, return it.  */
+      if (integer_zerop (arg0))
+	return fold_convert (type, arg0);
+    case TRUTH_AND_EXPR:
+      /* If either arg is constant true, drop it.  */
+      if (TREE_CODE (arg0) == INTEGER_CST && ! integer_zerop (arg0))
+	return non_lvalue (fold_convert (type, arg1));
+      if (TREE_CODE (arg1) == INTEGER_CST && ! integer_zerop (arg1)
+	  /* Preserve sequence points.  */
+	  && (code != TRUTH_ANDIF_EXPR || ! TREE_SIDE_EFFECTS (arg0)))
+	return non_lvalue (fold_convert (type, arg0));
+      /* If second arg is constant zero, result is zero, but first arg
+	 must be evaluated.  */
+      if (integer_zerop (arg1))
+	return omit_one_operand (type, arg1, arg0);
+      /* Likewise for first arg, but note that only the TRUTH_AND_EXPR
+	 case will be handled here.  */
+      if (integer_zerop (arg0))
+	return omit_one_operand (type, arg0, arg1);
+
+      /* !X && X is always false.  */
+      if (TREE_CODE (arg0) == TRUTH_NOT_EXPR
+	  && operand_equal_p (TREE_OPERAND (arg0, 0), arg1, 0))
+	return omit_one_operand (type, integer_zero_node, arg1);
+      /* X && !X is always false.  */
+      if (TREE_CODE (arg1) == TRUTH_NOT_EXPR
+	  && operand_equal_p (arg0, TREE_OPERAND (arg1, 0), 0))
+	return omit_one_operand (type, integer_zero_node, arg0);
+
+      /* A < X && A + 1 > Y ==> A < X && A >= Y.  Normally A + 1 > Y
+	 means A >= Y && A != MAX, but in this case we know that
+	 A < X <= MAX.  */
+
+      if (!TREE_SIDE_EFFECTS (arg0)
+	  && !TREE_SIDE_EFFECTS (arg1))
+	{
+	  tem = fold_to_nonsharp_ineq_using_bound (arg0, arg1);
+	  if (tem && !operand_equal_p (tem, arg0, 0))
+	    return fold_build2 (code, type, tem, arg1);
+
+	  tem = fold_to_nonsharp_ineq_using_bound (arg1, arg0);
+	  if (tem && !operand_equal_p (tem, arg1, 0))
+	    return fold_build2 (code, type, arg0, tem);
+	}
+
+    truth_andor:
+      /* We only do these simplifications if we are optimizing.  */
+      if (!optimize)
+	return NULL_TREE;
+
+      /* Check for things like (A || B) && (A || C).  We can convert this
+	 to A || (B && C).  Note that either operator can be any of the four
+	 truth and/or operations and the transformation will still be
+	 valid.   Also note that we only care about order for the
+	 ANDIF and ORIF operators.  If B contains side effects, this
+	 might change the truth-value of A.  */
+      if (TREE_CODE (arg0) == TREE_CODE (arg1)
+	  && (TREE_CODE (arg0) == TRUTH_ANDIF_EXPR
+	      || TREE_CODE (arg0) == TRUTH_ORIF_EXPR
+	      || TREE_CODE (arg0) == TRUTH_AND_EXPR
+	      || TREE_CODE (arg0) == TRUTH_OR_EXPR)
+	  && ! TREE_SIDE_EFFECTS (TREE_OPERAND (arg0, 1)))
+	{
+	  tree a00 = TREE_OPERAND (arg0, 0);
+	  tree a01 = TREE_OPERAND (arg0, 1);
+	  tree a10 = TREE_OPERAND (arg1, 0);
+	  tree a11 = TREE_OPERAND (arg1, 1);
+	  int commutative = ((TREE_CODE (arg0) == TRUTH_OR_EXPR
+			      || TREE_CODE (arg0) == TRUTH_AND_EXPR)
+			     && (code == TRUTH_AND_EXPR
+				 || code == TRUTH_OR_EXPR));
+
+	  if (operand_equal_p (a00, a10, 0))
+	    return fold_build2 (TREE_CODE (arg0), type, a00,
+				fold_build2 (code, type, a01, a11));
+	  else if (commutative && operand_equal_p (a00, a11, 0))
+	    return fold_build2 (TREE_CODE (arg0), type, a00,
+				fold_build2 (code, type, a01, a10));
+	  else if (commutative && operand_equal_p (a01, a10, 0))
+	    return fold_build2 (TREE_CODE (arg0), type, a01,
+				fold_build2 (code, type, a00, a11));
+
+	  /* This case if tricky because we must either have commutative
+	     operators or else A10 must not have side-effects.  */
+
+	  else if ((commutative || ! TREE_SIDE_EFFECTS (a10))
+		   && operand_equal_p (a01, a11, 0))
+	    return fold_build2 (TREE_CODE (arg0), type,
+				fold_build2 (code, type, a00, a10),
+				a01);
+	}
+
+      /* See if we can build a range comparison.  */
+      if (0 != (tem = fold_range_test (code, type, op0, op1)))
+	return tem;
+
+      /* Check for the possibility of merging component references.  If our
+	 lhs is another similar operation, try to merge its rhs with our
+	 rhs.  Then try to merge our lhs and rhs.  */
+      if (TREE_CODE (arg0) == code
+	  && 0 != (tem = fold_truthop (code, type,
+				       TREE_OPERAND (arg0, 1), arg1)))
+	return fold_build2 (code, type, TREE_OPERAND (arg0, 0), tem);
+
+      if ((tem = fold_truthop (code, type, arg0, arg1)) != 0)
+	return tem;
+
+      return NULL_TREE;
+
+    case TRUTH_ORIF_EXPR:
+      /* Note that the operands of this must be ints
+	 and their values must be 0 or true.
+	 ("true" is a fixed value perhaps depending on the language.)  */
+      /* If first arg is constant true, return it.  */
+      if (TREE_CODE (arg0) == INTEGER_CST && ! integer_zerop (arg0))
+	return fold_convert (type, arg0);
+    case TRUTH_OR_EXPR:
+      /* If either arg is constant zero, drop it.  */
+      if (TREE_CODE (arg0) == INTEGER_CST && integer_zerop (arg0))
+	return non_lvalue (fold_convert (type, arg1));
+      if (TREE_CODE (arg1) == INTEGER_CST && integer_zerop (arg1)
+	  /* Preserve sequence points.  */
+	  && (code != TRUTH_ORIF_EXPR || ! TREE_SIDE_EFFECTS (arg0)))
+	return non_lvalue (fold_convert (type, arg0));
+      /* If second arg is constant true, result is true, but we must
+	 evaluate first arg.  */
+      if (TREE_CODE (arg1) == INTEGER_CST && ! integer_zerop (arg1))
+	return omit_one_operand (type, arg1, arg0);
+      /* Likewise for first arg, but note this only occurs here for
+	 TRUTH_OR_EXPR.  */
+      if (TREE_CODE (arg0) == INTEGER_CST && ! integer_zerop (arg0))
+	return omit_one_operand (type, arg0, arg1);
+
+      /* !X || X is always true.  */
+      if (TREE_CODE (arg0) == TRUTH_NOT_EXPR
+	  && operand_equal_p (TREE_OPERAND (arg0, 0), arg1, 0))
+	return omit_one_operand (type, integer_one_node, arg1);
+      /* X || !X is always true.  */
+      if (TREE_CODE (arg1) == TRUTH_NOT_EXPR
+	  && operand_equal_p (arg0, TREE_OPERAND (arg1, 0), 0))
+	return omit_one_operand (type, integer_one_node, arg0);
+
+      goto truth_andor;
+
+    case TRUTH_XOR_EXPR:
+      /* If the second arg is constant zero, drop it.  */
+      if (integer_zerop (arg1))
+	return non_lvalue (fold_convert (type, arg0));
+      /* If the second arg is constant true, this is a logical inversion.  */
+      if (integer_onep (arg1))
+	{
+	  /* Only call invert_truthvalue if operand is a truth value.  */
+	  if (TREE_CODE (TREE_TYPE (arg0)) != BOOLEAN_TYPE)
+	    tem = fold_build1 (TRUTH_NOT_EXPR, TREE_TYPE (arg0), arg0);
+	  else
+	    tem = invert_truthvalue (arg0);
+	  return non_lvalue (fold_convert (type, tem));
+	}
+      /* Identical arguments cancel to zero.  */
+      if (operand_equal_p (arg0, arg1, 0))
+	return omit_one_operand (type, integer_zero_node, arg0);
+
+      /* !X ^ X is always true.  */
+      if (TREE_CODE (arg0) == TRUTH_NOT_EXPR
+	  && operand_equal_p (TREE_OPERAND (arg0, 0), arg1, 0))
+	return omit_one_operand (type, integer_one_node, arg1);
+
+      /* X ^ !X is always true.  */
+      if (TREE_CODE (arg1) == TRUTH_NOT_EXPR
+	  && operand_equal_p (arg0, TREE_OPERAND (arg1, 0), 0))
+	return omit_one_operand (type, integer_one_node, arg0);
+
+      return NULL_TREE;
+
+    case EQ_EXPR:
+    case NE_EXPR:
+    case LT_EXPR:
+    case GT_EXPR:
+    case LE_EXPR:
+    case GE_EXPR:	
+      /* If one arg is a real or integer constant, put it last.  */
+      if (tree_swap_operands_p (arg0, arg1, true))
+	return fold_build2 (swap_tree_comparison (code), type, op1, op0);
+	
+      /* bool_var != 0 becomes bool_var. */
+      if (TREE_CODE (TREE_TYPE (arg0)) == BOOLEAN_TYPE && integer_zerop (arg1)
+          && code == NE_EXPR)
+        return non_lvalue (fold_convert (type, arg0));
+	
+      /* bool_var == 1 becomes bool_var. */
+      if (TREE_CODE (TREE_TYPE (arg0)) == BOOLEAN_TYPE && integer_onep (arg1)
+          && code == EQ_EXPR)
+        return non_lvalue (fold_convert (type, arg0));
+
+      /* If this is an equality comparison of the address of a non-weak
+	 object against zero, then we know the result.  */
+      if ((code == EQ_EXPR || code == NE_EXPR)
+	  && TREE_CODE (arg0) == ADDR_EXPR
+	  && VAR_OR_FUNCTION_DECL_P (TREE_OPERAND (arg0, 0))
+	  && ! DECL_WEAK (TREE_OPERAND (arg0, 0))
+	  && integer_zerop (arg1))
+	return constant_boolean_node (code != EQ_EXPR, type);
+
+      /* If this is an equality comparison of the address of two non-weak,
+	 unaliased symbols neither of which are extern (since we do not
+	 have access to attributes for externs), then we know the result.  */
+      if ((code == EQ_EXPR || code == NE_EXPR)
+	  && TREE_CODE (arg0) == ADDR_EXPR
+	  && VAR_OR_FUNCTION_DECL_P (TREE_OPERAND (arg0, 0))
+	  && ! DECL_WEAK (TREE_OPERAND (arg0, 0))
+	  && ! lookup_attribute ("alias",
+				 DECL_ATTRIBUTES (TREE_OPERAND (arg0, 0)))
+	  && ! DECL_EXTERNAL (TREE_OPERAND (arg0, 0))
+	  && TREE_CODE (arg1) == ADDR_EXPR
+	  && VAR_OR_FUNCTION_DECL_P (TREE_OPERAND (arg1, 0))
+	  && ! DECL_WEAK (TREE_OPERAND (arg1, 0))
+	  && ! lookup_attribute ("alias",
+				 DECL_ATTRIBUTES (TREE_OPERAND (arg1, 0)))
+	  && ! DECL_EXTERNAL (TREE_OPERAND (arg1, 0)))
+	{
+	  /* We know that we're looking at the address of two
+	     non-weak, unaliased, static _DECL nodes.
+
+	     It is both wasteful and incorrect to call operand_equal_p
+	     to compare the two ADDR_EXPR nodes.  It is wasteful in that
+	     all we need to do is test pointer equality for the arguments
+	     to the two ADDR_EXPR nodes.  It is incorrect to use
+	     operand_equal_p as that function is NOT equivalent to a
+	     C equality test.  It can in fact return false for two
+	     objects which would test as equal using the C equality
+	     operator.  */
+	  bool equal = TREE_OPERAND (arg0, 0) == TREE_OPERAND (arg1, 0);
+	  return constant_boolean_node (equal
+				        ? code == EQ_EXPR : code != EQ_EXPR,
+				        type);
+	}
+
+      /* If this is a comparison of two exprs that look like an
+	 ARRAY_REF of the same object, then we can fold this to a
+	 comparison of the two offsets.  This is only safe for
+	 EQ_EXPR and NE_EXPR because of overflow issues.  */
+      if (code == EQ_EXPR || code == NE_EXPR)
+	{
+	  tree base0, offset0, base1, offset1;
+
+	  if (extract_array_ref (arg0, &base0, &offset0)
+	      && extract_array_ref (arg1, &base1, &offset1)
+	      && operand_equal_p (base0, base1, 0))
+	    {
+	      /* Handle no offsets on both sides specially.  */
+	      if (offset0 == NULL_TREE
+		  && offset1 == NULL_TREE)
+		return fold_build2 (code, type, integer_zero_node,
+				    integer_zero_node);
+
+	      if (!offset0 || !offset1
+		  || TREE_TYPE (offset0) == TREE_TYPE (offset1))
+		{
+		  if (offset0 == NULL_TREE)
+		    offset0 = build_int_cst (TREE_TYPE (offset1), 0);
+		  if (offset1 == NULL_TREE)
+		    offset1 = build_int_cst (TREE_TYPE (offset0), 0);
+		  return fold_build2 (code, type, offset0, offset1);
+		}
+	    }
+	}
+
+      /* Transform comparisons of the form X +- C CMP X.  */
+      if ((code != EQ_EXPR && code != NE_EXPR)
+	  && (TREE_CODE (arg0) == PLUS_EXPR || TREE_CODE (arg0) == MINUS_EXPR)
+	  && operand_equal_p (TREE_OPERAND (arg0, 0), arg1, 0)
+	  && ((TREE_CODE (TREE_OPERAND (arg0, 1)) == REAL_CST
+	       && !HONOR_SNANS (TYPE_MODE (TREE_TYPE (arg0))))
+	      || (TREE_CODE (TREE_OPERAND (arg0, 1)) == INTEGER_CST
+	          && !TYPE_UNSIGNED (TREE_TYPE (arg1))
+		  && !(flag_wrapv || flag_trapv))))
+	{
+	  tree arg01 = TREE_OPERAND (arg0, 1);
+	  enum tree_code code0 = TREE_CODE (arg0);
+	  int is_positive;
+
+	  if (TREE_CODE (arg01) == REAL_CST)
+	    is_positive = REAL_VALUE_NEGATIVE (TREE_REAL_CST (arg01)) ? -1 : 1;
+	  else
+	    is_positive = tree_int_cst_sgn (arg01);
+
+	  /* (X - c) > X becomes false.  */
+	  if (code == GT_EXPR
+	      && ((code0 == MINUS_EXPR && is_positive >= 0)
+		  || (code0 == PLUS_EXPR && is_positive <= 0)))
+	    return constant_boolean_node (0, type);
+
+	  /* Likewise (X + c) < X becomes false.  */
+	  if (code == LT_EXPR
+	      && ((code0 == PLUS_EXPR && is_positive >= 0)
+		  || (code0 == MINUS_EXPR && is_positive <= 0)))
+	    return constant_boolean_node (0, type);
+
+	  /* Convert (X - c) <= X to true.  */
+	  if (!HONOR_NANS (TYPE_MODE (TREE_TYPE (arg1)))
+	      && code == LE_EXPR
+	      && ((code0 == MINUS_EXPR && is_positive >= 0)
+		  || (code0 == PLUS_EXPR && is_positive <= 0)))
+	    return constant_boolean_node (1, type);
+
+	  /* Convert (X + c) >= X to true.  */
+	  if (!HONOR_NANS (TYPE_MODE (TREE_TYPE (arg1)))
+	      && code == GE_EXPR
+	      && ((code0 == PLUS_EXPR && is_positive >= 0)
+		  || (code0 == MINUS_EXPR && is_positive <= 0)))
+	    return constant_boolean_node (1, type);
+
+	  if (TREE_CODE (arg01) == INTEGER_CST)
+	    {
+	      /* Convert X + c > X and X - c < X to true for integers.  */
+	      if (code == GT_EXPR
+	          && ((code0 == PLUS_EXPR && is_positive > 0)
+		      || (code0 == MINUS_EXPR && is_positive < 0)))
+		return constant_boolean_node (1, type);
+
+	      if (code == LT_EXPR
+	          && ((code0 == MINUS_EXPR && is_positive > 0)
+		      || (code0 == PLUS_EXPR && is_positive < 0)))
+		return constant_boolean_node (1, type);
+
+	      /* Convert X + c <= X and X - c >= X to false for integers.  */
+	      if (code == LE_EXPR
+	          && ((code0 == PLUS_EXPR && is_positive > 0)
+		      || (code0 == MINUS_EXPR && is_positive < 0)))
+		return constant_boolean_node (0, type);
+
+	      if (code == GE_EXPR
+	          && ((code0 == MINUS_EXPR && is_positive > 0)
+		      || (code0 == PLUS_EXPR && is_positive < 0)))
+		return constant_boolean_node (0, type);
+	    }
+	}
+
+      /* Transform comparisons of the form X +- C1 CMP C2 to X CMP C2 +- C1.  */
+      if ((TREE_CODE (arg0) == PLUS_EXPR || TREE_CODE (arg0) == MINUS_EXPR)
+	  && (TREE_CODE (TREE_OPERAND (arg0, 1)) == INTEGER_CST
+	      && !TREE_OVERFLOW (TREE_OPERAND (arg0, 1))
+	      && !TYPE_UNSIGNED (TREE_TYPE (arg1))
+	      && !(flag_wrapv || flag_trapv))
+	  && (TREE_CODE (arg1) == INTEGER_CST
+	      && !TREE_OVERFLOW (arg1)))
+	{
+	  tree const1 = TREE_OPERAND (arg0, 1);
+	  tree const2 = arg1;
+	  tree variable = TREE_OPERAND (arg0, 0);
+	  tree lhs;
+	  int lhs_add;
+	  lhs_add = TREE_CODE (arg0) != PLUS_EXPR;
+	  
+	  lhs = fold_build2 (lhs_add ? PLUS_EXPR : MINUS_EXPR,
+			     TREE_TYPE (arg1), const2, const1);
+	  if (TREE_CODE (lhs) == TREE_CODE (arg1)
+	      && (TREE_CODE (lhs) != INTEGER_CST
+	          || !TREE_OVERFLOW (lhs)))
+	    return fold_build2 (code, type, variable, lhs);
+	}
+
+      if (FLOAT_TYPE_P (TREE_TYPE (arg0)))
+	{
+	  tree targ0 = strip_float_extensions (arg0);
+	  tree targ1 = strip_float_extensions (arg1);
+	  tree newtype = TREE_TYPE (targ0);
+
+	  if (TYPE_PRECISION (TREE_TYPE (targ1)) > TYPE_PRECISION (newtype))
+	    newtype = TREE_TYPE (targ1);
+
+	  /* Fold (double)float1 CMP (double)float2 into float1 CMP float2.  */
+	  if (TYPE_PRECISION (newtype) < TYPE_PRECISION (TREE_TYPE (arg0)))
+	    return fold_build2 (code, type, fold_convert (newtype, targ0),
+				fold_convert (newtype, targ1));
+
+	  /* (-a) CMP (-b) -> b CMP a  */
+	  if (TREE_CODE (arg0) == NEGATE_EXPR
+	      && TREE_CODE (arg1) == NEGATE_EXPR)
+	    return fold_build2 (code, type, TREE_OPERAND (arg1, 0),
+				TREE_OPERAND (arg0, 0));
+
+	  if (TREE_CODE (arg1) == REAL_CST)
+	  {
+	    REAL_VALUE_TYPE cst;
+	    cst = TREE_REAL_CST (arg1);
+
+	    /* (-a) CMP CST -> a swap(CMP) (-CST)  */
+	    if (TREE_CODE (arg0) == NEGATE_EXPR)
+	      return
+		fold_build2 (swap_tree_comparison (code), type,
+			     TREE_OPERAND (arg0, 0),
+			     build_real (TREE_TYPE (arg1),
+					 REAL_VALUE_NEGATE (cst)));
+
+	    /* IEEE doesn't distinguish +0 and -0 in comparisons.  */
+	    /* a CMP (-0) -> a CMP 0  */
+	    if (REAL_VALUE_MINUS_ZERO (cst))
+	      return fold_build2 (code, type, arg0,
+				  build_real (TREE_TYPE (arg1), dconst0));
+
+	    /* x != NaN is always true, other ops are always false.  */
+	    if (REAL_VALUE_ISNAN (cst)
+		&& ! HONOR_SNANS (TYPE_MODE (TREE_TYPE (arg1))))
+	      {
+		tem = (code == NE_EXPR) ? integer_one_node : integer_zero_node;
+		return omit_one_operand (type, tem, arg0);
+	      }
+
+	    /* Fold comparisons against infinity.  */
+	    if (REAL_VALUE_ISINF (cst))
+	      {
+		tem = fold_inf_compare (code, type, arg0, arg1);
+		if (tem != NULL_TREE)
+		  return tem;
+	      }
+	  }
+
+	  /* If this is a comparison of a real constant with a PLUS_EXPR
+	     or a MINUS_EXPR of a real constant, we can convert it into a
+	     comparison with a revised real constant as long as no overflow
+	     occurs when unsafe_math_optimizations are enabled.  */
+	  if (flag_unsafe_math_optimizations
+	      && TREE_CODE (arg1) == REAL_CST
+	      && (TREE_CODE (arg0) == PLUS_EXPR
+		  || TREE_CODE (arg0) == MINUS_EXPR)
+	      && TREE_CODE (TREE_OPERAND (arg0, 1)) == REAL_CST
+	      && 0 != (tem = const_binop (TREE_CODE (arg0) == PLUS_EXPR
+					  ? MINUS_EXPR : PLUS_EXPR,
+					  arg1, TREE_OPERAND (arg0, 1), 0))
+	      && ! TREE_CONSTANT_OVERFLOW (tem))
+	    return fold_build2 (code, type, TREE_OPERAND (arg0, 0), tem);
+
+	  /* Likewise, we can simplify a comparison of a real constant with
+	     a MINUS_EXPR whose first operand is also a real constant, i.e.
+	     (c1 - x) < c2 becomes x > c1-c2.  */
+	  if (flag_unsafe_math_optimizations
+	      && TREE_CODE (arg1) == REAL_CST
+	      && TREE_CODE (arg0) == MINUS_EXPR
+	      && TREE_CODE (TREE_OPERAND (arg0, 0)) == REAL_CST
+	      && 0 != (tem = const_binop (MINUS_EXPR, TREE_OPERAND (arg0, 0),
+					  arg1, 0))
+	      && ! TREE_CONSTANT_OVERFLOW (tem))
+	    return fold_build2 (swap_tree_comparison (code), type,
+				TREE_OPERAND (arg0, 1), tem);
+
+	  /* Fold comparisons against built-in math functions.  */
+	  if (TREE_CODE (arg1) == REAL_CST
+	      && flag_unsafe_math_optimizations
+	      && ! flag_errno_math)
+	    {
+	      enum built_in_function fcode = builtin_mathfn_code (arg0);
+
+	      if (fcode != END_BUILTINS)
+		{
+		  tem = fold_mathfn_compare (fcode, code, type, arg0, arg1);
+		  if (tem != NULL_TREE)
+		    return tem;
+		}
+	    }
+	}
+
+      /* Convert foo++ == CONST into ++foo == CONST + INCR.  */
+      if (TREE_CONSTANT (arg1)
+	  && (TREE_CODE (arg0) == POSTINCREMENT_EXPR
+	      || TREE_CODE (arg0) == POSTDECREMENT_EXPR)
+	  /* This optimization is invalid for ordered comparisons
+	     if CONST+INCR overflows or if foo+incr might overflow.
+	     This optimization is invalid for floating point due to rounding.
+	     For pointer types we assume overflow doesn't happen.  */
+	  && (POINTER_TYPE_P (TREE_TYPE (arg0))
+	      || (INTEGRAL_TYPE_P (TREE_TYPE (arg0))
+		  && (code == EQ_EXPR || code == NE_EXPR))))
+	{
+	  tree varop, newconst;
+
+	  if (TREE_CODE (arg0) == POSTINCREMENT_EXPR)
+	    {
+	      newconst = fold_build2 (PLUS_EXPR, TREE_TYPE (arg0),
+				      arg1, TREE_OPERAND (arg0, 1));
+	      varop = build2 (PREINCREMENT_EXPR, TREE_TYPE (arg0),
+			      TREE_OPERAND (arg0, 0),
+			      TREE_OPERAND (arg0, 1));
+	    }
+	  else
+	    {
+	      newconst = fold_build2 (MINUS_EXPR, TREE_TYPE (arg0),
+				      arg1, TREE_OPERAND (arg0, 1));
+	      varop = build2 (PREDECREMENT_EXPR, TREE_TYPE (arg0),
+			      TREE_OPERAND (arg0, 0),
+			      TREE_OPERAND (arg0, 1));
+	    }
+
+
+	  /* If VAROP is a reference to a bitfield, we must mask
+	     the constant by the width of the field.  */
+	  if (TREE_CODE (TREE_OPERAND (varop, 0)) == COMPONENT_REF
+	      && DECL_BIT_FIELD (TREE_OPERAND (TREE_OPERAND (varop, 0), 1))
+	      && host_integerp (DECL_SIZE (TREE_OPERAND
+					   (TREE_OPERAND (varop, 0), 1)), 1))
+	    {
+	      tree fielddecl = TREE_OPERAND (TREE_OPERAND (varop, 0), 1);
+	      HOST_WIDE_INT size = tree_low_cst (DECL_SIZE (fielddecl), 1);
+	      tree folded_compare, shift;
+
+	      /* First check whether the comparison would come out
+		 always the same.  If we don't do that we would
+		 change the meaning with the masking.  */
+	      folded_compare = fold_build2 (code, type,
+					    TREE_OPERAND (varop, 0), arg1);
+	      if (integer_zerop (folded_compare)
+		  || integer_onep (folded_compare))
+		return omit_one_operand (type, folded_compare, varop);
+
+	      shift = build_int_cst (NULL_TREE,
+				     TYPE_PRECISION (TREE_TYPE (varop)) - size);
+	      shift = fold_convert (TREE_TYPE (varop), shift);
+	      newconst = fold_build2 (LSHIFT_EXPR, TREE_TYPE (varop),
+				      newconst, shift);
+	      newconst = fold_build2 (RSHIFT_EXPR, TREE_TYPE (varop),
+				      newconst, shift);
+	    }
+
+	  return fold_build2 (code, type, varop, newconst);
+	}
+
+      /* Change X >= C to X > (C - 1) and X < C to X <= (C - 1) if C > 0.
+	 This transformation affects the cases which are handled in later
+	 optimizations involving comparisons with non-negative constants.  */
+      if (TREE_CODE (arg1) == INTEGER_CST
+	  && TREE_CODE (arg0) != INTEGER_CST
+	  && tree_int_cst_sgn (arg1) > 0)
+	{
+	  switch (code)
+	    {
+	    case GE_EXPR:
+	      arg1 = const_binop (MINUS_EXPR, arg1,
+			          build_int_cst (TREE_TYPE (arg1), 1), 0);
+	      return fold_build2 (GT_EXPR, type, arg0,
+				  fold_convert (TREE_TYPE (arg0), arg1));
+
+	    case LT_EXPR:
+	      arg1 = const_binop (MINUS_EXPR, arg1,
+			          build_int_cst (TREE_TYPE (arg1), 1), 0);
+	      return fold_build2 (LE_EXPR, type, arg0,
+				  fold_convert (TREE_TYPE (arg0), arg1));
+
+	    default:
+	      break;
+	    }
+	}
+
+      /* Comparisons with the highest or lowest possible integer of
+	 the specified size will have known values.  */
+      {
+	int width = GET_MODE_BITSIZE (TYPE_MODE (TREE_TYPE (arg1)));
+
+	if (TREE_CODE (arg1) == INTEGER_CST
+	    && ! TREE_CONSTANT_OVERFLOW (arg1)
+	    && width <= 2 * HOST_BITS_PER_WIDE_INT
+	    && (INTEGRAL_TYPE_P (TREE_TYPE (arg1))
+		|| POINTER_TYPE_P (TREE_TYPE (arg1))))
+	  {
+	    HOST_WIDE_INT signed_max_hi;
+	    unsigned HOST_WIDE_INT signed_max_lo;
+	    unsigned HOST_WIDE_INT max_hi, max_lo, min_hi, min_lo;
+
+	    if (width <= HOST_BITS_PER_WIDE_INT)
+	      {
+		signed_max_lo = ((unsigned HOST_WIDE_INT) 1 << (width - 1))
+				- 1;
+		signed_max_hi = 0;
+		max_hi = 0;
+
+		if (TYPE_UNSIGNED (TREE_TYPE (arg1)))
+		  {
+		    max_lo = ((unsigned HOST_WIDE_INT) 2 << (width - 1)) - 1;
+		    min_lo = 0;
+		    min_hi = 0;
+		  }
+		else
+		  {
+		    max_lo = signed_max_lo;
+		    min_lo = ((unsigned HOST_WIDE_INT) -1 << (width - 1));
+		    min_hi = -1;
+		  }
+	      }
+	    else
+	      {
+		width -= HOST_BITS_PER_WIDE_INT;
+		signed_max_lo = -1;
+		signed_max_hi = ((unsigned HOST_WIDE_INT) 1 << (width - 1))
+				- 1;
+		max_lo = -1;
+		min_lo = 0;
+
+		if (TYPE_UNSIGNED (TREE_TYPE (arg1)))
+		  {
+		    max_hi = ((unsigned HOST_WIDE_INT) 2 << (width - 1)) - 1;
+		    min_hi = 0;
+		  }
+		else
+		  {
+		    max_hi = signed_max_hi;
+		    min_hi = ((unsigned HOST_WIDE_INT) -1 << (width - 1));
+		  }
+	      }
+
+	    if ((unsigned HOST_WIDE_INT) TREE_INT_CST_HIGH (arg1) == max_hi
+		&& TREE_INT_CST_LOW (arg1) == max_lo)
+	      switch (code)
+		{
+		case GT_EXPR:
+		  return omit_one_operand (type, integer_zero_node, arg0);
+
+		case GE_EXPR:
+		  return fold_build2 (EQ_EXPR, type, arg0, arg1);
+
+		case LE_EXPR:
+		  return omit_one_operand (type, integer_one_node, arg0);
+
+		case LT_EXPR:
+		  return fold_build2 (NE_EXPR, type, arg0, arg1);
+
+		/* The GE_EXPR and LT_EXPR cases above are not normally
+		   reached because of previous transformations.  */
+
+		default:
+		  break;
+		}
+	    else if ((unsigned HOST_WIDE_INT) TREE_INT_CST_HIGH (arg1)
+		     == max_hi
+		     && TREE_INT_CST_LOW (arg1) == max_lo - 1)
+	      switch (code)
+		{
+		case GT_EXPR:
+		  arg1 = const_binop (PLUS_EXPR, arg1, integer_one_node, 0);
+		  return fold_build2 (EQ_EXPR, type, arg0, arg1);
+		case LE_EXPR:
+		  arg1 = const_binop (PLUS_EXPR, arg1, integer_one_node, 0);
+		  return fold_build2 (NE_EXPR, type, arg0, arg1);
+		default:
+		  break;
+		}
+	    else if ((unsigned HOST_WIDE_INT) TREE_INT_CST_HIGH (arg1)
+		     == min_hi
+		     && TREE_INT_CST_LOW (arg1) == min_lo)
+	      switch (code)
+		{
+		case LT_EXPR:
+		  return omit_one_operand (type, integer_zero_node, arg0);
+
+		case LE_EXPR:
+		  return fold_build2 (EQ_EXPR, type, arg0, arg1);
+
+		case GE_EXPR:
+		  return omit_one_operand (type, integer_one_node, arg0);
+
+		case GT_EXPR:
+		  return fold_build2 (NE_EXPR, type, op0, op1);
+
+		default:
+		  break;
+		}
+	    else if ((unsigned HOST_WIDE_INT) TREE_INT_CST_HIGH (arg1)
+		     == min_hi
+		     && TREE_INT_CST_LOW (arg1) == min_lo + 1)
+	      switch (code)
+		{
+		case GE_EXPR:
+		  arg1 = const_binop (MINUS_EXPR, arg1, integer_one_node, 0);
+		  return fold_build2 (NE_EXPR, type, arg0, arg1);
+		case LT_EXPR:
+		  arg1 = const_binop (MINUS_EXPR, arg1, integer_one_node, 0);
+		  return fold_build2 (EQ_EXPR, type, arg0, arg1);
+		default:
+		  break;
+		}
+
+	    else if (!in_gimple_form
+		     && TREE_INT_CST_HIGH (arg1) == signed_max_hi
+		     && TREE_INT_CST_LOW (arg1) == signed_max_lo
+		     && TYPE_UNSIGNED (TREE_TYPE (arg1))
+		     /* signed_type does not work on pointer types.  */
+		     && INTEGRAL_TYPE_P (TREE_TYPE (arg1)))
+	      {
+		/* The following case also applies to X < signed_max+1
+		   and X >= signed_max+1 because previous transformations.  */
+		if (code == LE_EXPR || code == GT_EXPR)
+		  {
+		    tree st0, st1;
+		    st0 = lang_hooks.types.signed_type (TREE_TYPE (arg0));
+		    st1 = lang_hooks.types.signed_type (TREE_TYPE (arg1));
+		    return fold_build2 (code == LE_EXPR ? GE_EXPR: LT_EXPR,
+			       		type, fold_convert (st0, arg0),
+			       		build_int_cst (st1, 0));
+		  }
+	      }
+	  }
+      }
+
+      /* If this is an EQ or NE comparison of a constant with a PLUS_EXPR or
+	 a MINUS_EXPR of a constant, we can convert it into a comparison with
+	 a revised constant as long as no overflow occurs.  */
+      if ((code == EQ_EXPR || code == NE_EXPR)
+	  && TREE_CODE (arg1) == INTEGER_CST
+	  && (TREE_CODE (arg0) == PLUS_EXPR
+	      || TREE_CODE (arg0) == MINUS_EXPR)
+	  && TREE_CODE (TREE_OPERAND (arg0, 1)) == INTEGER_CST
+	  && 0 != (tem = const_binop (TREE_CODE (arg0) == PLUS_EXPR
+				      ? MINUS_EXPR : PLUS_EXPR,
+				      arg1, TREE_OPERAND (arg0, 1), 0))
+	  && ! TREE_CONSTANT_OVERFLOW (tem))
+	return fold_build2 (code, type, TREE_OPERAND (arg0, 0), tem);
+
+      /* Similarly for a NEGATE_EXPR.  */
+      else if ((code == EQ_EXPR || code == NE_EXPR)
+	       && TREE_CODE (arg0) == NEGATE_EXPR
+	       && TREE_CODE (arg1) == INTEGER_CST
+	       && 0 != (tem = negate_expr (arg1))
+	       && TREE_CODE (tem) == INTEGER_CST
+	       && ! TREE_CONSTANT_OVERFLOW (tem))
+	return fold_build2 (code, type, TREE_OPERAND (arg0, 0), tem);
+
+      /* If we have X - Y == 0, we can convert that to X == Y and similarly
+	 for !=.  Don't do this for ordered comparisons due to overflow.  */
+      else if ((code == NE_EXPR || code == EQ_EXPR)
+	       && integer_zerop (arg1) && TREE_CODE (arg0) == MINUS_EXPR)
+	return fold_build2 (code, type,
+			    TREE_OPERAND (arg0, 0), TREE_OPERAND (arg0, 1));
+
+      else if (TREE_CODE (TREE_TYPE (arg0)) == INTEGER_TYPE
+	       && (TREE_CODE (arg0) == NOP_EXPR
+		   || TREE_CODE (arg0) == CONVERT_EXPR))
+	{
+	  /* If we are widening one operand of an integer comparison,
+	     see if the other operand is similarly being widened.  Perhaps we
+	     can do the comparison in the narrower type.  */
+	  tem = fold_widened_comparison (code, type, arg0, arg1);
+	  if (tem)
+	    return tem;
+
+	  /* Or if we are changing signedness.  */
+	  tem = fold_sign_changed_comparison (code, type, arg0, arg1);
+	  if (tem)
+	    return tem;
+	}
+
+      /* If this is comparing a constant with a MIN_EXPR or a MAX_EXPR of a
+	 constant, we can simplify it.  */
+      else if (TREE_CODE (arg1) == INTEGER_CST
+	       && (TREE_CODE (arg0) == MIN_EXPR
+		   || TREE_CODE (arg0) == MAX_EXPR)
+	       && TREE_CODE (TREE_OPERAND (arg0, 1)) == INTEGER_CST)
+	{
+	  tem = optimize_minmax_comparison (code, type, op0, op1);
+	  if (tem)
+	    return tem;
+
+	  return NULL_TREE;
+	}
+
+      /* If we are comparing an ABS_EXPR with a constant, we can
+	 convert all the cases into explicit comparisons, but they may
+	 well not be faster than doing the ABS and one comparison.
+	 But ABS (X) <= C is a range comparison, which becomes a subtraction
+	 and a comparison, and is probably faster.  */
+      else if (code == LE_EXPR && TREE_CODE (arg1) == INTEGER_CST
+	       && TREE_CODE (arg0) == ABS_EXPR
+	       && ! TREE_SIDE_EFFECTS (arg0)
+	       && (0 != (tem = negate_expr (arg1)))
+	       && TREE_CODE (tem) == INTEGER_CST
+	       && ! TREE_CONSTANT_OVERFLOW (tem))
+	return fold_build2 (TRUTH_ANDIF_EXPR, type,
+			    build2 (GE_EXPR, type,
+				    TREE_OPERAND (arg0, 0), tem),
+			    build2 (LE_EXPR, type,
+				    TREE_OPERAND (arg0, 0), arg1));
+
+      /* Convert ABS_EXPR<x> >= 0 to true.  */
+      else if (code == GE_EXPR
+	       && tree_expr_nonnegative_p (arg0)
+	       && (integer_zerop (arg1)
+		   || (! HONOR_NANS (TYPE_MODE (TREE_TYPE (arg0)))
+                       && real_zerop (arg1))))
+	return omit_one_operand (type, integer_one_node, arg0);
+
+      /* Convert ABS_EXPR<x> < 0 to false.  */
+      else if (code == LT_EXPR
+	       && tree_expr_nonnegative_p (arg0)
+	       && (integer_zerop (arg1) || real_zerop (arg1)))
+	return omit_one_operand (type, integer_zero_node, arg0);
+
+      /* Convert ABS_EXPR<x> == 0 or ABS_EXPR<x> != 0 to x == 0 or x != 0.  */
+      else if ((code == EQ_EXPR || code == NE_EXPR)
+	       && TREE_CODE (arg0) == ABS_EXPR
+	       && (integer_zerop (arg1) || real_zerop (arg1)))
+	return fold_build2 (code, type, TREE_OPERAND (arg0, 0), arg1);
+
+      /* If this is an EQ or NE comparison with zero and ARG0 is
+	 (1 << foo) & bar, convert it to (bar >> foo) & 1.  Both require
+	 two operations, but the latter can be done in one less insn
+	 on machines that have only two-operand insns or on which a
+	 constant cannot be the first operand.  */
+      if (integer_zerop (arg1) && (code == EQ_EXPR || code == NE_EXPR)
+	  && TREE_CODE (arg0) == BIT_AND_EXPR)
+	{
+	  tree arg00 = TREE_OPERAND (arg0, 0);
+	  tree arg01 = TREE_OPERAND (arg0, 1);
+	  if (TREE_CODE (arg00) == LSHIFT_EXPR
+	      && integer_onep (TREE_OPERAND (arg00, 0)))
+	    return
+	      fold_build2 (code, type,
+			   build2 (BIT_AND_EXPR, TREE_TYPE (arg0),
+				   build2 (RSHIFT_EXPR, TREE_TYPE (arg00),
+					   arg01, TREE_OPERAND (arg00, 1)),
+				   fold_convert (TREE_TYPE (arg0),
+						 integer_one_node)),
+			   arg1);
+	  else if (TREE_CODE (TREE_OPERAND (arg0, 1)) == LSHIFT_EXPR
+		   && integer_onep (TREE_OPERAND (TREE_OPERAND (arg0, 1), 0)))
+	    return
+	      fold_build2 (code, type,
+			   build2 (BIT_AND_EXPR, TREE_TYPE (arg0),
+				   build2 (RSHIFT_EXPR, TREE_TYPE (arg01),
+					   arg00, TREE_OPERAND (arg01, 1)),
+				   fold_convert (TREE_TYPE (arg0),
+						 integer_one_node)),
+			   arg1);
+	}
+
+      /* If this is an NE or EQ comparison of zero against the result of a
+	 signed MOD operation whose second operand is a power of 2, make
+	 the MOD operation unsigned since it is simpler and equivalent.  */
+      if ((code == NE_EXPR || code == EQ_EXPR)
+	  && integer_zerop (arg1)
+	  && !TYPE_UNSIGNED (TREE_TYPE (arg0))
+	  && (TREE_CODE (arg0) == TRUNC_MOD_EXPR
+	      || TREE_CODE (arg0) == CEIL_MOD_EXPR
+	      || TREE_CODE (arg0) == FLOOR_MOD_EXPR
+	      || TREE_CODE (arg0) == ROUND_MOD_EXPR)
+	  && integer_pow2p (TREE_OPERAND (arg0, 1)))
+	{
+	  tree newtype = lang_hooks.types.unsigned_type (TREE_TYPE (arg0));
+	  tree newmod = fold_build2 (TREE_CODE (arg0), newtype,
+				     fold_convert (newtype,
+						   TREE_OPERAND (arg0, 0)),
+				     fold_convert (newtype,
+						   TREE_OPERAND (arg0, 1)));
+
+	  return fold_build2 (code, type, newmod,
+			      fold_convert (newtype, arg1));
+	}
+
+      /* If this is an NE comparison of zero with an AND of one, remove the
+	 comparison since the AND will give the correct value.  */
+      if (code == NE_EXPR && integer_zerop (arg1)
+	  && TREE_CODE (arg0) == BIT_AND_EXPR
+	  && integer_onep (TREE_OPERAND (arg0, 1)))
+	return fold_convert (type, arg0);
+
+      /* If we have (A & C) == C where C is a power of 2, convert this into
+	 (A & C) != 0.  Similarly for NE_EXPR.  */
+      if ((code == EQ_EXPR || code == NE_EXPR)
+	  && TREE_CODE (arg0) == BIT_AND_EXPR
+	  && integer_pow2p (TREE_OPERAND (arg0, 1))
+	  && operand_equal_p (TREE_OPERAND (arg0, 1), arg1, 0))
+	return fold_build2 (code == EQ_EXPR ? NE_EXPR : EQ_EXPR, type,
+			    arg0, fold_convert (TREE_TYPE (arg0),
+						integer_zero_node));
+
+      /* If we have (A & C) != 0 or (A & C) == 0 and C is the sign
+	 bit, then fold the expression into A < 0 or A >= 0.  */
+      tem = fold_single_bit_test_into_sign_test (code, arg0, arg1, type);
+      if (tem)
+	return tem;
+
+      /* If we have (A & C) == D where D & ~C != 0, convert this into 0.
+	 Similarly for NE_EXPR.  */
+      if ((code == EQ_EXPR || code == NE_EXPR)
+	  && TREE_CODE (arg0) == BIT_AND_EXPR
+	  && TREE_CODE (arg1) == INTEGER_CST
+	  && TREE_CODE (TREE_OPERAND (arg0, 1)) == INTEGER_CST)
+	{
+	  tree notc = fold_build1 (BIT_NOT_EXPR,
+				   TREE_TYPE (TREE_OPERAND (arg0, 1)),
+				   TREE_OPERAND (arg0, 1));
+	  tree dandnotc = fold_build2 (BIT_AND_EXPR, TREE_TYPE (arg0),
+				       arg1, notc);
+	  tree rslt = code == EQ_EXPR ? integer_zero_node : integer_one_node;
+	  if (integer_nonzerop (dandnotc))
+	    return omit_one_operand (type, rslt, arg0);
+	}
+
+      /* If we have (A | C) == D where C & ~D != 0, convert this into 0.
+	 Similarly for NE_EXPR.  */
+      if ((code == EQ_EXPR || code == NE_EXPR)
+	  && TREE_CODE (arg0) == BIT_IOR_EXPR
+	  && TREE_CODE (arg1) == INTEGER_CST
+	  && TREE_CODE (TREE_OPERAND (arg0, 1)) == INTEGER_CST)
+	{
+	  tree notd = fold_build1 (BIT_NOT_EXPR, TREE_TYPE (arg1), arg1);
+	  tree candnotd = fold_build2 (BIT_AND_EXPR, TREE_TYPE (arg0),
+				       TREE_OPERAND (arg0, 1), notd);
+	  tree rslt = code == EQ_EXPR ? integer_zero_node : integer_one_node;
+	  if (integer_nonzerop (candnotd))
+	    return omit_one_operand (type, rslt, arg0);
+	}
+
+      /* If X is unsigned, convert X < (1 << Y) into X >> Y == 0
+	 and similarly for >= into !=.  */
+      if ((code == LT_EXPR || code == GE_EXPR)
+	  && TYPE_UNSIGNED (TREE_TYPE (arg0))
+	  && TREE_CODE (arg1) == LSHIFT_EXPR
+	  && integer_onep (TREE_OPERAND (arg1, 0)))
+	return build2 (code == LT_EXPR ? EQ_EXPR : NE_EXPR, type,
+		       build2 (RSHIFT_EXPR, TREE_TYPE (arg0), arg0,
+			       TREE_OPERAND (arg1, 1)),
+		       fold_convert (TREE_TYPE (arg0), integer_zero_node));
+
+      else if ((code == LT_EXPR || code == GE_EXPR)
+	       && TYPE_UNSIGNED (TREE_TYPE (arg0))
+	       && (TREE_CODE (arg1) == NOP_EXPR
+		   || TREE_CODE (arg1) == CONVERT_EXPR)
+	       && TREE_CODE (TREE_OPERAND (arg1, 0)) == LSHIFT_EXPR
+	       && integer_onep (TREE_OPERAND (TREE_OPERAND (arg1, 0), 0)))
+	return
+	  build2 (code == LT_EXPR ? EQ_EXPR : NE_EXPR, type,
+		  fold_convert (TREE_TYPE (arg0),
+				build2 (RSHIFT_EXPR, TREE_TYPE (arg0), arg0,
+					TREE_OPERAND (TREE_OPERAND (arg1, 0),
+						      1))),
+		  fold_convert (TREE_TYPE (arg0), integer_zero_node));
+
+      /* Simplify comparison of something with itself.  (For IEEE
+	 floating-point, we can only do some of these simplifications.)  */
+      if (operand_equal_p (arg0, arg1, 0))
+	{
+	  switch (code)
+	    {
+	    case EQ_EXPR:
+	      if (! FLOAT_TYPE_P (TREE_TYPE (arg0))
+		  || ! HONOR_NANS (TYPE_MODE (TREE_TYPE (arg0))))
+		return constant_boolean_node (1, type);
+	      break;
+
+	    case GE_EXPR:
+	    case LE_EXPR:
+	      if (! FLOAT_TYPE_P (TREE_TYPE (arg0))
+		  || ! HONOR_NANS (TYPE_MODE (TREE_TYPE (arg0))))
+		return constant_boolean_node (1, type);
+	      return fold_build2 (EQ_EXPR, type, arg0, arg1);
+
+	    case NE_EXPR:
+	      /* For NE, we can only do this simplification if integer
+		 or we don't honor IEEE floating point NaNs.  */
+	      if (FLOAT_TYPE_P (TREE_TYPE (arg0))
+		  && HONOR_NANS (TYPE_MODE (TREE_TYPE (arg0))))
+		break;
+	      /* ... fall through ...  */
+	    case GT_EXPR:
+	    case LT_EXPR:
+	      return constant_boolean_node (0, type);
+	    default:
+	      gcc_unreachable ();
+	    }
+	}
+
+      /* If we are comparing an expression that just has comparisons
+	 of two integer values, arithmetic expressions of those comparisons,
+	 and constants, we can simplify it.  There are only three cases
+	 to check: the two values can either be equal, the first can be
+	 greater, or the second can be greater.  Fold the expression for
+	 those three values.  Since each value must be 0 or 1, we have
+	 eight possibilities, each of which corresponds to the constant 0
+	 or 1 or one of the six possible comparisons.
+
+	 This handles common cases like (a > b) == 0 but also handles
+	 expressions like  ((x > y) - (y > x)) > 0, which supposedly
+	 occur in macroized code.  */
+
+      if (TREE_CODE (arg1) == INTEGER_CST && TREE_CODE (arg0) != INTEGER_CST)
+	{
+	  tree cval1 = 0, cval2 = 0;
+	  int save_p = 0;
+
+	  if (twoval_comparison_p (arg0, &cval1, &cval2, &save_p)
+	      /* Don't handle degenerate cases here; they should already
+		 have been handled anyway.  */
+	      && cval1 != 0 && cval2 != 0
+	      && ! (TREE_CONSTANT (cval1) && TREE_CONSTANT (cval2))
+	      && TREE_TYPE (cval1) == TREE_TYPE (cval2)
+	      && INTEGRAL_TYPE_P (TREE_TYPE (cval1))
+	      && TYPE_MAX_VALUE (TREE_TYPE (cval1))
+	      && TYPE_MAX_VALUE (TREE_TYPE (cval2))
+	      && ! operand_equal_p (TYPE_MIN_VALUE (TREE_TYPE (cval1)),
+				    TYPE_MAX_VALUE (TREE_TYPE (cval2)), 0))
+	    {
+	      tree maxval = TYPE_MAX_VALUE (TREE_TYPE (cval1));
+	      tree minval = TYPE_MIN_VALUE (TREE_TYPE (cval1));
+
+	      /* We can't just pass T to eval_subst in case cval1 or cval2
+		 was the same as ARG1.  */
+
+	      tree high_result
+		= fold_build2 (code, type,
+			       eval_subst (arg0, cval1, maxval,
+					   cval2, minval),
+			       arg1);
+	      tree equal_result
+		= fold_build2 (code, type,
+			       eval_subst (arg0, cval1, maxval,
+					   cval2, maxval),
+			       arg1);
+	      tree low_result
+		= fold_build2 (code, type,
+			       eval_subst (arg0, cval1, minval,
+					   cval2, maxval),
+			       arg1);
+
+	      /* All three of these results should be 0 or 1.  Confirm they
+		 are.  Then use those values to select the proper code
+		 to use.  */
+
+	      if ((integer_zerop (high_result)
+		   || integer_onep (high_result))
+		  && (integer_zerop (equal_result)
+		      || integer_onep (equal_result))
+		  && (integer_zerop (low_result)
+		      || integer_onep (low_result)))
+		{
+		  /* Make a 3-bit mask with the high-order bit being the
+		     value for `>', the next for '=', and the low for '<'.  */
+		  switch ((integer_onep (high_result) * 4)
+			  + (integer_onep (equal_result) * 2)
+			  + integer_onep (low_result))
+		    {
+		    case 0:
+		      /* Always false.  */
+		      return omit_one_operand (type, integer_zero_node, arg0);
+		    case 1:
+		      code = LT_EXPR;
+		      break;
+		    case 2:
+		      code = EQ_EXPR;
+		      break;
+		    case 3:
+		      code = LE_EXPR;
+		      break;
+		    case 4:
+		      code = GT_EXPR;
+		      break;
+		    case 5:
+		      code = NE_EXPR;
+		      break;
+		    case 6:
+		      code = GE_EXPR;
+		      break;
+		    case 7:
+		      /* Always true.  */
+		      return omit_one_operand (type, integer_one_node, arg0);
+		    }
+
+		  if (save_p)
+		    return save_expr (build2 (code, type, cval1, cval2));
+		  else
+		    return fold_build2 (code, type, cval1, cval2);
+		}
+	    }
+	}
+
+      /* If this is a comparison of a field, we may be able to simplify it.  */
+      if (((TREE_CODE (arg0) == COMPONENT_REF
+	    && lang_hooks.can_use_bit_fields_p ())
+	   || TREE_CODE (arg0) == BIT_FIELD_REF)
+	  && (code == EQ_EXPR || code == NE_EXPR)
+	  /* Handle the constant case even without -O
+	     to make sure the warnings are given.  */
+	  && (optimize || TREE_CODE (arg1) == INTEGER_CST))
+	{
+	  t1 = optimize_bit_field_compare (code, type, arg0, arg1);
+	  if (t1)
+	    return t1;
+	}
+
+      /* Fold a comparison of the address of COMPONENT_REFs with the same
+         type and component to a comparison of the address of the base
+	 object.  In short, &x->a OP &y->a to x OP y and
+         &x->a OP &y.a to x OP &y  */
+      if (TREE_CODE (arg0) == ADDR_EXPR
+	  && TREE_CODE (TREE_OPERAND (arg0, 0)) == COMPONENT_REF
+	  && TREE_CODE (arg1) == ADDR_EXPR
+	  && TREE_CODE (TREE_OPERAND (arg1, 0)) == COMPONENT_REF)
+        {
+	  tree cref0 = TREE_OPERAND (arg0, 0);
+	  tree cref1 = TREE_OPERAND (arg1, 0);
+	  if (TREE_OPERAND (cref0, 1) == TREE_OPERAND (cref1, 1))
+	    {
+	      tree op0 = TREE_OPERAND (cref0, 0);
+	      tree op1 = TREE_OPERAND (cref1, 0);
+	      return fold_build2 (code, type,
+			          build_fold_addr_expr (op0),
+				  build_fold_addr_expr (op1));
+	    }
+	}
+
+      /* Optimize comparisons of strlen vs zero to a compare of the
+	 first character of the string vs zero.  To wit,
+		strlen(ptr) == 0   =>  *ptr == 0
+		strlen(ptr) != 0   =>  *ptr != 0
+	 Other cases should reduce to one of these two (or a constant)
+	 due to the return value of strlen being unsigned.  */
+      if ((code == EQ_EXPR || code == NE_EXPR)
+	  && integer_zerop (arg1)
+	  && TREE_CODE (arg0) == CALL_EXPR)
+	{
+	  tree fndecl = get_callee_fndecl (arg0);
+	  tree arglist;
+
+	  if (fndecl
+	      && DECL_BUILT_IN_CLASS (fndecl) == BUILT_IN_NORMAL
+	      && DECL_FUNCTION_CODE (fndecl) == BUILT_IN_STRLEN
+	      && (arglist = TREE_OPERAND (arg0, 1))
+	      && TREE_CODE (TREE_TYPE (TREE_VALUE (arglist))) == POINTER_TYPE
+	      && ! TREE_CHAIN (arglist))
+	    {
+	      tree iref = build_fold_indirect_ref (TREE_VALUE (arglist));
+	      return fold_build2 (code, type, iref,
+				  build_int_cst (TREE_TYPE (iref), 0));
+	    }
+	}
+
+      /* We can fold X/C1 op C2 where C1 and C2 are integer constants
+	 into a single range test.  */
+      if ((TREE_CODE (arg0) == TRUNC_DIV_EXPR
+	   || TREE_CODE (arg0) == EXACT_DIV_EXPR)
+	  && TREE_CODE (arg1) == INTEGER_CST
+	  && TREE_CODE (TREE_OPERAND (arg0, 1)) == INTEGER_CST
+	  && !integer_zerop (TREE_OPERAND (arg0, 1))
+	  && !TREE_OVERFLOW (TREE_OPERAND (arg0, 1))
+	  && !TREE_OVERFLOW (arg1))
+	{
+	  t1 = fold_div_compare (code, type, arg0, arg1);
+	  if (t1 != NULL_TREE)
+	    return t1;
+	}
+
+      if ((code == EQ_EXPR || code == NE_EXPR)
+	  && integer_zerop (arg1)
+	  && tree_expr_nonzero_p (arg0))
+        {
+	  tree res = constant_boolean_node (code==NE_EXPR, type);
+	  return omit_one_operand (type, res, arg0);
+	}
+
+      t1 = fold_relational_const (code, type, arg0, arg1);
+      return t1 == NULL_TREE ? NULL_TREE : t1;
+
+    case UNORDERED_EXPR:
+    case ORDERED_EXPR:
+    case UNLT_EXPR:
+    case UNLE_EXPR:
+    case UNGT_EXPR:
+    case UNGE_EXPR:
+    case UNEQ_EXPR:
+    case LTGT_EXPR:
+      if (TREE_CODE (arg0) == REAL_CST && TREE_CODE (arg1) == REAL_CST)
+	{
+	  t1 = fold_relational_const (code, type, arg0, arg1);
+	  if (t1 != NULL_TREE)
+	    return t1;
+	}
+
+      /* If the first operand is NaN, the result is constant.  */
+      if (TREE_CODE (arg0) == REAL_CST
+	  && REAL_VALUE_ISNAN (TREE_REAL_CST (arg0))
+	  && (code != LTGT_EXPR || ! flag_trapping_math))
+	{
+	  t1 = (code == ORDERED_EXPR || code == LTGT_EXPR)
+	       ? integer_zero_node
+	       : integer_one_node;
+	  return omit_one_operand (type, t1, arg1);
+	}
+
+      /* If the second operand is NaN, the result is constant.  */
+      if (TREE_CODE (arg1) == REAL_CST
+	  && REAL_VALUE_ISNAN (TREE_REAL_CST (arg1))
+	  && (code != LTGT_EXPR || ! flag_trapping_math))
+	{
+	  t1 = (code == ORDERED_EXPR || code == LTGT_EXPR)
+	       ? integer_zero_node
+	       : integer_one_node;
+	  return omit_one_operand (type, t1, arg0);
+	}
+
+      /* Simplify unordered comparison of something with itself.  */
+      if ((code == UNLE_EXPR || code == UNGE_EXPR || code == UNEQ_EXPR)
+	  && operand_equal_p (arg0, arg1, 0))
+	return constant_boolean_node (1, type);
+
+      if (code == LTGT_EXPR
+	  && !flag_trapping_math
+	  && operand_equal_p (arg0, arg1, 0))
+	return constant_boolean_node (0, type);
+
+      /* Fold (double)float1 CMP (double)float2 into float1 CMP float2.  */
+      {
+	tree targ0 = strip_float_extensions (arg0);
+	tree targ1 = strip_float_extensions (arg1);
+	tree newtype = TREE_TYPE (targ0);
+
+	if (TYPE_PRECISION (TREE_TYPE (targ1)) > TYPE_PRECISION (newtype))
+	  newtype = TREE_TYPE (targ1);
+
+	if (TYPE_PRECISION (newtype) < TYPE_PRECISION (TREE_TYPE (arg0)))
+	  return fold_build2 (code, type, fold_convert (newtype, targ0),
+			      fold_convert (newtype, targ1));
+      }
+
+      return NULL_TREE;
+
+    case COMPOUND_EXPR:
+      /* When pedantic, a compound expression can be neither an lvalue
+	 nor an integer constant expression.  */
+      if (TREE_SIDE_EFFECTS (arg0) || TREE_CONSTANT (arg1))
+	return NULL_TREE;
+      /* Don't let (0, 0) be null pointer constant.  */
+      tem = integer_zerop (arg1) ? build1 (NOP_EXPR, type, arg1)
+				 : fold_convert (type, arg1);
+      return pedantic_non_lvalue (tem);
+
+    case COMPLEX_EXPR:
+      if (wins)
+	return build_complex (type, arg0, arg1);
+      return NULL_TREE;
+
+    case ASSERT_EXPR:
+      /* An ASSERT_EXPR should never be passed to fold_binary.  */
+      gcc_unreachable ();
+
+    default:
+      return NULL_TREE;
+    } /* switch (code) */
+}
+
+/* Callback for walk_tree, looking for LABEL_EXPR.
+   Returns tree TP if it is LABEL_EXPR. Otherwise it returns NULL_TREE.
+   Do not check the sub-tree of GOTO_EXPR.  */
+
+static tree
+contains_label_1 (tree *tp,
+                  int *walk_subtrees,
+                  void *data ATTRIBUTE_UNUSED)
+{
+  switch (TREE_CODE (*tp))
+    {
+    case LABEL_EXPR:
+      return *tp;
+    case GOTO_EXPR:
+      *walk_subtrees = 0;
+    /* no break */
+    default:
+      return NULL_TREE;
+    }
+}
+
+/* Checks whether the sub-tree ST contains a label LABEL_EXPR which is
+   accessible from outside the sub-tree. Returns NULL_TREE if no
+   addressable label is found.  */
+
+static bool
+contains_label_p (tree st)
+{
+  return (walk_tree (&st, contains_label_1 , NULL, NULL) != NULL_TREE);
+}
+
+/* Fold a ternary expression of code CODE and type TYPE with operands
+   OP0, OP1, and OP2.  Return the folded expression if folding is
+   successful.  Otherwise, return NULL_TREE.  */
+
+tree
+fold_ternary (enum tree_code code, tree type, tree op0, tree op1, tree op2)
+{
+  tree tem;
+  tree arg0 = NULL_TREE, arg1 = NULL_TREE;
+  enum tree_code_class kind = TREE_CODE_CLASS (code);
+
+  gcc_assert (IS_EXPR_CODE_CLASS (kind)
+	      && TREE_CODE_LENGTH (code) == 3);
+
+  /* Strip any conversions that don't change the mode.  This is safe
+     for every expression, except for a comparison expression because
+     its signedness is derived from its operands.  So, in the latter
+     case, only strip conversions that don't change the signedness.
+
+     Note that this is done as an internal manipulation within the
+     constant folder, in order to find the simplest representation of
+     the arguments so that their form can be studied.  In any cases,
+     the appropriate type conversions should be put back in the tree
+     that will get out of the constant folder.  */
+  if (op0)
+    {
+      arg0 = op0;
+      STRIP_NOPS (arg0);
+    }
+
+  if (op1)
+    {
+      arg1 = op1;
+      STRIP_NOPS (arg1);
+    }
+
+  switch (code)
+    {
+    case COMPONENT_REF:
+      if (TREE_CODE (arg0) == CONSTRUCTOR
+	  && ! type_contains_placeholder_p (TREE_TYPE (arg0)))
+	{
+	  unsigned HOST_WIDE_INT idx;
+	  tree field, value;
+	  FOR_EACH_CONSTRUCTOR_ELT (CONSTRUCTOR_ELTS (arg0), idx, field, value)
+	    if (field == arg1)
+	      return value;
+	}
+      return NULL_TREE;
+
+    case COND_EXPR:
+      /* Pedantic ANSI C says that a conditional expression is never an lvalue,
+	 so all simple results must be passed through pedantic_non_lvalue.  */
+      if (TREE_CODE (arg0) == INTEGER_CST)
+	{
+	  tree unused_op = integer_zerop (arg0) ? op1 : op2;
+	  tem = integer_zerop (arg0) ? op2 : op1;
+	  /* Only optimize constant conditions when the selected branch
+	     has the same type as the COND_EXPR.  This avoids optimizing
+             away "c ? x : throw", where the throw has a void type.
+             Avoid throwing away that operand which contains label.  */
+          if ((!TREE_SIDE_EFFECTS (unused_op)
+               || !contains_label_p (unused_op))
+              && (! VOID_TYPE_P (TREE_TYPE (tem))
+                  || VOID_TYPE_P (type)))
+	    return pedantic_non_lvalue (tem);
+	  return NULL_TREE;
+	}
+      if (operand_equal_p (arg1, op2, 0))
+	return pedantic_omit_one_operand (type, arg1, arg0);
+
+      /* If we have A op B ? A : C, we may be able to convert this to a
+	 simpler expression, depending on the operation and the values
+	 of B and C.  Signed zeros prevent all of these transformations,
+	 for reasons given above each one.
+
+         Also try swapping the arguments and inverting the conditional.  */
+      if (COMPARISON_CLASS_P (arg0)
+	  && operand_equal_for_comparison_p (TREE_OPERAND (arg0, 0),
+					     arg1, TREE_OPERAND (arg0, 1))
+	  && !HONOR_SIGNED_ZEROS (TYPE_MODE (TREE_TYPE (arg1))))
+	{
+	  tem = fold_cond_expr_with_comparison (type, arg0, op1, op2);
+	  if (tem)
+	    return tem;
+	}
+
+      if (COMPARISON_CLASS_P (arg0)
+	  && operand_equal_for_comparison_p (TREE_OPERAND (arg0, 0),
+					     op2,
+					     TREE_OPERAND (arg0, 1))
+	  && !HONOR_SIGNED_ZEROS (TYPE_MODE (TREE_TYPE (op2))))
+	{
+	  tem = invert_truthvalue (arg0);
+	  if (COMPARISON_CLASS_P (tem))
+	    {
+	      tem = fold_cond_expr_with_comparison (type, tem, op2, op1);
+	      if (tem)
+		return tem;
+	    }
+	}
+
+      /* If the second operand is simpler than the third, swap them
+	 since that produces better jump optimization results.  */
+      if (truth_value_p (TREE_CODE (arg0))
+	  && tree_swap_operands_p (op1, op2, false))
+	{
+	  /* See if this can be inverted.  If it can't, possibly because
+	     it was a floating-point inequality comparison, don't do
+	     anything.  */
+	  tem = invert_truthvalue (arg0);
+
+	  if (TREE_CODE (tem) != TRUTH_NOT_EXPR)
+	    return fold_build3 (code, type, tem, op2, op1);
+	}
+
+      /* Convert A ? 1 : 0 to simply A.  */
+      if (integer_onep (op1)
+	  && integer_zerop (op2)
+	  /* If we try to convert OP0 to our type, the
+	     call to fold will try to move the conversion inside
+	     a COND, which will recurse.  In that case, the COND_EXPR
+	     is probably the best choice, so leave it alone.  */
+	  && type == TREE_TYPE (arg0))
+	return pedantic_non_lvalue (arg0);
+
+      /* Convert A ? 0 : 1 to !A.  This prefers the use of NOT_EXPR
+	 over COND_EXPR in cases such as floating point comparisons.  */
+      if (integer_zerop (op1)
+	  && integer_onep (op2)
+	  && truth_value_p (TREE_CODE (arg0)))
+	return pedantic_non_lvalue (fold_convert (type,
+						  invert_truthvalue (arg0)));
+
+      /* A < 0 ? <sign bit of A> : 0 is simply (A & <sign bit of A>).  */
+      if (TREE_CODE (arg0) == LT_EXPR
+          && integer_zerop (TREE_OPERAND (arg0, 1))
+          && integer_zerop (op2)
+          && (tem = sign_bit_p (TREE_OPERAND (arg0, 0), arg1)))
+        return fold_convert (type, fold_build2 (BIT_AND_EXPR,
+						TREE_TYPE (tem), tem, arg1));
+
+      /* (A >> N) & 1 ? (1 << N) : 0 is simply A & (1 << N).  A & 1 was
+	 already handled above.  */
+      if (TREE_CODE (arg0) == BIT_AND_EXPR
+	  && integer_onep (TREE_OPERAND (arg0, 1))
+	  && integer_zerop (op2)
+	  && integer_pow2p (arg1))
+	{
+	  tree tem = TREE_OPERAND (arg0, 0);
+	  STRIP_NOPS (tem);
+	  if (TREE_CODE (tem) == RSHIFT_EXPR
+              && TREE_CODE (TREE_OPERAND (tem, 1)) == INTEGER_CST
+              && (unsigned HOST_WIDE_INT) tree_log2 (arg1) ==
+	         TREE_INT_CST_LOW (TREE_OPERAND (tem, 1)))
+	    return fold_build2 (BIT_AND_EXPR, type,
+				TREE_OPERAND (tem, 0), arg1);
+	}
+
+      /* A & N ? N : 0 is simply A & N if N is a power of two.  This
+	 is probably obsolete because the first operand should be a
+	 truth value (that's why we have the two cases above), but let's
+	 leave it in until we can confirm this for all front-ends.  */
+      if (integer_zerop (op2)
+	  && TREE_CODE (arg0) == NE_EXPR
+	  && integer_zerop (TREE_OPERAND (arg0, 1))
+	  && integer_pow2p (arg1)
+	  && TREE_CODE (TREE_OPERAND (arg0, 0)) == BIT_AND_EXPR
+	  && operand_equal_p (TREE_OPERAND (TREE_OPERAND (arg0, 0), 1),
+			      arg1, OEP_ONLY_CONST))
+	return pedantic_non_lvalue (fold_convert (type,
+						  TREE_OPERAND (arg0, 0)));
+
+      /* Convert A ? B : 0 into A && B if A and B are truth values.  */
+      if (integer_zerop (op2)
+	  && truth_value_p (TREE_CODE (arg0))
+	  && truth_value_p (TREE_CODE (arg1)))
+	return fold_build2 (TRUTH_ANDIF_EXPR, type,
+			    fold_convert (type, arg0),
+			    arg1);
+
+      /* Convert A ? B : 1 into !A || B if A and B are truth values.  */
+      if (integer_onep (op2)
+	  && truth_value_p (TREE_CODE (arg0))
+	  && truth_value_p (TREE_CODE (arg1)))
+	{
+	  /* Only perform transformation if ARG0 is easily inverted.  */
+	  tem = invert_truthvalue (arg0);
+	  if (TREE_CODE (tem) != TRUTH_NOT_EXPR)
+	    return fold_build2 (TRUTH_ORIF_EXPR, type,
+				fold_convert (type, tem),
+				arg1);
+	}
+
+      /* Convert A ? 0 : B into !A && B if A and B are truth values.  */
+      if (integer_zerop (arg1)
+	  && truth_value_p (TREE_CODE (arg0))
+	  && truth_value_p (TREE_CODE (op2)))
+	{
+	  /* Only perform transformation if ARG0 is easily inverted.  */
+	  tem = invert_truthvalue (arg0);
+	  if (TREE_CODE (tem) != TRUTH_NOT_EXPR)
+	    return fold_build2 (TRUTH_ANDIF_EXPR, type,
+				fold_convert (type, tem),
+				op2);
+	}
+
+      /* Convert A ? 1 : B into A || B if A and B are truth values.  */
+      if (integer_onep (arg1)
+	  && truth_value_p (TREE_CODE (arg0))
+	  && truth_value_p (TREE_CODE (op2)))
+	return fold_build2 (TRUTH_ORIF_EXPR, type,
+			    fold_convert (type, arg0),
+			    op2);
+
+      return NULL_TREE;
+
+    case CALL_EXPR:
+      /* Check for a built-in function.  */
+      if (TREE_CODE (op0) == ADDR_EXPR
+	  && TREE_CODE (TREE_OPERAND (op0, 0)) == FUNCTION_DECL
+	  && DECL_BUILT_IN (TREE_OPERAND (op0, 0)))
+	return fold_builtin (TREE_OPERAND (op0, 0), op1, false);
+      return NULL_TREE;
+
+    case BIT_FIELD_REF:
+      if (TREE_CODE (arg0) == VECTOR_CST
+	  && type == TREE_TYPE (TREE_TYPE (arg0))
+	  && host_integerp (arg1, 1)
+	  && host_integerp (op2, 1))
+	{
+	  unsigned HOST_WIDE_INT width = tree_low_cst (arg1, 1);
+	  unsigned HOST_WIDE_INT idx = tree_low_cst (op2, 1);
+
+	  if (width != 0
+	      && simple_cst_equal (arg1, TYPE_SIZE (type)) == 1
+	      && (idx % width) == 0
+	      && (idx = idx / width)
+		 < TYPE_VECTOR_SUBPARTS (TREE_TYPE (arg0)))
+	    {
+	      tree elements = TREE_VECTOR_CST_ELTS (arg0);
+	      while (idx-- > 0 && elements)
+		elements = TREE_CHAIN (elements);
+	      if (elements)
+		return TREE_VALUE (elements);
+	      else
+		return fold_convert (type, integer_zero_node);
+	    }
+	}
+      return NULL_TREE;
+
+    default:
+      return NULL_TREE;
+    } /* switch (code) */
+}
+
+/* Perform constant folding and related simplification of EXPR.
+   The related simplifications include x*1 => x, x*0 => 0, etc.,
+   and application of the associative law.
+   NOP_EXPR conversions may be removed freely (as long as we
+   are careful not to change the type of the overall expression).
+   We cannot simplify through a CONVERT_EXPR, FIX_EXPR or FLOAT_EXPR,
+   but we can constant-fold them if they have constant operands.  */
+
+#ifdef ENABLE_FOLD_CHECKING
+# define fold(x) fold_1 (x)
+static tree fold_1 (tree);
+static
+#endif
+tree
+fold (tree expr)
+{
+  const tree t = expr;
+  enum tree_code code = TREE_CODE (t);
+  enum tree_code_class kind = TREE_CODE_CLASS (code);
+  tree tem;
+
+  /* Return right away if a constant.  */
+  if (kind == tcc_constant)
+    return t;
+
+  if (IS_EXPR_CODE_CLASS (kind))
+    {
+      tree type = TREE_TYPE (t);
+      tree op0, op1, op2;
+
+      switch (TREE_CODE_LENGTH (code))
+	{
+	case 1:
+	  op0 = TREE_OPERAND (t, 0);
+	  tem = fold_unary (code, type, op0);
+	  return tem ? tem : expr;
+	case 2:
+	  op0 = TREE_OPERAND (t, 0);
+	  op1 = TREE_OPERAND (t, 1);
+	  tem = fold_binary (code, type, op0, op1);
+	  return tem ? tem : expr;
+	case 3:
+	  op0 = TREE_OPERAND (t, 0);
+	  op1 = TREE_OPERAND (t, 1);
+	  op2 = TREE_OPERAND (t, 2);
+	  tem = fold_ternary (code, type, op0, op1, op2);
+	  return tem ? tem : expr;
+	default:
+	  break;
+	}
+    }
+
+  switch (code)
+    {
+    case CONST_DECL:
+      return fold (DECL_INITIAL (t));
+
+    default:
+      return t;
+    } /* switch (code) */
+}
+
+#ifdef ENABLE_FOLD_CHECKING
+#undef fold
+
+static void fold_checksum_tree (tree, struct md5_ctx *, htab_t);
+static void fold_check_failed (tree, tree);
+void print_fold_checksum (tree);
+
+/* When --enable-checking=fold, compute a digest of expr before
+   and after actual fold call to see if fold did not accidentally
+   change original expr.  */
+
+tree
+fold (tree expr)
+{
+  tree ret;
+  struct md5_ctx ctx;
+  unsigned char checksum_before[16], checksum_after[16];
+  htab_t ht;
+
+  ht = htab_create (32, htab_hash_pointer, htab_eq_pointer, NULL);
+  md5_init_ctx (&ctx);
+  fold_checksum_tree (expr, &ctx, ht);
+  md5_finish_ctx (&ctx, checksum_before);
+  htab_empty (ht);
+
+  ret = fold_1 (expr);
+
+  md5_init_ctx (&ctx);
+  fold_checksum_tree (expr, &ctx, ht);
+  md5_finish_ctx (&ctx, checksum_after);
+  htab_delete (ht);
+
+  if (memcmp (checksum_before, checksum_after, 16))
+    fold_check_failed (expr, ret);
+
+  return ret;
+}
+
+void
+print_fold_checksum (tree expr)
+{
+  struct md5_ctx ctx;
+  unsigned char checksum[16], cnt;
+  htab_t ht;
+
+  ht = htab_create (32, htab_hash_pointer, htab_eq_pointer, NULL);
+  md5_init_ctx (&ctx);
+  fold_checksum_tree (expr, &ctx, ht);
+  md5_finish_ctx (&ctx, checksum);
+  htab_delete (ht);
+  for (cnt = 0; cnt < 16; ++cnt)
+    fprintf (stderr, "%02x", checksum[cnt]);
+  putc ('\n', stderr);
+}
+
+static void
+fold_check_failed (tree expr ATTRIBUTE_UNUSED, tree ret ATTRIBUTE_UNUSED)
+{
+  internal_error ("fold check: original tree changed by fold");
+}
+
+static void
+fold_checksum_tree (tree expr, struct md5_ctx *ctx, htab_t ht)
+{
+  void **slot;
+  enum tree_code code;
+  struct tree_function_decl buf;
+  int i, len;
+  
+recursive_label:
+
+  gcc_assert ((sizeof (struct tree_exp) + 5 * sizeof (tree)
+	       <= sizeof (struct tree_function_decl))
+	      && sizeof (struct tree_type) <= sizeof (struct tree_function_decl));
+  if (expr == NULL)
+    return;
+  slot = htab_find_slot (ht, expr, INSERT);
+  if (*slot != NULL)
+    return;
+  *slot = expr;
+  code = TREE_CODE (expr);
+  if (TREE_CODE_CLASS (code) == tcc_declaration
+      && DECL_ASSEMBLER_NAME_SET_P (expr))
+    {
+      /* Allow DECL_ASSEMBLER_NAME to be modified.  */
+      memcpy ((char *) &buf, expr, tree_size (expr));
+      expr = (tree) &buf;
+      SET_DECL_ASSEMBLER_NAME (expr, NULL);
+    }
+  else if (TREE_CODE_CLASS (code) == tcc_type
+	   && (TYPE_POINTER_TO (expr) || TYPE_REFERENCE_TO (expr)
+	       || TYPE_CACHED_VALUES_P (expr)
+	       || TYPE_CONTAINS_PLACEHOLDER_INTERNAL (expr)))
+    {
+      /* Allow these fields to be modified.  */
+      memcpy ((char *) &buf, expr, tree_size (expr));
+      expr = (tree) &buf;
+      TYPE_CONTAINS_PLACEHOLDER_INTERNAL (expr) = 0;
+      TYPE_POINTER_TO (expr) = NULL;
+      TYPE_REFERENCE_TO (expr) = NULL;
+      if (TYPE_CACHED_VALUES_P (expr))
+	{
+	  TYPE_CACHED_VALUES_P (expr) = 0;
+	  TYPE_CACHED_VALUES (expr) = NULL;
+	}
+    }
+  md5_process_bytes (expr, tree_size (expr), ctx);
+  fold_checksum_tree (TREE_TYPE (expr), ctx, ht);
+  if (TREE_CODE_CLASS (code) != tcc_type
+      && TREE_CODE_CLASS (code) != tcc_declaration
+      && code != TREE_LIST)
+    fold_checksum_tree (TREE_CHAIN (expr), ctx, ht);
+  switch (TREE_CODE_CLASS (code))
+    {
+    case tcc_constant:
+      switch (code)
+	{
+	case STRING_CST:
+	  md5_process_bytes (TREE_STRING_POINTER (expr),
+			     TREE_STRING_LENGTH (expr), ctx);
+	  break;
+	case COMPLEX_CST:
+	  fold_checksum_tree (TREE_REALPART (expr), ctx, ht);
+	  fold_checksum_tree (TREE_IMAGPART (expr), ctx, ht);
+	  break;
+	case VECTOR_CST:
+	  fold_checksum_tree (TREE_VECTOR_CST_ELTS (expr), ctx, ht);
+	  break;
+	default:
+	  break;
+	}
+      break;
+    case tcc_exceptional:
+      switch (code)
+	{
+	case TREE_LIST:
+	  fold_checksum_tree (TREE_PURPOSE (expr), ctx, ht);
+	  fold_checksum_tree (TREE_VALUE (expr), ctx, ht);
+	  expr = TREE_CHAIN (expr);
+	  goto recursive_label;
+	  break;
+	case TREE_VEC:
+	  for (i = 0; i < TREE_VEC_LENGTH (expr); ++i)
+	    fold_checksum_tree (TREE_VEC_ELT (expr, i), ctx, ht);
+	  break;
+	default:
+	  break;
+	}
+      break;
+    case tcc_expression:
+    case tcc_reference:
+    case tcc_comparison:
+    case tcc_unary:
+    case tcc_binary:
+    case tcc_statement:
+      len = TREE_CODE_LENGTH (code);
+      for (i = 0; i < len; ++i)
+	fold_checksum_tree (TREE_OPERAND (expr, i), ctx, ht);
+      break;
+    case tcc_declaration:
+      fold_checksum_tree (DECL_SIZE (expr), ctx, ht);
+      fold_checksum_tree (DECL_SIZE_UNIT (expr), ctx, ht);
+      fold_checksum_tree (DECL_NAME (expr), ctx, ht);
+      fold_checksum_tree (DECL_CONTEXT (expr), ctx, ht);
+      fold_checksum_tree (DECL_INITIAL (expr), ctx, ht);
+      fold_checksum_tree (DECL_ABSTRACT_ORIGIN (expr), ctx, ht);
+      fold_checksum_tree (DECL_ATTRIBUTES (expr), ctx, ht);
+      if (CODE_CONTAINS_STRUCT (TREE_CODE (expr), TS_DECL_WITH_VIS))
+	fold_checksum_tree (DECL_SECTION_NAME (expr), ctx, ht);
+	  
+      if (CODE_CONTAINS_STRUCT (TREE_CODE (expr), TS_DECL_NON_COMMON))
+	{
+	  fold_checksum_tree (DECL_VINDEX (expr), ctx, ht);
+	  fold_checksum_tree (DECL_RESULT_FLD (expr), ctx, ht);
+	  fold_checksum_tree (DECL_ARGUMENT_FLD (expr), ctx, ht);
+	}
+      break;
+    case tcc_type:
+      if (TREE_CODE (expr) == ENUMERAL_TYPE)
+        fold_checksum_tree (TYPE_VALUES (expr), ctx, ht);
+      fold_checksum_tree (TYPE_SIZE (expr), ctx, ht);
+      fold_checksum_tree (TYPE_SIZE_UNIT (expr), ctx, ht);
+      fold_checksum_tree (TYPE_ATTRIBUTES (expr), ctx, ht);
+      fold_checksum_tree (TYPE_NAME (expr), ctx, ht);
+      if (INTEGRAL_TYPE_P (expr)
+          || SCALAR_FLOAT_TYPE_P (expr))
+	{
+	  fold_checksum_tree (TYPE_MIN_VALUE (expr), ctx, ht);
+	  fold_checksum_tree (TYPE_MAX_VALUE (expr), ctx, ht);
+	}
+      fold_checksum_tree (TYPE_MAIN_VARIANT (expr), ctx, ht);
+      if (TREE_CODE (expr) == RECORD_TYPE
+	  || TREE_CODE (expr) == UNION_TYPE
+	  || TREE_CODE (expr) == QUAL_UNION_TYPE)
+	fold_checksum_tree (TYPE_BINFO (expr), ctx, ht);
+      fold_checksum_tree (TYPE_CONTEXT (expr), ctx, ht);
+      break;
+    default:
+      break;
+    }
+}
+
+#endif
+
+/* Fold a unary tree expression with code CODE of type TYPE with an
+   operand OP0.  Return a folded expression if successful.  Otherwise,
+   return a tree expression with code CODE of type TYPE with an
+   operand OP0.  */
+
+tree
+fold_build1_stat (enum tree_code code, tree type, tree op0 MEM_STAT_DECL)
+{
+  tree tem;
+#ifdef ENABLE_FOLD_CHECKING
+  unsigned char checksum_before[16], checksum_after[16];
+  struct md5_ctx ctx;
+  htab_t ht;
+
+  ht = htab_create (32, htab_hash_pointer, htab_eq_pointer, NULL);
+  md5_init_ctx (&ctx);
+  fold_checksum_tree (op0, &ctx, ht);
+  md5_finish_ctx (&ctx, checksum_before);
+  htab_empty (ht);
+#endif
+  
+  tem = fold_unary (code, type, op0);
+  if (!tem)
+    tem = build1_stat (code, type, op0 PASS_MEM_STAT);
+  
+#ifdef ENABLE_FOLD_CHECKING
+  md5_init_ctx (&ctx);
+  fold_checksum_tree (op0, &ctx, ht);
+  md5_finish_ctx (&ctx, checksum_after);
+  htab_delete (ht);
+
+  if (memcmp (checksum_before, checksum_after, 16))
+    fold_check_failed (op0, tem);
+#endif
+  return tem;
+}
+
+/* Fold a binary tree expression with code CODE of type TYPE with
+   operands OP0 and OP1.  Return a folded expression if successful.
+   Otherwise, return a tree expression with code CODE of type TYPE
+   with operands OP0 and OP1.  */
+
+tree
+fold_build2_stat (enum tree_code code, tree type, tree op0, tree op1
+		  MEM_STAT_DECL)
+{
+  tree tem;
+#ifdef ENABLE_FOLD_CHECKING
+  unsigned char checksum_before_op0[16],
+                checksum_before_op1[16],
+		checksum_after_op0[16],
+		checksum_after_op1[16];
+  struct md5_ctx ctx;
+  htab_t ht;
+
+  ht = htab_create (32, htab_hash_pointer, htab_eq_pointer, NULL);
+  md5_init_ctx (&ctx);
+  fold_checksum_tree (op0, &ctx, ht);
+  md5_finish_ctx (&ctx, checksum_before_op0);
+  htab_empty (ht);
+
+  md5_init_ctx (&ctx);
+  fold_checksum_tree (op1, &ctx, ht);
+  md5_finish_ctx (&ctx, checksum_before_op1);
+  htab_empty (ht);
+#endif
+
+  tem = fold_binary (code, type, op0, op1);
+  if (!tem)
+    tem = build2_stat (code, type, op0, op1 PASS_MEM_STAT);
+  
+#ifdef ENABLE_FOLD_CHECKING
+  md5_init_ctx (&ctx);
+  fold_checksum_tree (op0, &ctx, ht);
+  md5_finish_ctx (&ctx, checksum_after_op0);
+  htab_empty (ht);
+
+  if (memcmp (checksum_before_op0, checksum_after_op0, 16))
+    fold_check_failed (op0, tem);
+  
+  md5_init_ctx (&ctx);
+  fold_checksum_tree (op1, &ctx, ht);
+  md5_finish_ctx (&ctx, checksum_after_op1);
+  htab_delete (ht);
+
+  if (memcmp (checksum_before_op1, checksum_after_op1, 16))
+    fold_check_failed (op1, tem);
+#endif
+  return tem;
+}
+
+/* Fold a ternary tree expression with code CODE of type TYPE with
+   operands OP0, OP1, and OP2.  Return a folded expression if
+   successful.  Otherwise, return a tree expression with code CODE of
+   type TYPE with operands OP0, OP1, and OP2.  */
+
+tree
+fold_build3_stat (enum tree_code code, tree type, tree op0, tree op1, tree op2
+	     MEM_STAT_DECL)
+{
+  tree tem;
+#ifdef ENABLE_FOLD_CHECKING
+  unsigned char checksum_before_op0[16],
+                checksum_before_op1[16],
+                checksum_before_op2[16],
+		checksum_after_op0[16],
+		checksum_after_op1[16],
+		checksum_after_op2[16];
+  struct md5_ctx ctx;
+  htab_t ht;
+
+  ht = htab_create (32, htab_hash_pointer, htab_eq_pointer, NULL);
+  md5_init_ctx (&ctx);
+  fold_checksum_tree (op0, &ctx, ht);
+  md5_finish_ctx (&ctx, checksum_before_op0);
+  htab_empty (ht);
+
+  md5_init_ctx (&ctx);
+  fold_checksum_tree (op1, &ctx, ht);
+  md5_finish_ctx (&ctx, checksum_before_op1);
+  htab_empty (ht);
+
+  md5_init_ctx (&ctx);
+  fold_checksum_tree (op2, &ctx, ht);
+  md5_finish_ctx (&ctx, checksum_before_op2);
+  htab_empty (ht);
+#endif
+  
+  tem = fold_ternary (code, type, op0, op1, op2);
+  if (!tem)
+    tem =  build3_stat (code, type, op0, op1, op2 PASS_MEM_STAT);
+      
+#ifdef ENABLE_FOLD_CHECKING
+  md5_init_ctx (&ctx);
+  fold_checksum_tree (op0, &ctx, ht);
+  md5_finish_ctx (&ctx, checksum_after_op0);
+  htab_empty (ht);
+
+  if (memcmp (checksum_before_op0, checksum_after_op0, 16))
+    fold_check_failed (op0, tem);
+  
+  md5_init_ctx (&ctx);
+  fold_checksum_tree (op1, &ctx, ht);
+  md5_finish_ctx (&ctx, checksum_after_op1);
+  htab_empty (ht);
+
+  if (memcmp (checksum_before_op1, checksum_after_op1, 16))
+    fold_check_failed (op1, tem);
+  
+  md5_init_ctx (&ctx);
+  fold_checksum_tree (op2, &ctx, ht);
+  md5_finish_ctx (&ctx, checksum_after_op2);
+  htab_delete (ht);
+
+  if (memcmp (checksum_before_op2, checksum_after_op2, 16))
+    fold_check_failed (op2, tem);
+#endif
+  return tem;
+}
+
+/* Perform constant folding and related simplification of initializer
+   expression EXPR.  These behave identically to "fold_buildN" but ignore
+   potential run-time traps and exceptions that fold must preserve.  */
+
+#define START_FOLD_INIT \
+  int saved_signaling_nans = flag_signaling_nans;\
+  int saved_trapping_math = flag_trapping_math;\
+  int saved_rounding_math = flag_rounding_math;\
+  int saved_trapv = flag_trapv;\
+  flag_signaling_nans = 0;\
+  flag_trapping_math = 0;\
+  flag_rounding_math = 0;\
+  flag_trapv = 0
+
+#define END_FOLD_INIT \
+  flag_signaling_nans = saved_signaling_nans;\
+  flag_trapping_math = saved_trapping_math;\
+  flag_rounding_math = saved_rounding_math;\
+  flag_trapv = saved_trapv
+
+tree
+fold_build1_initializer (enum tree_code code, tree type, tree op)
+{
+  tree result;
+  START_FOLD_INIT;
+
+  result = fold_build1 (code, type, op);
+
+  END_FOLD_INIT;
+  return result;
+}
+
+tree
+fold_build2_initializer (enum tree_code code, tree type, tree op0, tree op1)
+{
+  tree result;
+  START_FOLD_INIT;
+
+  result = fold_build2 (code, type, op0, op1);
+
+  END_FOLD_INIT;
+  return result;
+}
+
+tree
+fold_build3_initializer (enum tree_code code, tree type, tree op0, tree op1,
+			 tree op2)
+{
+  tree result;
+  START_FOLD_INIT;
+
+  result = fold_build3 (code, type, op0, op1, op2);
+
+  END_FOLD_INIT;
+  return result;
+}
+
+#undef START_FOLD_INIT
+#undef END_FOLD_INIT
+
+/* Determine if first argument is a multiple of second argument.  Return 0 if
+   it is not, or we cannot easily determined it to be.
+
+   An example of the sort of thing we care about (at this point; this routine
+   could surely be made more general, and expanded to do what the *_DIV_EXPR's
+   fold cases do now) is discovering that
+
+     SAVE_EXPR (I) * SAVE_EXPR (J * 8)
+
+   is a multiple of
+
+     SAVE_EXPR (J * 8)
+
+   when we know that the two SAVE_EXPR (J * 8) nodes are the same node.
+
+   This code also handles discovering that
+
+     SAVE_EXPR (I) * SAVE_EXPR (J * 8)
+
+   is a multiple of 8 so we don't have to worry about dealing with a
+   possible remainder.
+
+   Note that we *look* inside a SAVE_EXPR only to determine how it was
+   calculated; it is not safe for fold to do much of anything else with the
+   internals of a SAVE_EXPR, since it cannot know when it will be evaluated
+   at run time.  For example, the latter example above *cannot* be implemented
+   as SAVE_EXPR (I) * J or any variant thereof, since the value of J at
+   evaluation time of the original SAVE_EXPR is not necessarily the same at
+   the time the new expression is evaluated.  The only optimization of this
+   sort that would be valid is changing
+
+     SAVE_EXPR (I) * SAVE_EXPR (SAVE_EXPR (J) * 8)
+
+   divided by 8 to
+
+     SAVE_EXPR (I) * SAVE_EXPR (J)
+
+   (where the same SAVE_EXPR (J) is used in the original and the
+   transformed version).  */
+
+static int
+multiple_of_p (tree type, tree top, tree bottom)
+{
+  if (operand_equal_p (top, bottom, 0))
+    return 1;
+
+  if (TREE_CODE (type) != INTEGER_TYPE)
+    return 0;
+
+  switch (TREE_CODE (top))
+    {
+    case BIT_AND_EXPR:
+      /* Bitwise and provides a power of two multiple.  If the mask is
+	 a multiple of BOTTOM then TOP is a multiple of BOTTOM.  */
+      if (!integer_pow2p (bottom))
+	return 0;
+      /* FALLTHRU */
+
+    case MULT_EXPR:
+      return (multiple_of_p (type, TREE_OPERAND (top, 0), bottom)
+	      || multiple_of_p (type, TREE_OPERAND (top, 1), bottom));
+
+    case PLUS_EXPR:
+    case MINUS_EXPR:
+      return (multiple_of_p (type, TREE_OPERAND (top, 0), bottom)
+	      && multiple_of_p (type, TREE_OPERAND (top, 1), bottom));
+
+    case LSHIFT_EXPR:
+      if (TREE_CODE (TREE_OPERAND (top, 1)) == INTEGER_CST)
+	{
+	  tree op1, t1;
+
+	  op1 = TREE_OPERAND (top, 1);
+	  /* const_binop may not detect overflow correctly,
+	     so check for it explicitly here.  */
+	  if (TYPE_PRECISION (TREE_TYPE (size_one_node))
+	      > TREE_INT_CST_LOW (op1)
+	      && TREE_INT_CST_HIGH (op1) == 0
+	      && 0 != (t1 = fold_convert (type,
+					  const_binop (LSHIFT_EXPR,
+						       size_one_node,
+						       op1, 0)))
+	      && ! TREE_OVERFLOW (t1))
+	    return multiple_of_p (type, t1, bottom);
+	}
+      return 0;
+
+    case NOP_EXPR:
+      /* Can't handle conversions from non-integral or wider integral type.  */
+      if ((TREE_CODE (TREE_TYPE (TREE_OPERAND (top, 0))) != INTEGER_TYPE)
+	  || (TYPE_PRECISION (type)
+	      < TYPE_PRECISION (TREE_TYPE (TREE_OPERAND (top, 0)))))
+	return 0;
+
+      /* .. fall through ...  */
+
+    case SAVE_EXPR:
+      return multiple_of_p (type, TREE_OPERAND (top, 0), bottom);
+
+    case INTEGER_CST:
+      if (TREE_CODE (bottom) != INTEGER_CST
+	  || (TYPE_UNSIGNED (type)
+	      && (tree_int_cst_sgn (top) < 0
+		  || tree_int_cst_sgn (bottom) < 0)))
+	return 0;
+      return integer_zerop (const_binop (TRUNC_MOD_EXPR,
+					 top, bottom, 0));
+
+    default:
+      return 0;
+    }
+}
+
+/* Return true if `t' is known to be non-negative.  */
+
+int
+tree_expr_nonnegative_p (tree t)
+{
+  if (t == error_mark_node)
+    return 0;
+
+  if (TYPE_UNSIGNED (TREE_TYPE (t)))
+    return 1;
+
+  switch (TREE_CODE (t))
+    {
+    case ABS_EXPR:
+      /* We can't return 1 if flag_wrapv is set because
+	 ABS_EXPR<INT_MIN> = INT_MIN.  */
+      if (!(flag_wrapv && INTEGRAL_TYPE_P (TREE_TYPE (t))))
+        return 1;
+      break;
+
+    case INTEGER_CST:
+      return tree_int_cst_sgn (t) >= 0;
+
+    case REAL_CST:
+      return ! REAL_VALUE_NEGATIVE (TREE_REAL_CST (t));
+
+    case PLUS_EXPR:
+      if (FLOAT_TYPE_P (TREE_TYPE (t)))
+	return tree_expr_nonnegative_p (TREE_OPERAND (t, 0))
+	       && tree_expr_nonnegative_p (TREE_OPERAND (t, 1));
+
+      /* zero_extend(x) + zero_extend(y) is non-negative if x and y are
+	 both unsigned and at least 2 bits shorter than the result.  */
+      if (TREE_CODE (TREE_TYPE (t)) == INTEGER_TYPE
+	  && TREE_CODE (TREE_OPERAND (t, 0)) == NOP_EXPR
+	  && TREE_CODE (TREE_OPERAND (t, 1)) == NOP_EXPR)
+	{
+	  tree inner1 = TREE_TYPE (TREE_OPERAND (TREE_OPERAND (t, 0), 0));
+	  tree inner2 = TREE_TYPE (TREE_OPERAND (TREE_OPERAND (t, 1), 0));
+	  if (TREE_CODE (inner1) == INTEGER_TYPE && TYPE_UNSIGNED (inner1)
+	      && TREE_CODE (inner2) == INTEGER_TYPE && TYPE_UNSIGNED (inner2))
+	    {
+	      unsigned int prec = MAX (TYPE_PRECISION (inner1),
+				       TYPE_PRECISION (inner2)) + 1;
+	      return prec < TYPE_PRECISION (TREE_TYPE (t));
+	    }
+	}
+      break;
+
+    case MULT_EXPR:
+      if (FLOAT_TYPE_P (TREE_TYPE (t)))
+	{
+	  /* x * x for floating point x is always non-negative.  */
+	  if (operand_equal_p (TREE_OPERAND (t, 0), TREE_OPERAND (t, 1), 0))
+	    return 1;
+	  return tree_expr_nonnegative_p (TREE_OPERAND (t, 0))
+		 && tree_expr_nonnegative_p (TREE_OPERAND (t, 1));
+	}
+
+      /* zero_extend(x) * zero_extend(y) is non-negative if x and y are
+	 both unsigned and their total bits is shorter than the result.  */
+      if (TREE_CODE (TREE_TYPE (t)) == INTEGER_TYPE
+	  && TREE_CODE (TREE_OPERAND (t, 0)) == NOP_EXPR
+	  && TREE_CODE (TREE_OPERAND (t, 1)) == NOP_EXPR)
+	{
+	  tree inner1 = TREE_TYPE (TREE_OPERAND (TREE_OPERAND (t, 0), 0));
+	  tree inner2 = TREE_TYPE (TREE_OPERAND (TREE_OPERAND (t, 1), 0));
+	  if (TREE_CODE (inner1) == INTEGER_TYPE && TYPE_UNSIGNED (inner1)
+	      && TREE_CODE (inner2) == INTEGER_TYPE && TYPE_UNSIGNED (inner2))
+	    return TYPE_PRECISION (inner1) + TYPE_PRECISION (inner2)
+		   < TYPE_PRECISION (TREE_TYPE (t));
+	}
+      return 0;
+
+    case BIT_AND_EXPR:
+    case MAX_EXPR:
+      return tree_expr_nonnegative_p (TREE_OPERAND (t, 0))
+	     || tree_expr_nonnegative_p (TREE_OPERAND (t, 1));
+
+    case BIT_IOR_EXPR:
+    case BIT_XOR_EXPR:
+    case MIN_EXPR:
+    case RDIV_EXPR:
+    case TRUNC_DIV_EXPR:
+    case CEIL_DIV_EXPR:
+    case FLOOR_DIV_EXPR:
+    case ROUND_DIV_EXPR:
+      return tree_expr_nonnegative_p (TREE_OPERAND (t, 0))
+	     && tree_expr_nonnegative_p (TREE_OPERAND (t, 1));
+
+    case TRUNC_MOD_EXPR:
+    case CEIL_MOD_EXPR:
+    case FLOOR_MOD_EXPR:
+    case ROUND_MOD_EXPR:
+    case SAVE_EXPR:
+    case NON_LVALUE_EXPR:
+    case FLOAT_EXPR:
+      return tree_expr_nonnegative_p (TREE_OPERAND (t, 0));
+
+    case COMPOUND_EXPR:
+    case MODIFY_EXPR:
+      return tree_expr_nonnegative_p (TREE_OPERAND (t, 1));
+
+    case BIND_EXPR:
+      return tree_expr_nonnegative_p (expr_last (TREE_OPERAND (t, 1)));
+
+    case COND_EXPR:
+      return tree_expr_nonnegative_p (TREE_OPERAND (t, 1))
+	     && tree_expr_nonnegative_p (TREE_OPERAND (t, 2));
+
+    case NOP_EXPR:
+      {
+	tree inner_type = TREE_TYPE (TREE_OPERAND (t, 0));
+	tree outer_type = TREE_TYPE (t);
+
+	if (TREE_CODE (outer_type) == REAL_TYPE)
+	  {
+	    if (TREE_CODE (inner_type) == REAL_TYPE)
+	      return tree_expr_nonnegative_p (TREE_OPERAND (t, 0));
+	    if (TREE_CODE (inner_type) == INTEGER_TYPE)
+	      {
+		if (TYPE_UNSIGNED (inner_type))
+		  return 1;
+		return tree_expr_nonnegative_p (TREE_OPERAND (t, 0));
+	      }
+	  }
+	else if (TREE_CODE (outer_type) == INTEGER_TYPE)
+	  {
+	    if (TREE_CODE (inner_type) == REAL_TYPE)
+	      return tree_expr_nonnegative_p (TREE_OPERAND (t,0));
+	    if (TREE_CODE (inner_type) == INTEGER_TYPE)
+	      return TYPE_PRECISION (inner_type) < TYPE_PRECISION (outer_type)
+		      && TYPE_UNSIGNED (inner_type);
+	  }
+      }
+      break;
+
+    case TARGET_EXPR:
+      {
+	tree temp = TARGET_EXPR_SLOT (t);
+	t = TARGET_EXPR_INITIAL (t);
+
+	/* If the initializer is non-void, then it's a normal expression
+	   that will be assigned to the slot.  */
+	if (!VOID_TYPE_P (t))
+	  return tree_expr_nonnegative_p (t);
+
+	/* Otherwise, the initializer sets the slot in some way.  One common
+	   way is an assignment statement at the end of the initializer.  */
+	while (1)
+	  {
+	    if (TREE_CODE (t) == BIND_EXPR)
+	      t = expr_last (BIND_EXPR_BODY (t));
+	    else if (TREE_CODE (t) == TRY_FINALLY_EXPR
+		     || TREE_CODE (t) == TRY_CATCH_EXPR)
+	      t = expr_last (TREE_OPERAND (t, 0));
+	    else if (TREE_CODE (t) == STATEMENT_LIST)
+	      t = expr_last (t);
+	    else
+	      break;
+	  }
+	if (TREE_CODE (t) == MODIFY_EXPR
+	    && TREE_OPERAND (t, 0) == temp)
+	  return tree_expr_nonnegative_p (TREE_OPERAND (t, 1));
+
+	return 0;
+      }
+
+    case CALL_EXPR:
+      {
+	tree fndecl = get_callee_fndecl (t);
+	tree arglist = TREE_OPERAND (t, 1);
+	if (fndecl && DECL_BUILT_IN_CLASS (fndecl) == BUILT_IN_NORMAL)
+	  switch (DECL_FUNCTION_CODE (fndecl))
+	    {
+#define CASE_BUILTIN_F(BUILT_IN_FN) \
+  case BUILT_IN_FN: case BUILT_IN_FN##F: case BUILT_IN_FN##L:
+#define CASE_BUILTIN_I(BUILT_IN_FN) \
+  case BUILT_IN_FN: case BUILT_IN_FN##L: case BUILT_IN_FN##LL:
+
+	    CASE_BUILTIN_F (BUILT_IN_ACOS)
+	    CASE_BUILTIN_F (BUILT_IN_ACOSH)
+	    CASE_BUILTIN_F (BUILT_IN_CABS)
+	    CASE_BUILTIN_F (BUILT_IN_COSH)
+	    CASE_BUILTIN_F (BUILT_IN_ERFC)
+	    CASE_BUILTIN_F (BUILT_IN_EXP)
+	    CASE_BUILTIN_F (BUILT_IN_EXP10)
+	    CASE_BUILTIN_F (BUILT_IN_EXP2)
+	    CASE_BUILTIN_F (BUILT_IN_FABS)
+	    CASE_BUILTIN_F (BUILT_IN_FDIM)
+	    CASE_BUILTIN_F (BUILT_IN_HYPOT)
+	    CASE_BUILTIN_F (BUILT_IN_POW10)
+	    CASE_BUILTIN_I (BUILT_IN_FFS)
+	    CASE_BUILTIN_I (BUILT_IN_PARITY)
+	    CASE_BUILTIN_I (BUILT_IN_POPCOUNT)
+	      /* Always true.  */
+	      return 1;
+
+	    CASE_BUILTIN_F (BUILT_IN_SQRT)
+	      /* sqrt(-0.0) is -0.0.  */
+	      if (!HONOR_SIGNED_ZEROS (TYPE_MODE (TREE_TYPE (t))))
+		return 1;
+	      return tree_expr_nonnegative_p (TREE_VALUE (arglist));
+
+	    CASE_BUILTIN_F (BUILT_IN_ASINH)
+	    CASE_BUILTIN_F (BUILT_IN_ATAN)
+	    CASE_BUILTIN_F (BUILT_IN_ATANH)
+	    CASE_BUILTIN_F (BUILT_IN_CBRT)
+	    CASE_BUILTIN_F (BUILT_IN_CEIL)
+	    CASE_BUILTIN_F (BUILT_IN_ERF)
+	    CASE_BUILTIN_F (BUILT_IN_EXPM1)
+	    CASE_BUILTIN_F (BUILT_IN_FLOOR)
+	    CASE_BUILTIN_F (BUILT_IN_FMOD)
+	    CASE_BUILTIN_F (BUILT_IN_FREXP)
+	    CASE_BUILTIN_F (BUILT_IN_LCEIL)
+	    CASE_BUILTIN_F (BUILT_IN_LDEXP)
+	    CASE_BUILTIN_F (BUILT_IN_LFLOOR)
+	    CASE_BUILTIN_F (BUILT_IN_LLCEIL)
+	    CASE_BUILTIN_F (BUILT_IN_LLFLOOR)
+	    CASE_BUILTIN_F (BUILT_IN_LLRINT)
+	    CASE_BUILTIN_F (BUILT_IN_LLROUND)
+	    CASE_BUILTIN_F (BUILT_IN_LRINT)
+	    CASE_BUILTIN_F (BUILT_IN_LROUND)
+	    CASE_BUILTIN_F (BUILT_IN_MODF)
+	    CASE_BUILTIN_F (BUILT_IN_NEARBYINT)
+	    CASE_BUILTIN_F (BUILT_IN_POW)
+	    CASE_BUILTIN_F (BUILT_IN_RINT)
+	    CASE_BUILTIN_F (BUILT_IN_ROUND)
+	    CASE_BUILTIN_F (BUILT_IN_SIGNBIT)
+	    CASE_BUILTIN_F (BUILT_IN_SINH)
+	    CASE_BUILTIN_F (BUILT_IN_TANH)
+	    CASE_BUILTIN_F (BUILT_IN_TRUNC)
+	      /* True if the 1st argument is nonnegative.  */
+	      return tree_expr_nonnegative_p (TREE_VALUE (arglist));
+
+	    CASE_BUILTIN_F (BUILT_IN_FMAX)
+	      /* True if the 1st OR 2nd arguments are nonnegative.  */
+	      return tree_expr_nonnegative_p (TREE_VALUE (arglist))
+	        || tree_expr_nonnegative_p (TREE_VALUE (TREE_CHAIN (arglist)));
+
+	    CASE_BUILTIN_F (BUILT_IN_FMIN)
+	      /* True if the 1st AND 2nd arguments are nonnegative.  */
+	      return tree_expr_nonnegative_p (TREE_VALUE (arglist))
+	        && tree_expr_nonnegative_p (TREE_VALUE (TREE_CHAIN (arglist)));
+
+	    CASE_BUILTIN_F (BUILT_IN_COPYSIGN)
+	      /* True if the 2nd argument is nonnegative.  */
+	      return tree_expr_nonnegative_p (TREE_VALUE (TREE_CHAIN (arglist)));
+
+	    default:
+	      break;
+#undef CASE_BUILTIN_F
+#undef CASE_BUILTIN_I
+	    }
+      }
+
+      /* ... fall through ...  */
+
+    default:
+      if (truth_value_p (TREE_CODE (t)))
+	/* Truth values evaluate to 0 or 1, which is nonnegative.  */
+	return 1;
+    }
+
+  /* We don't know sign of `t', so be conservative and return false.  */
+  return 0;
+}
+
+/* Return true when T is an address and is known to be nonzero.
+   For floating point we further ensure that T is not denormal.
+   Similar logic is present in nonzero_address in rtlanal.h.  */
+
+bool
+tree_expr_nonzero_p (tree t)
+{
+  tree type = TREE_TYPE (t);
+
+  /* Doing something useful for floating point would need more work.  */
+  if (!INTEGRAL_TYPE_P (type) && !POINTER_TYPE_P (type))
+    return false;
+
+  switch (TREE_CODE (t))
+    {
+    case ABS_EXPR:
+      return tree_expr_nonzero_p (TREE_OPERAND (t, 0));
+
+    case INTEGER_CST:
+      /* We used to test for !integer_zerop here.  This does not work correctly
+	 if TREE_CONSTANT_OVERFLOW (t).  */
+      return (TREE_INT_CST_LOW (t) != 0
+	      || TREE_INT_CST_HIGH (t) != 0);
+
+    case PLUS_EXPR:
+      if (!TYPE_UNSIGNED (type) && !flag_wrapv)
+	{
+	  /* With the presence of negative values it is hard
+	     to say something.  */
+	  if (!tree_expr_nonnegative_p (TREE_OPERAND (t, 0))
+	      || !tree_expr_nonnegative_p (TREE_OPERAND (t, 1)))
+	    return false;
+	  /* One of operands must be positive and the other non-negative.  */
+	  return (tree_expr_nonzero_p (TREE_OPERAND (t, 0))
+	          || tree_expr_nonzero_p (TREE_OPERAND (t, 1)));
+	}
+      break;
+
+    case MULT_EXPR:
+      if (!TYPE_UNSIGNED (type) && !flag_wrapv)
+	{
+	  return (tree_expr_nonzero_p (TREE_OPERAND (t, 0))
+	          && tree_expr_nonzero_p (TREE_OPERAND (t, 1)));
+	}
+      break;
+
+    case NOP_EXPR:
+      {
+	tree inner_type = TREE_TYPE (TREE_OPERAND (t, 0));
+	tree outer_type = TREE_TYPE (t);
+
+	return (TYPE_PRECISION (inner_type) >= TYPE_PRECISION (outer_type)
+		&& tree_expr_nonzero_p (TREE_OPERAND (t, 0)));
+      }
+      break;
+
+   case ADDR_EXPR:
+      {
+	tree base = get_base_address (TREE_OPERAND (t, 0));
+
+	if (!base)
+	  return false;
+
+	/* Weak declarations may link to NULL.  */
+	if (VAR_OR_FUNCTION_DECL_P (base))
+	  return !DECL_WEAK (base);
+
+	/* Constants are never weak.  */
+	if (CONSTANT_CLASS_P (base))
+	  return true;
+
+	return false;
+      }
+
+    case COND_EXPR:
+      return (tree_expr_nonzero_p (TREE_OPERAND (t, 1))
+	      && tree_expr_nonzero_p (TREE_OPERAND (t, 2)));
+
+    case MIN_EXPR:
+      return (tree_expr_nonzero_p (TREE_OPERAND (t, 0))
+	      && tree_expr_nonzero_p (TREE_OPERAND (t, 1)));
+
+    case MAX_EXPR:
+      if (tree_expr_nonzero_p (TREE_OPERAND (t, 0)))
+	{
+	  /* When both operands are nonzero, then MAX must be too.  */
+	  if (tree_expr_nonzero_p (TREE_OPERAND (t, 1)))
+	    return true;
+
+	  /* MAX where operand 0 is positive is positive.  */
+	  return tree_expr_nonnegative_p (TREE_OPERAND (t, 0));
+	}
+      /* MAX where operand 1 is positive is positive.  */
+      else if (tree_expr_nonzero_p (TREE_OPERAND (t, 1))
+	       && tree_expr_nonnegative_p (TREE_OPERAND (t, 1)))
+	return true;
+      break;
+
+    case COMPOUND_EXPR:
+    case MODIFY_EXPR:
+    case BIND_EXPR:
+      return tree_expr_nonzero_p (TREE_OPERAND (t, 1));
+
+    case SAVE_EXPR:
+    case NON_LVALUE_EXPR:
+      return tree_expr_nonzero_p (TREE_OPERAND (t, 0));
+
+    case BIT_IOR_EXPR:
+      return tree_expr_nonzero_p (TREE_OPERAND (t, 1))
+	     || tree_expr_nonzero_p (TREE_OPERAND (t, 0));
+
+    case CALL_EXPR:
+      return alloca_call_p (t);
+
+    default:
+      break;
+    }
+  return false;
+}
+
+/* Given the components of a binary expression CODE, TYPE, OP0 and OP1,
+   attempt to fold the expression to a constant without modifying TYPE,
+   OP0 or OP1.
+
+   If the expression could be simplified to a constant, then return
+   the constant.  If the expression would not be simplified to a
+   constant, then return NULL_TREE.  */
+
+tree
+fold_binary_to_constant (enum tree_code code, tree type, tree op0, tree op1)
+{
+  tree tem = fold_binary (code, type, op0, op1);
+  return (tem && TREE_CONSTANT (tem)) ? tem : NULL_TREE;
+}
+
+/* Given the components of a unary expression CODE, TYPE and OP0,
+   attempt to fold the expression to a constant without modifying
+   TYPE or OP0.
+
+   If the expression could be simplified to a constant, then return
+   the constant.  If the expression would not be simplified to a
+   constant, then return NULL_TREE.  */
+
+tree
+fold_unary_to_constant (enum tree_code code, tree type, tree op0)
+{
+  tree tem = fold_unary (code, type, op0);
+  return (tem && TREE_CONSTANT (tem)) ? tem : NULL_TREE;
+}
+
+/* If EXP represents referencing an element in a constant string
+   (either via pointer arithmetic or array indexing), return the
+   tree representing the value accessed, otherwise return NULL.  */
+
+tree
+fold_read_from_constant_string (tree exp)
+{
+  if (TREE_CODE (exp) == INDIRECT_REF || TREE_CODE (exp) == ARRAY_REF)
+    {
+      tree exp1 = TREE_OPERAND (exp, 0);
+      tree index;
+      tree string;
+
+      if (TREE_CODE (exp) == INDIRECT_REF)
+	string = string_constant (exp1, &index);
+      else
+	{
+	  tree low_bound = array_ref_low_bound (exp);
+	  index = fold_convert (sizetype, TREE_OPERAND (exp, 1));
+
+	  /* Optimize the special-case of a zero lower bound.
+
+	     We convert the low_bound to sizetype to avoid some problems
+	     with constant folding.  (E.g. suppose the lower bound is 1,
+	     and its mode is QI.  Without the conversion,l (ARRAY
+	     +(INDEX-(unsigned char)1)) becomes ((ARRAY+(-(unsigned char)1))
+	     +INDEX), which becomes (ARRAY+255+INDEX).  Opps!)  */
+	  if (! integer_zerop (low_bound))
+	    index = size_diffop (index, fold_convert (sizetype, low_bound));
+
+	  string = exp1;
+	}
+
+      if (string
+	  && TREE_TYPE (exp) == TREE_TYPE (TREE_TYPE (string))
+	  && TREE_CODE (string) == STRING_CST
+	  && TREE_CODE (index) == INTEGER_CST
+	  && compare_tree_int (index, TREE_STRING_LENGTH (string)) < 0
+	  && (GET_MODE_CLASS (TYPE_MODE (TREE_TYPE (TREE_TYPE (string))))
+	      == MODE_INT)
+	  && (GET_MODE_SIZE (TYPE_MODE (TREE_TYPE (TREE_TYPE (string)))) == 1))
+	return fold_convert (TREE_TYPE (exp),
+			     build_int_cst (NULL_TREE,
+					    (TREE_STRING_POINTER (string)
+					     [TREE_INT_CST_LOW (index)])));
+    }
+  return NULL;
+}
+
+/* Return the tree for neg (ARG0) when ARG0 is known to be either
+   an integer constant or real constant.
+
+   TYPE is the type of the result.  */
+
+static tree
+fold_negate_const (tree arg0, tree type)
+{
+  tree t = NULL_TREE;
+
+  switch (TREE_CODE (arg0))
+    {
+    case INTEGER_CST:
+      {
+	unsigned HOST_WIDE_INT low;
+	HOST_WIDE_INT high;
+	int overflow = neg_double (TREE_INT_CST_LOW (arg0),
+				   TREE_INT_CST_HIGH (arg0),
+				   &low, &high);
+	t = build_int_cst_wide (type, low, high);
+	t = force_fit_type (t, 1,
+			    (overflow | TREE_OVERFLOW (arg0))
+			    && !TYPE_UNSIGNED (type),
+			    TREE_CONSTANT_OVERFLOW (arg0));
+	break;
+      }
+
+    case REAL_CST:
+      t = build_real (type, REAL_VALUE_NEGATE (TREE_REAL_CST (arg0)));
+      break;
+
+    default:
+      gcc_unreachable ();
+    }
+
+  return t;
+}
+
+/* Return the tree for abs (ARG0) when ARG0 is known to be either
+   an integer constant or real constant.
+
+   TYPE is the type of the result.  */
+
+tree
+fold_abs_const (tree arg0, tree type)
+{
+  tree t = NULL_TREE;
+
+  switch (TREE_CODE (arg0))
+    {
+    case INTEGER_CST:
+      /* If the value is unsigned, then the absolute value is
+	 the same as the ordinary value.  */
+      if (TYPE_UNSIGNED (type))
+	t = arg0;
+      /* Similarly, if the value is non-negative.  */
+      else if (INT_CST_LT (integer_minus_one_node, arg0))
+	t = arg0;
+      /* If the value is negative, then the absolute value is
+	 its negation.  */
+      else
+	{
+	  unsigned HOST_WIDE_INT low;
+	  HOST_WIDE_INT high;
+	  int overflow = neg_double (TREE_INT_CST_LOW (arg0),
+				     TREE_INT_CST_HIGH (arg0),
+				     &low, &high);
+	  t = build_int_cst_wide (type, low, high);
+	  t = force_fit_type (t, -1, overflow | TREE_OVERFLOW (arg0),
+			      TREE_CONSTANT_OVERFLOW (arg0));
+	}
+      break;
+
+    case REAL_CST:
+      if (REAL_VALUE_NEGATIVE (TREE_REAL_CST (arg0)))
+	t = build_real (type, REAL_VALUE_NEGATE (TREE_REAL_CST (arg0)));
+      else
+	t =  arg0;
+      break;
+
+    default:
+      gcc_unreachable ();
+    }
+
+  return t;
+}
+
+/* Return the tree for not (ARG0) when ARG0 is known to be an integer
+   constant.  TYPE is the type of the result.  */
+
+static tree
+fold_not_const (tree arg0, tree type)
+{
+  tree t = NULL_TREE;
+
+  gcc_assert (TREE_CODE (arg0) == INTEGER_CST);
+
+  t = build_int_cst_wide (type,
+			  ~ TREE_INT_CST_LOW (arg0),
+			  ~ TREE_INT_CST_HIGH (arg0));
+  t = force_fit_type (t, 0, TREE_OVERFLOW (arg0),
+		      TREE_CONSTANT_OVERFLOW (arg0));
+
+  return t;
+}
+
+/* Given CODE, a relational operator, the target type, TYPE and two
+   constant operands OP0 and OP1, return the result of the
+   relational operation.  If the result is not a compile time
+   constant, then return NULL_TREE.  */
+
+static tree
+fold_relational_const (enum tree_code code, tree type, tree op0, tree op1)
+{
+  int result, invert;
+
+  /* From here on, the only cases we handle are when the result is
+     known to be a constant.  */
+
+  if (TREE_CODE (op0) == REAL_CST && TREE_CODE (op1) == REAL_CST)
+    {
+      const REAL_VALUE_TYPE *c0 = TREE_REAL_CST_PTR (op0);
+      const REAL_VALUE_TYPE *c1 = TREE_REAL_CST_PTR (op1);
+
+      /* Handle the cases where either operand is a NaN.  */
+      if (real_isnan (c0) || real_isnan (c1))
+	{
+	  switch (code)
+	    {
+	    case EQ_EXPR:
+	    case ORDERED_EXPR:
+	      result = 0;
+	      break;
+
+	    case NE_EXPR:
+	    case UNORDERED_EXPR:
+	    case UNLT_EXPR:
+	    case UNLE_EXPR:
+	    case UNGT_EXPR:
+	    case UNGE_EXPR:
+	    case UNEQ_EXPR:
+              result = 1;
+	      break;
+
+	    case LT_EXPR:
+	    case LE_EXPR:
+	    case GT_EXPR:
+	    case GE_EXPR:
+	    case LTGT_EXPR:
+	      if (flag_trapping_math)
+		return NULL_TREE;
+	      result = 0;
+	      break;
+
+	    default:
+	      gcc_unreachable ();
+	    }
+
+	  return constant_boolean_node (result, type);
+	}
+
+      return constant_boolean_node (real_compare (code, c0, c1), type);
+    }
+
+  /* From here on we only handle LT, LE, GT, GE, EQ and NE.
+
+     To compute GT, swap the arguments and do LT.
+     To compute GE, do LT and invert the result.
+     To compute LE, swap the arguments, do LT and invert the result.
+     To compute NE, do EQ and invert the result.
+
+     Therefore, the code below must handle only EQ and LT.  */
+
+  if (code == LE_EXPR || code == GT_EXPR)
+    {
+      tree tem = op0;
+      op0 = op1;
+      op1 = tem;
+      code = swap_tree_comparison (code);
+    }
+
+  /* Note that it is safe to invert for real values here because we
+     have already handled the one case that it matters.  */
+
+  invert = 0;
+  if (code == NE_EXPR || code == GE_EXPR)
+    {
+      invert = 1;
+      code = invert_tree_comparison (code, false);
+    }
+
+  /* Compute a result for LT or EQ if args permit;
+     Otherwise return T.  */
+  if (TREE_CODE (op0) == INTEGER_CST && TREE_CODE (op1) == INTEGER_CST)
+    {
+      if (code == EQ_EXPR)
+	result = tree_int_cst_equal (op0, op1);
+      else if (TYPE_UNSIGNED (TREE_TYPE (op0)))
+	result = INT_CST_LT_UNSIGNED (op0, op1);
+      else
+	result = INT_CST_LT (op0, op1);
+    }
+  else
+    return NULL_TREE;
+
+  if (invert)
+    result ^= 1;
+  return constant_boolean_node (result, type);
+}
+
+/* Build an expression for the a clean point containing EXPR with type TYPE.
+   Don't build a cleanup point expression for EXPR which don't have side
+   effects.  */
+
+tree
+fold_build_cleanup_point_expr (tree type, tree expr)
+{
+  /* If the expression does not have side effects then we don't have to wrap
+     it with a cleanup point expression.  */
+  if (!TREE_SIDE_EFFECTS (expr))
+    return expr;
+
+  /* If the expression is a return, check to see if the expression inside the
+     return has no side effects or the right hand side of the modify expression
+     inside the return. If either don't have side effects set we don't need to
+     wrap the expression in a cleanup point expression.  Note we don't check the
+     left hand side of the modify because it should always be a return decl.  */
+  if (TREE_CODE (expr) == RETURN_EXPR)
+    {
+      tree op = TREE_OPERAND (expr, 0);
+      if (!op || !TREE_SIDE_EFFECTS (op))
+        return expr;
+      op = TREE_OPERAND (op, 1);
+      if (!TREE_SIDE_EFFECTS (op))
+        return expr;
+    }
+  
+  return build1 (CLEANUP_POINT_EXPR, type, expr);
+}
+
+/* Build an expression for the address of T.  Folds away INDIRECT_REF to
+   avoid confusing the gimplify process.  */
+
+tree
+build_fold_addr_expr_with_type (tree t, tree ptrtype)
+{
+  /* The size of the object is not relevant when talking about its address.  */
+  if (TREE_CODE (t) == WITH_SIZE_EXPR)
+    t = TREE_OPERAND (t, 0);
+
+  /* Note: doesn't apply to ALIGN_INDIRECT_REF */
+  if (TREE_CODE (t) == INDIRECT_REF
+      || TREE_CODE (t) == MISALIGNED_INDIRECT_REF)
+    {
+      t = TREE_OPERAND (t, 0);
+      if (TREE_TYPE (t) != ptrtype)
+	t = build1 (NOP_EXPR, ptrtype, t);
+    }
+  else
+    {
+      tree base = t;
+
+      while (handled_component_p (base))
+	base = TREE_OPERAND (base, 0);
+      if (DECL_P (base))
+	TREE_ADDRESSABLE (base) = 1;
+
+      t = build1 (ADDR_EXPR, ptrtype, t);
+    }
+
+  return t;
+}
+
+tree
+build_fold_addr_expr (tree t)
+{
+  return build_fold_addr_expr_with_type (t, build_pointer_type (TREE_TYPE (t)));
+}
+
+/* Given a pointer value OP0 and a type TYPE, return a simplified version
+   of an indirection through OP0, or NULL_TREE if no simplification is
+   possible.  */
+
+tree
+fold_indirect_ref_1 (tree type, tree op0)
+{
+  tree sub = op0;
+  tree subtype;
+
+  STRIP_NOPS (sub);
+  subtype = TREE_TYPE (sub);
+  if (!POINTER_TYPE_P (subtype))
+    return NULL_TREE;
+
+  if (TREE_CODE (sub) == ADDR_EXPR)
+    {
+      tree op = TREE_OPERAND (sub, 0);
+      tree optype = TREE_TYPE (op);
+      /* *&p => p */
+      if (type == optype)
+	return op;
+      /* *(foo *)&fooarray => fooarray[0] */
+      else if (TREE_CODE (optype) == ARRAY_TYPE
+	       && type == TREE_TYPE (optype))
+	{
+	  tree type_domain = TYPE_DOMAIN (optype);
+	  tree min_val = size_zero_node;
+	  if (type_domain && TYPE_MIN_VALUE (type_domain))
+	    min_val = TYPE_MIN_VALUE (type_domain);
+	  return build4 (ARRAY_REF, type, op, min_val, NULL_TREE, NULL_TREE);
+	}
+    }
+
+  /* *(foo *)fooarrptr => (*fooarrptr)[0] */
+  if (TREE_CODE (TREE_TYPE (subtype)) == ARRAY_TYPE
+      && type == TREE_TYPE (TREE_TYPE (subtype)))
+    {
+      tree type_domain;
+      tree min_val = size_zero_node;
+      sub = build_fold_indirect_ref (sub);
+      type_domain = TYPE_DOMAIN (TREE_TYPE (sub));
+      if (type_domain && TYPE_MIN_VALUE (type_domain))
+	min_val = TYPE_MIN_VALUE (type_domain);
+      return build4 (ARRAY_REF, type, sub, min_val, NULL_TREE, NULL_TREE);
+    }
+
+  return NULL_TREE;
+}
+
+/* Builds an expression for an indirection through T, simplifying some
+   cases.  */
+
+tree
+build_fold_indirect_ref (tree t)
+{
+  tree type = TREE_TYPE (TREE_TYPE (t));
+  tree sub = fold_indirect_ref_1 (type, t);
+
+  if (sub)
+    return sub;
+  else
+    return build1 (INDIRECT_REF, type, t);
+}
+
+/* Given an INDIRECT_REF T, return either T or a simplified version.  */
+
+tree
+fold_indirect_ref (tree t)
+{
+  tree sub = fold_indirect_ref_1 (TREE_TYPE (t), TREE_OPERAND (t, 0));
+
+  if (sub)
+    return sub;
+  else
+    return t;
+}
+
+/* Strip non-trapping, non-side-effecting tree nodes from an expression
+   whose result is ignored.  The type of the returned tree need not be
+   the same as the original expression.  */
+
+tree
+fold_ignored_result (tree t)
+{
+  if (!TREE_SIDE_EFFECTS (t))
+    return integer_zero_node;
+
+  for (;;)
+    switch (TREE_CODE_CLASS (TREE_CODE (t)))
+      {
+      case tcc_unary:
+	t = TREE_OPERAND (t, 0);
+	break;
+
+      case tcc_binary:
+      case tcc_comparison:
+	if (!TREE_SIDE_EFFECTS (TREE_OPERAND (t, 1)))
+	  t = TREE_OPERAND (t, 0);
+	else if (!TREE_SIDE_EFFECTS (TREE_OPERAND (t, 0)))
+	  t = TREE_OPERAND (t, 1);
+	else
+	  return t;
+	break;
+
+      case tcc_expression:
+	switch (TREE_CODE (t))
+	  {
+	  case COMPOUND_EXPR:
+	    if (TREE_SIDE_EFFECTS (TREE_OPERAND (t, 1)))
+	      return t;
+	    t = TREE_OPERAND (t, 0);
+	    break;
+
+	  case COND_EXPR:
+	    if (TREE_SIDE_EFFECTS (TREE_OPERAND (t, 1))
+		|| TREE_SIDE_EFFECTS (TREE_OPERAND (t, 2)))
+	      return t;
+	    t = TREE_OPERAND (t, 0);
+	    break;
+
+	  default:
+	    return t;
+	  }
+	break;
+
+      default:
+	return t;
+      }
+}
+
+/* Return the value of VALUE, rounded up to a multiple of DIVISOR.
+   This can only be applied to objects of a sizetype.  */
+
+tree
+round_up (tree value, int divisor)
+{
+  tree div = NULL_TREE;
+
+  gcc_assert (divisor > 0);
+  if (divisor == 1)
+    return value;
+
+  /* See if VALUE is already a multiple of DIVISOR.  If so, we don't
+     have to do anything.  Only do this when we are not given a const,
+     because in that case, this check is more expensive than just
+     doing it.  */
+  if (TREE_CODE (value) != INTEGER_CST)
+    {
+      div = build_int_cst (TREE_TYPE (value), divisor);
+
+      if (multiple_of_p (TREE_TYPE (value), value, div))
+	return value;
+    }
+
+  /* If divisor is a power of two, simplify this to bit manipulation.  */
+  if (divisor == (divisor & -divisor))
+    {
+      tree t;
+
+      t = build_int_cst (TREE_TYPE (value), divisor - 1);
+      value = size_binop (PLUS_EXPR, value, t);
+      t = build_int_cst (TREE_TYPE (value), -divisor);
+      value = size_binop (BIT_AND_EXPR, value, t);
+    }
+  else
+    {
+      if (!div)
+	div = build_int_cst (TREE_TYPE (value), divisor);
+      value = size_binop (CEIL_DIV_EXPR, value, div);
+      value = size_binop (MULT_EXPR, value, div);
+    }
+
+  return value;
+}
+
+/* Likewise, but round down.  */
+
+tree
+round_down (tree value, int divisor)
+{
+  tree div = NULL_TREE;
+
+  gcc_assert (divisor > 0);
+  if (divisor == 1)
+    return value;
+
+  /* See if VALUE is already a multiple of DIVISOR.  If so, we don't
+     have to do anything.  Only do this when we are not given a const,
+     because in that case, this check is more expensive than just
+     doing it.  */
+  if (TREE_CODE (value) != INTEGER_CST)
+    {
+      div = build_int_cst (TREE_TYPE (value), divisor);
+
+      if (multiple_of_p (TREE_TYPE (value), value, div))
+	return value;
+    }
+
+  /* If divisor is a power of two, simplify this to bit manipulation.  */
+  if (divisor == (divisor & -divisor))
+    {
+      tree t;
+
+      t = build_int_cst (TREE_TYPE (value), -divisor);
+      value = size_binop (BIT_AND_EXPR, value, t);
+    }
+  else
+    {
+      if (!div)
+	div = build_int_cst (TREE_TYPE (value), divisor);
+      value = size_binop (FLOOR_DIV_EXPR, value, div);
+      value = size_binop (MULT_EXPR, value, div);
+    }
+
+  return value;
+}
+
+/* Returns the pointer to the base of the object addressed by EXP and
+   extracts the information about the offset of the access, storing it
+   to PBITPOS and POFFSET.  */
+
+static tree
+split_address_to_core_and_offset (tree exp,
+				  HOST_WIDE_INT *pbitpos, tree *poffset)
+{
+  tree core;
+  enum machine_mode mode;
+  int unsignedp, volatilep;
+  HOST_WIDE_INT bitsize;
+
+  if (TREE_CODE (exp) == ADDR_EXPR)
+    {
+      core = get_inner_reference (TREE_OPERAND (exp, 0), &bitsize, pbitpos,
+				  poffset, &mode, &unsignedp, &volatilep,
+				  false);
+      core = build_fold_addr_expr (core);
+    }
+  else
+    {
+      core = exp;
+      *pbitpos = 0;
+      *poffset = NULL_TREE;
+    }
+
+  return core;
+}
+
+/* Returns true if addresses of E1 and E2 differ by a constant, false
+   otherwise.  If they do, E1 - E2 is stored in *DIFF.  */
+
+bool
+ptr_difference_const (tree e1, tree e2, HOST_WIDE_INT *diff)
+{
+  tree core1, core2;
+  HOST_WIDE_INT bitpos1, bitpos2;
+  tree toffset1, toffset2, tdiff, type;
+
+  core1 = split_address_to_core_and_offset (e1, &bitpos1, &toffset1);
+  core2 = split_address_to_core_and_offset (e2, &bitpos2, &toffset2);
+
+  if (bitpos1 % BITS_PER_UNIT != 0
+      || bitpos2 % BITS_PER_UNIT != 0
+      || !operand_equal_p (core1, core2, 0))
+    return false;
+
+  if (toffset1 && toffset2)
+    {
+      type = TREE_TYPE (toffset1);
+      if (type != TREE_TYPE (toffset2))
+	toffset2 = fold_convert (type, toffset2);
+
+      tdiff = fold_build2 (MINUS_EXPR, type, toffset1, toffset2);
+      if (!cst_and_fits_in_hwi (tdiff))
+	return false;
+
+      *diff = int_cst_value (tdiff);
+    }
+  else if (toffset1 || toffset2)
+    {
+      /* If only one of the offsets is non-constant, the difference cannot
+	 be a constant.  */
+      return false;
+    }
+  else
+    *diff = 0;
+
+  *diff += (bitpos1 - bitpos2) / BITS_PER_UNIT;
+  return true;
+}
+
+/* Simplify the floating point expression EXP when the sign of the
+   result is not significant.  Return NULL_TREE if no simplification
+   is possible.  */
+
+tree
+fold_strip_sign_ops (tree exp)
+{
+  tree arg0, arg1;
+
+  switch (TREE_CODE (exp))
+    {
+    case ABS_EXPR:
+    case NEGATE_EXPR:
+      arg0 = fold_strip_sign_ops (TREE_OPERAND (exp, 0));
+      return arg0 ? arg0 : TREE_OPERAND (exp, 0);
+
+    case MULT_EXPR:
+    case RDIV_EXPR:
+      if (HONOR_SIGN_DEPENDENT_ROUNDING (TYPE_MODE (TREE_TYPE (exp))))
+	return NULL_TREE;
+      arg0 = fold_strip_sign_ops (TREE_OPERAND (exp, 0));
+      arg1 = fold_strip_sign_ops (TREE_OPERAND (exp, 1));
+      if (arg0 != NULL_TREE || arg1 != NULL_TREE)
+	return fold_build2 (TREE_CODE (exp), TREE_TYPE (exp),
+			    arg0 ? arg0 : TREE_OPERAND (exp, 0),
+			    arg1 ? arg1 : TREE_OPERAND (exp, 1));
+      break;
+
+    default:
+      break;
+    }
+  return NULL_TREE;
+}
+
--- a/gcc/rtl.h
+++ b/gcc/rtl.h
@@ -243,6 +243,10 @@
      1 in a MEM if it refers to a scalar.
      1 in a SYMBOL_REF for a weak symbol.  */
   unsigned return_val : 1;
+  /* var_section indicates which section the variable is defined. */
+  unsigned int var_section : 4;
+  /* shift_type - shift by a constant amount or by an amount in a register.  */
+  unsigned int shift_type : 2;
 
   /* The first element of the operands of this rtx.
      The number of operands and their types are controlled
@@ -1321,6 +1325,11 @@
 #ifndef USE_STORE_PRE_DECREMENT
 #define USE_STORE_PRE_DECREMENT(MODE)   HAVE_PRE_DECREMENT
 #endif
+
+#ifdef MICROBLAZE
+#define VAR_SECTION(RTX) ((RTX)->var_section)
+#define SHIFT_TYPE(RTX) ((RTX)->shift_type)
+#endif
 
 /* Nonzero when we are generating CONCATs.  */
 extern int generating_concat_p;
-- 
1.5.3.8

